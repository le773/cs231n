{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 49000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  adagrad\n",
      "(Iteration 1 / 7640) loss: 6.531850\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.110000\n",
      "(Iteration 11 / 7640) loss: 5.916892\n",
      "(Iteration 21 / 7640) loss: 5.737517\n",
      "(Iteration 31 / 7640) loss: 5.563036\n",
      "(Iteration 41 / 7640) loss: 5.563081\n",
      "(Iteration 51 / 7640) loss: 5.474001\n",
      "(Iteration 61 / 7640) loss: 5.489438\n",
      "(Iteration 71 / 7640) loss: 5.365482\n",
      "(Iteration 81 / 7640) loss: 5.345763\n",
      "(Iteration 91 / 7640) loss: 5.421001\n",
      "(Iteration 101 / 7640) loss: 5.304609\n",
      "(Iteration 111 / 7640) loss: 5.242787\n",
      "(Iteration 121 / 7640) loss: 5.333087\n",
      "(Iteration 131 / 7640) loss: 5.217071\n",
      "(Iteration 141 / 7640) loss: 5.189731\n",
      "(Iteration 151 / 7640) loss: 5.155795\n",
      "(Iteration 161 / 7640) loss: 5.063757\n",
      "(Iteration 171 / 7640) loss: 5.106763\n",
      "(Iteration 181 / 7640) loss: 5.089823\n",
      "(Iteration 191 / 7640) loss: 5.075023\n",
      "(Iteration 201 / 7640) loss: 5.098399\n",
      "(Iteration 211 / 7640) loss: 5.028157\n",
      "(Iteration 221 / 7640) loss: 5.022632\n",
      "(Iteration 231 / 7640) loss: 5.007395\n",
      "(Iteration 241 / 7640) loss: 5.115407\n",
      "(Iteration 251 / 7640) loss: 5.325508\n",
      "(Iteration 261 / 7640) loss: 5.010027\n",
      "(Iteration 271 / 7640) loss: 4.978642\n",
      "(Iteration 281 / 7640) loss: 5.033854\n",
      "(Iteration 291 / 7640) loss: 4.931610\n",
      "(Iteration 301 / 7640) loss: 4.923506\n",
      "(Iteration 311 / 7640) loss: 4.792743\n",
      "(Iteration 321 / 7640) loss: 5.054925\n",
      "(Iteration 331 / 7640) loss: 4.981291\n",
      "(Iteration 341 / 7640) loss: 5.088833\n",
      "(Iteration 351 / 7640) loss: 4.858493\n",
      "(Iteration 361 / 7640) loss: 5.143911\n",
      "(Iteration 371 / 7640) loss: 5.068987\n",
      "(Iteration 381 / 7640) loss: 4.823786\n",
      "(Epoch 1 / 20) train acc: 0.449000; val_acc: 0.453000\n",
      "(Iteration 391 / 7640) loss: 4.955083\n",
      "(Iteration 401 / 7640) loss: 4.775089\n",
      "(Iteration 411 / 7640) loss: 4.799862\n",
      "(Iteration 421 / 7640) loss: 4.901930\n",
      "(Iteration 431 / 7640) loss: 5.110862\n",
      "(Iteration 441 / 7640) loss: 4.844986\n",
      "(Iteration 451 / 7640) loss: 4.841795\n",
      "(Iteration 461 / 7640) loss: 4.864116\n",
      "(Iteration 471 / 7640) loss: 4.928328\n",
      "(Iteration 481 / 7640) loss: 5.012723\n",
      "(Iteration 491 / 7640) loss: 4.806285\n",
      "(Iteration 501 / 7640) loss: 4.961288\n",
      "(Iteration 511 / 7640) loss: 4.995811\n",
      "(Iteration 521 / 7640) loss: 4.956300\n",
      "(Iteration 531 / 7640) loss: 4.805795\n",
      "(Iteration 541 / 7640) loss: 4.820602\n",
      "(Iteration 551 / 7640) loss: 4.833374\n",
      "(Iteration 561 / 7640) loss: 4.918198\n",
      "(Iteration 571 / 7640) loss: 4.876861\n",
      "(Iteration 581 / 7640) loss: 4.737278\n",
      "(Iteration 591 / 7640) loss: 4.881979\n",
      "(Iteration 601 / 7640) loss: 4.667131\n",
      "(Iteration 611 / 7640) loss: 4.747588\n",
      "(Iteration 621 / 7640) loss: 4.934839\n",
      "(Iteration 631 / 7640) loss: 4.801264\n",
      "(Iteration 641 / 7640) loss: 4.882455\n",
      "(Iteration 651 / 7640) loss: 4.773615\n",
      "(Iteration 661 / 7640) loss: 4.629939\n",
      "(Iteration 671 / 7640) loss: 4.797006\n",
      "(Iteration 681 / 7640) loss: 4.826963\n",
      "(Iteration 691 / 7640) loss: 4.746456\n",
      "(Iteration 701 / 7640) loss: 4.724719\n",
      "(Iteration 711 / 7640) loss: 4.800249\n",
      "(Iteration 721 / 7640) loss: 4.625676\n",
      "(Iteration 731 / 7640) loss: 4.694447\n",
      "(Iteration 741 / 7640) loss: 4.758382\n",
      "(Iteration 751 / 7640) loss: 4.679966\n",
      "(Iteration 761 / 7640) loss: 4.782681\n",
      "(Epoch 2 / 20) train acc: 0.463000; val_acc: 0.474000\n",
      "(Iteration 771 / 7640) loss: 4.777495\n",
      "(Iteration 781 / 7640) loss: 4.831727\n",
      "(Iteration 791 / 7640) loss: 4.797421\n",
      "(Iteration 801 / 7640) loss: 4.830983\n",
      "(Iteration 811 / 7640) loss: 4.713999\n",
      "(Iteration 821 / 7640) loss: 4.790933\n",
      "(Iteration 831 / 7640) loss: 4.683797\n",
      "(Iteration 841 / 7640) loss: 4.444964\n",
      "(Iteration 851 / 7640) loss: 4.667424\n",
      "(Iteration 861 / 7640) loss: 4.909176\n",
      "(Iteration 871 / 7640) loss: 4.598173\n",
      "(Iteration 881 / 7640) loss: 4.649516\n",
      "(Iteration 891 / 7640) loss: 4.899069\n",
      "(Iteration 901 / 7640) loss: 4.744224\n",
      "(Iteration 911 / 7640) loss: 4.757746\n",
      "(Iteration 921 / 7640) loss: 4.670177\n",
      "(Iteration 931 / 7640) loss: 4.656129\n",
      "(Iteration 941 / 7640) loss: 4.570134\n",
      "(Iteration 951 / 7640) loss: 4.830932\n",
      "(Iteration 961 / 7640) loss: 4.733997\n",
      "(Iteration 971 / 7640) loss: 4.596553\n",
      "(Iteration 981 / 7640) loss: 4.524616\n",
      "(Iteration 991 / 7640) loss: 4.659663\n",
      "(Iteration 1001 / 7640) loss: 4.683995\n",
      "(Iteration 1011 / 7640) loss: 4.652978\n",
      "(Iteration 1021 / 7640) loss: 4.599048\n",
      "(Iteration 1031 / 7640) loss: 4.545655\n",
      "(Iteration 1041 / 7640) loss: 4.664462\n",
      "(Iteration 1051 / 7640) loss: 4.522076\n",
      "(Iteration 1061 / 7640) loss: 4.626624\n",
      "(Iteration 1071 / 7640) loss: 4.501399\n",
      "(Iteration 1081 / 7640) loss: 4.776212\n",
      "(Iteration 1091 / 7640) loss: 4.620395\n",
      "(Iteration 1101 / 7640) loss: 4.678706\n",
      "(Iteration 1111 / 7640) loss: 4.621618\n",
      "(Iteration 1121 / 7640) loss: 4.714145\n",
      "(Iteration 1131 / 7640) loss: 4.492494\n",
      "(Iteration 1141 / 7640) loss: 4.629630\n",
      "(Epoch 3 / 20) train acc: 0.516000; val_acc: 0.496000\n",
      "(Iteration 1151 / 7640) loss: 4.545461\n",
      "(Iteration 1161 / 7640) loss: 4.594578\n",
      "(Iteration 1171 / 7640) loss: 4.679013\n",
      "(Iteration 1181 / 7640) loss: 4.474368\n",
      "(Iteration 1191 / 7640) loss: 4.473789\n",
      "(Iteration 1201 / 7640) loss: 4.661219\n",
      "(Iteration 1211 / 7640) loss: 4.674921\n",
      "(Iteration 1221 / 7640) loss: 4.520251\n",
      "(Iteration 1231 / 7640) loss: 4.513752\n",
      "(Iteration 1241 / 7640) loss: 4.573614\n",
      "(Iteration 1251 / 7640) loss: 4.545312\n",
      "(Iteration 1261 / 7640) loss: 4.476127\n",
      "(Iteration 1271 / 7640) loss: 4.622716\n",
      "(Iteration 1281 / 7640) loss: 4.394888\n",
      "(Iteration 1291 / 7640) loss: 4.710738\n",
      "(Iteration 1301 / 7640) loss: 4.499992\n",
      "(Iteration 1311 / 7640) loss: 4.715914\n",
      "(Iteration 1321 / 7640) loss: 4.537673\n",
      "(Iteration 1331 / 7640) loss: 4.403614\n",
      "(Iteration 1341 / 7640) loss: 4.451177\n",
      "(Iteration 1351 / 7640) loss: 4.516736\n",
      "(Iteration 1361 / 7640) loss: 4.424159\n",
      "(Iteration 1371 / 7640) loss: 4.292038\n",
      "(Iteration 1381 / 7640) loss: 4.376415\n",
      "(Iteration 1391 / 7640) loss: 4.407964\n",
      "(Iteration 1401 / 7640) loss: 4.623529\n",
      "(Iteration 1411 / 7640) loss: 4.667740\n",
      "(Iteration 1421 / 7640) loss: 4.417797\n",
      "(Iteration 1431 / 7640) loss: 4.671858\n",
      "(Iteration 1441 / 7640) loss: 4.619417\n",
      "(Iteration 1451 / 7640) loss: 4.494755\n",
      "(Iteration 1461 / 7640) loss: 4.565251\n",
      "(Iteration 1471 / 7640) loss: 4.450575\n",
      "(Iteration 1481 / 7640) loss: 4.498293\n",
      "(Iteration 1491 / 7640) loss: 4.522452\n",
      "(Iteration 1501 / 7640) loss: 4.505362\n",
      "(Iteration 1511 / 7640) loss: 4.392020\n",
      "(Iteration 1521 / 7640) loss: 4.495913\n",
      "(Epoch 4 / 20) train acc: 0.548000; val_acc: 0.513000\n",
      "(Iteration 1531 / 7640) loss: 4.367780\n",
      "(Iteration 1541 / 7640) loss: 4.362239\n",
      "(Iteration 1551 / 7640) loss: 4.458892\n",
      "(Iteration 1561 / 7640) loss: 4.374692\n",
      "(Iteration 1571 / 7640) loss: 4.458284\n",
      "(Iteration 1581 / 7640) loss: 4.427602\n",
      "(Iteration 1591 / 7640) loss: 4.400843\n",
      "(Iteration 1601 / 7640) loss: 4.507648\n",
      "(Iteration 1611 / 7640) loss: 4.334920\n",
      "(Iteration 1621 / 7640) loss: 4.386332\n",
      "(Iteration 1631 / 7640) loss: 4.462715\n",
      "(Iteration 1641 / 7640) loss: 4.548278\n",
      "(Iteration 1651 / 7640) loss: 4.386151\n",
      "(Iteration 1661 / 7640) loss: 4.326120\n",
      "(Iteration 1671 / 7640) loss: 4.297907\n",
      "(Iteration 1681 / 7640) loss: 4.385335\n",
      "(Iteration 1691 / 7640) loss: 4.465230\n",
      "(Iteration 1701 / 7640) loss: 4.463088\n",
      "(Iteration 1711 / 7640) loss: 4.431772\n",
      "(Iteration 1721 / 7640) loss: 4.512041\n",
      "(Iteration 1731 / 7640) loss: 4.418764\n",
      "(Iteration 1741 / 7640) loss: 4.413304\n",
      "(Iteration 1751 / 7640) loss: 4.367721\n",
      "(Iteration 1761 / 7640) loss: 4.399958\n",
      "(Iteration 1771 / 7640) loss: 4.370365\n",
      "(Iteration 1781 / 7640) loss: 4.436187\n",
      "(Iteration 1791 / 7640) loss: 4.527144\n",
      "(Iteration 1801 / 7640) loss: 4.397223\n",
      "(Iteration 1811 / 7640) loss: 4.324297\n",
      "(Iteration 1821 / 7640) loss: 4.475000\n",
      "(Iteration 1831 / 7640) loss: 4.283131\n",
      "(Iteration 1841 / 7640) loss: 4.338708\n",
      "(Iteration 1851 / 7640) loss: 4.449359\n",
      "(Iteration 1861 / 7640) loss: 4.357427\n",
      "(Iteration 1871 / 7640) loss: 4.274119\n",
      "(Iteration 1881 / 7640) loss: 4.443519\n",
      "(Iteration 1891 / 7640) loss: 4.384884\n",
      "(Iteration 1901 / 7640) loss: 4.356330\n",
      "(Epoch 5 / 20) train acc: 0.543000; val_acc: 0.513000\n",
      "(Iteration 1911 / 7640) loss: 4.269972\n",
      "(Iteration 1921 / 7640) loss: 4.404360\n",
      "(Iteration 1931 / 7640) loss: 4.431118\n",
      "(Iteration 1941 / 7640) loss: 4.407467\n",
      "(Iteration 1951 / 7640) loss: 4.390485\n",
      "(Iteration 1961 / 7640) loss: 4.309637\n",
      "(Iteration 1971 / 7640) loss: 4.485619\n",
      "(Iteration 1981 / 7640) loss: 4.417711\n",
      "(Iteration 1991 / 7640) loss: 4.275484\n",
      "(Iteration 2001 / 7640) loss: 4.315063\n",
      "(Iteration 2011 / 7640) loss: 4.341276\n",
      "(Iteration 2021 / 7640) loss: 4.581824\n",
      "(Iteration 2031 / 7640) loss: 4.283158\n",
      "(Iteration 2041 / 7640) loss: 4.261172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2051 / 7640) loss: 4.419381\n",
      "(Iteration 2061 / 7640) loss: 4.230339\n",
      "(Iteration 2071 / 7640) loss: 4.247891\n",
      "(Iteration 2081 / 7640) loss: 4.379881\n",
      "(Iteration 2091 / 7640) loss: 4.290512\n",
      "(Iteration 2101 / 7640) loss: 4.350750\n",
      "(Iteration 2111 / 7640) loss: 4.365533\n",
      "(Iteration 2121 / 7640) loss: 4.302517\n",
      "(Iteration 2131 / 7640) loss: 4.287841\n",
      "(Iteration 2141 / 7640) loss: 4.366960\n",
      "(Iteration 2151 / 7640) loss: 4.217970\n",
      "(Iteration 2161 / 7640) loss: 4.163602\n",
      "(Iteration 2171 / 7640) loss: 4.276423\n",
      "(Iteration 2181 / 7640) loss: 4.449403\n",
      "(Iteration 2191 / 7640) loss: 4.224004\n",
      "(Iteration 2201 / 7640) loss: 4.323230\n",
      "(Iteration 2211 / 7640) loss: 4.468042\n",
      "(Iteration 2221 / 7640) loss: 4.253990\n",
      "(Iteration 2231 / 7640) loss: 4.382289\n",
      "(Iteration 2241 / 7640) loss: 4.267651\n",
      "(Iteration 2251 / 7640) loss: 4.319640\n",
      "(Iteration 2261 / 7640) loss: 4.271468\n",
      "(Iteration 2271 / 7640) loss: 4.271519\n",
      "(Iteration 2281 / 7640) loss: 4.174758\n",
      "(Iteration 2291 / 7640) loss: 4.202897\n",
      "(Epoch 6 / 20) train acc: 0.592000; val_acc: 0.530000\n",
      "(Iteration 2301 / 7640) loss: 4.337794\n",
      "(Iteration 2311 / 7640) loss: 4.287638\n",
      "(Iteration 2321 / 7640) loss: 4.422085\n",
      "(Iteration 2331 / 7640) loss: 4.380331\n",
      "(Iteration 2341 / 7640) loss: 4.247690\n",
      "(Iteration 2351 / 7640) loss: 4.235474\n",
      "(Iteration 2361 / 7640) loss: 4.249122\n",
      "(Iteration 2371 / 7640) loss: 4.116756\n",
      "(Iteration 2381 / 7640) loss: 4.269536\n",
      "(Iteration 2391 / 7640) loss: 4.374195\n",
      "(Iteration 2401 / 7640) loss: 4.302101\n",
      "(Iteration 2411 / 7640) loss: 4.167678\n",
      "(Iteration 2421 / 7640) loss: 4.213826\n",
      "(Iteration 2431 / 7640) loss: 4.434507\n",
      "(Iteration 2441 / 7640) loss: 4.147475\n",
      "(Iteration 2451 / 7640) loss: 4.207427\n",
      "(Iteration 2461 / 7640) loss: 4.186861\n",
      "(Iteration 2471 / 7640) loss: 4.326627\n",
      "(Iteration 2481 / 7640) loss: 4.348234\n",
      "(Iteration 2491 / 7640) loss: 4.089888\n",
      "(Iteration 2501 / 7640) loss: 4.101062\n",
      "(Iteration 2511 / 7640) loss: 4.070668\n",
      "(Iteration 2521 / 7640) loss: 4.192708\n",
      "(Iteration 2531 / 7640) loss: 4.228436\n",
      "(Iteration 2541 / 7640) loss: 4.109587\n",
      "(Iteration 2551 / 7640) loss: 4.247523\n",
      "(Iteration 2561 / 7640) loss: 4.235281\n",
      "(Iteration 2571 / 7640) loss: 4.206766\n",
      "(Iteration 2581 / 7640) loss: 4.371548\n",
      "(Iteration 2591 / 7640) loss: 4.112577\n",
      "(Iteration 2601 / 7640) loss: 4.283729\n",
      "(Iteration 2611 / 7640) loss: 4.176322\n",
      "(Iteration 2621 / 7640) loss: 4.196022\n",
      "(Iteration 2631 / 7640) loss: 4.205836\n",
      "(Iteration 2641 / 7640) loss: 4.234520\n",
      "(Iteration 2651 / 7640) loss: 4.243124\n",
      "(Iteration 2661 / 7640) loss: 4.170451\n",
      "(Iteration 2671 / 7640) loss: 4.188096\n",
      "(Epoch 7 / 20) train acc: 0.580000; val_acc: 0.533000\n",
      "(Iteration 2681 / 7640) loss: 4.182317\n",
      "(Iteration 2691 / 7640) loss: 4.104125\n",
      "(Iteration 2701 / 7640) loss: 4.387443\n",
      "(Iteration 2711 / 7640) loss: 4.090997\n",
      "(Iteration 2721 / 7640) loss: 4.188881\n",
      "(Iteration 2731 / 7640) loss: 4.185376\n",
      "(Iteration 2741 / 7640) loss: 4.197865\n",
      "(Iteration 2751 / 7640) loss: 4.167119\n",
      "(Iteration 2761 / 7640) loss: 4.222519\n",
      "(Iteration 2771 / 7640) loss: 4.242553\n",
      "(Iteration 2781 / 7640) loss: 3.982265\n",
      "(Iteration 2791 / 7640) loss: 4.258809\n",
      "(Iteration 2801 / 7640) loss: 4.259642\n",
      "(Iteration 2811 / 7640) loss: 4.312100\n",
      "(Iteration 2821 / 7640) loss: 4.148788\n",
      "(Iteration 2831 / 7640) loss: 4.266931\n",
      "(Iteration 2841 / 7640) loss: 4.150030\n",
      "(Iteration 2851 / 7640) loss: 4.250482\n",
      "(Iteration 2861 / 7640) loss: 4.267838\n",
      "(Iteration 2871 / 7640) loss: 4.107047\n",
      "(Iteration 2881 / 7640) loss: 4.132133\n",
      "(Iteration 2891 / 7640) loss: 4.210696\n",
      "(Iteration 2901 / 7640) loss: 4.207872\n",
      "(Iteration 2911 / 7640) loss: 4.086787\n",
      "(Iteration 2921 / 7640) loss: 4.151662\n",
      "(Iteration 2931 / 7640) loss: 4.153409\n",
      "(Iteration 2941 / 7640) loss: 4.258762\n",
      "(Iteration 2951 / 7640) loss: 4.253352\n",
      "(Iteration 2961 / 7640) loss: 4.256012\n",
      "(Iteration 2971 / 7640) loss: 4.277070\n",
      "(Iteration 2981 / 7640) loss: 4.172649\n",
      "(Iteration 2991 / 7640) loss: 4.305370\n",
      "(Iteration 3001 / 7640) loss: 4.060399\n",
      "(Iteration 3011 / 7640) loss: 4.305833\n",
      "(Iteration 3021 / 7640) loss: 4.158410\n",
      "(Iteration 3031 / 7640) loss: 4.102486\n",
      "(Iteration 3041 / 7640) loss: 4.078771\n",
      "(Iteration 3051 / 7640) loss: 4.179215\n",
      "(Epoch 8 / 20) train acc: 0.618000; val_acc: 0.545000\n",
      "(Iteration 3061 / 7640) loss: 4.048659\n",
      "(Iteration 3071 / 7640) loss: 4.233456\n",
      "(Iteration 3081 / 7640) loss: 4.058399\n",
      "(Iteration 3091 / 7640) loss: 4.150535\n",
      "(Iteration 3101 / 7640) loss: 4.126459\n",
      "(Iteration 3111 / 7640) loss: 4.202742\n",
      "(Iteration 3121 / 7640) loss: 4.277181\n",
      "(Iteration 3131 / 7640) loss: 4.093673\n",
      "(Iteration 3141 / 7640) loss: 4.038980\n",
      "(Iteration 3151 / 7640) loss: 4.208642\n",
      "(Iteration 3161 / 7640) loss: 4.189534\n",
      "(Iteration 3171 / 7640) loss: 4.138710\n",
      "(Iteration 3181 / 7640) loss: 4.202236\n",
      "(Iteration 3191 / 7640) loss: 4.241223\n",
      "(Iteration 3201 / 7640) loss: 4.065705\n",
      "(Iteration 3211 / 7640) loss: 4.261413\n",
      "(Iteration 3221 / 7640) loss: 4.167821\n",
      "(Iteration 3231 / 7640) loss: 3.966274\n",
      "(Iteration 3241 / 7640) loss: 4.020406\n",
      "(Iteration 3251 / 7640) loss: 4.019931\n",
      "(Iteration 3261 / 7640) loss: 4.018740\n",
      "(Iteration 3271 / 7640) loss: 4.099576\n",
      "(Iteration 3281 / 7640) loss: 4.064415\n",
      "(Iteration 3291 / 7640) loss: 4.191362\n",
      "(Iteration 3301 / 7640) loss: 4.246201\n",
      "(Iteration 3311 / 7640) loss: 4.137859\n",
      "(Iteration 3321 / 7640) loss: 4.107577\n",
      "(Iteration 3331 / 7640) loss: 4.155205\n",
      "(Iteration 3341 / 7640) loss: 4.082916\n",
      "(Iteration 3351 / 7640) loss: 4.094797\n",
      "(Iteration 3361 / 7640) loss: 4.047061\n",
      "(Iteration 3371 / 7640) loss: 4.135008\n",
      "(Iteration 3381 / 7640) loss: 4.072497\n",
      "(Iteration 3391 / 7640) loss: 3.941737\n",
      "(Iteration 3401 / 7640) loss: 4.201178\n",
      "(Iteration 3411 / 7640) loss: 4.161395\n",
      "(Iteration 3421 / 7640) loss: 4.141872\n",
      "(Iteration 3431 / 7640) loss: 4.166254\n",
      "(Epoch 9 / 20) train acc: 0.577000; val_acc: 0.534000\n",
      "(Iteration 3441 / 7640) loss: 4.261591\n",
      "(Iteration 3451 / 7640) loss: 3.966648\n",
      "(Iteration 3461 / 7640) loss: 4.047036\n",
      "(Iteration 3471 / 7640) loss: 4.060506\n",
      "(Iteration 3481 / 7640) loss: 4.047154\n",
      "(Iteration 3491 / 7640) loss: 4.187400\n",
      "(Iteration 3501 / 7640) loss: 4.096083\n",
      "(Iteration 3511 / 7640) loss: 4.065854\n",
      "(Iteration 3521 / 7640) loss: 4.208465\n",
      "(Iteration 3531 / 7640) loss: 4.112511\n",
      "(Iteration 3541 / 7640) loss: 4.103674\n",
      "(Iteration 3551 / 7640) loss: 4.076766\n",
      "(Iteration 3561 / 7640) loss: 4.077531\n",
      "(Iteration 3571 / 7640) loss: 3.937954\n",
      "(Iteration 3581 / 7640) loss: 3.945810\n",
      "(Iteration 3591 / 7640) loss: 4.206737\n",
      "(Iteration 3601 / 7640) loss: 3.980868\n",
      "(Iteration 3611 / 7640) loss: 4.136316\n",
      "(Iteration 3621 / 7640) loss: 4.001163\n",
      "(Iteration 3631 / 7640) loss: 3.978102\n",
      "(Iteration 3641 / 7640) loss: 4.159230\n",
      "(Iteration 3651 / 7640) loss: 4.035406\n",
      "(Iteration 3661 / 7640) loss: 4.097237\n",
      "(Iteration 3671 / 7640) loss: 4.011732\n",
      "(Iteration 3681 / 7640) loss: 4.002646\n",
      "(Iteration 3691 / 7640) loss: 4.151421\n",
      "(Iteration 3701 / 7640) loss: 4.255307\n",
      "(Iteration 3711 / 7640) loss: 4.073445\n",
      "(Iteration 3721 / 7640) loss: 4.250054\n",
      "(Iteration 3731 / 7640) loss: 4.079200\n",
      "(Iteration 3741 / 7640) loss: 4.115312\n",
      "(Iteration 3751 / 7640) loss: 4.109901\n",
      "(Iteration 3761 / 7640) loss: 4.017422\n",
      "(Iteration 3771 / 7640) loss: 4.076297\n",
      "(Iteration 3781 / 7640) loss: 4.055980\n",
      "(Iteration 3791 / 7640) loss: 4.092033\n",
      "(Iteration 3801 / 7640) loss: 3.949466\n",
      "(Iteration 3811 / 7640) loss: 4.089721\n",
      "(Epoch 10 / 20) train acc: 0.610000; val_acc: 0.550000\n",
      "(Iteration 3821 / 7640) loss: 3.956062\n",
      "(Iteration 3831 / 7640) loss: 4.066809\n",
      "(Iteration 3841 / 7640) loss: 3.906866\n",
      "(Iteration 3851 / 7640) loss: 3.950863\n",
      "(Iteration 3861 / 7640) loss: 4.087380\n",
      "(Iteration 3871 / 7640) loss: 3.819784\n",
      "(Iteration 3881 / 7640) loss: 4.006007\n",
      "(Iteration 3891 / 7640) loss: 4.080174\n",
      "(Iteration 3901 / 7640) loss: 4.092868\n",
      "(Iteration 3911 / 7640) loss: 4.098928\n",
      "(Iteration 3921 / 7640) loss: 4.106341\n",
      "(Iteration 3931 / 7640) loss: 4.018112\n",
      "(Iteration 3941 / 7640) loss: 3.967880\n",
      "(Iteration 3951 / 7640) loss: 4.220812\n",
      "(Iteration 3961 / 7640) loss: 4.042014\n",
      "(Iteration 3971 / 7640) loss: 3.971828\n",
      "(Iteration 3981 / 7640) loss: 4.100092\n",
      "(Iteration 3991 / 7640) loss: 3.906099\n",
      "(Iteration 4001 / 7640) loss: 3.927568\n",
      "(Iteration 4011 / 7640) loss: 4.008369\n",
      "(Iteration 4021 / 7640) loss: 4.053152\n",
      "(Iteration 4031 / 7640) loss: 4.004353\n",
      "(Iteration 4041 / 7640) loss: 3.914654\n",
      "(Iteration 4051 / 7640) loss: 4.001270\n",
      "(Iteration 4061 / 7640) loss: 4.193962\n",
      "(Iteration 4071 / 7640) loss: 4.063647\n",
      "(Iteration 4081 / 7640) loss: 3.942164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4091 / 7640) loss: 3.965409\n",
      "(Iteration 4101 / 7640) loss: 4.001639\n",
      "(Iteration 4111 / 7640) loss: 3.978370\n",
      "(Iteration 4121 / 7640) loss: 4.106954\n",
      "(Iteration 4131 / 7640) loss: 4.121790\n",
      "(Iteration 4141 / 7640) loss: 4.057649\n",
      "(Iteration 4151 / 7640) loss: 4.119054\n",
      "(Iteration 4161 / 7640) loss: 3.981333\n",
      "(Iteration 4171 / 7640) loss: 4.042330\n",
      "(Iteration 4181 / 7640) loss: 3.988582\n",
      "(Iteration 4191 / 7640) loss: 4.033933\n",
      "(Iteration 4201 / 7640) loss: 4.085843\n",
      "(Epoch 11 / 20) train acc: 0.641000; val_acc: 0.541000\n",
      "(Iteration 4211 / 7640) loss: 4.039578\n",
      "(Iteration 4221 / 7640) loss: 4.002010\n",
      "(Iteration 4231 / 7640) loss: 4.109588\n",
      "(Iteration 4241 / 7640) loss: 3.993988\n",
      "(Iteration 4251 / 7640) loss: 3.926261\n",
      "(Iteration 4261 / 7640) loss: 3.957505\n",
      "(Iteration 4271 / 7640) loss: 4.031847\n",
      "(Iteration 4281 / 7640) loss: 4.008936\n",
      "(Iteration 4291 / 7640) loss: 3.842503\n",
      "(Iteration 4301 / 7640) loss: 4.011889\n",
      "(Iteration 4311 / 7640) loss: 3.926226\n",
      "(Iteration 4321 / 7640) loss: 4.011785\n",
      "(Iteration 4331 / 7640) loss: 4.104707\n",
      "(Iteration 4341 / 7640) loss: 3.955800\n",
      "(Iteration 4351 / 7640) loss: 4.003637\n",
      "(Iteration 4361 / 7640) loss: 4.083593\n",
      "(Iteration 4371 / 7640) loss: 3.867696\n",
      "(Iteration 4381 / 7640) loss: 3.985000\n",
      "(Iteration 4391 / 7640) loss: 4.054424\n",
      "(Iteration 4401 / 7640) loss: 3.922303\n",
      "(Iteration 4411 / 7640) loss: 4.057186\n",
      "(Iteration 4421 / 7640) loss: 4.140836\n",
      "(Iteration 4431 / 7640) loss: 4.033621\n",
      "(Iteration 4441 / 7640) loss: 4.031665\n",
      "(Iteration 4451 / 7640) loss: 4.057046\n",
      "(Iteration 4461 / 7640) loss: 4.146475\n",
      "(Iteration 4471 / 7640) loss: 3.992820\n",
      "(Iteration 4481 / 7640) loss: 3.812129\n",
      "(Iteration 4491 / 7640) loss: 3.949033\n",
      "(Iteration 4501 / 7640) loss: 3.901847\n",
      "(Iteration 4511 / 7640) loss: 4.183311\n",
      "(Iteration 4521 / 7640) loss: 3.997197\n",
      "(Iteration 4531 / 7640) loss: 4.028995\n",
      "(Iteration 4541 / 7640) loss: 3.976092\n",
      "(Iteration 4551 / 7640) loss: 3.961219\n",
      "(Iteration 4561 / 7640) loss: 3.994099\n",
      "(Iteration 4571 / 7640) loss: 3.923066\n",
      "(Iteration 4581 / 7640) loss: 3.986865\n",
      "(Epoch 12 / 20) train acc: 0.640000; val_acc: 0.544000\n",
      "(Iteration 4591 / 7640) loss: 3.944136\n",
      "(Iteration 4601 / 7640) loss: 3.889155\n",
      "(Iteration 4611 / 7640) loss: 3.985401\n",
      "(Iteration 4621 / 7640) loss: 3.933738\n",
      "(Iteration 4631 / 7640) loss: 4.047985\n",
      "(Iteration 4641 / 7640) loss: 3.806909\n",
      "(Iteration 4651 / 7640) loss: 4.072801\n",
      "(Iteration 4661 / 7640) loss: 4.049110\n",
      "(Iteration 4671 / 7640) loss: 3.885223\n",
      "(Iteration 4681 / 7640) loss: 4.007875\n",
      "(Iteration 4691 / 7640) loss: 3.991421\n",
      "(Iteration 4701 / 7640) loss: 3.973021\n",
      "(Iteration 4711 / 7640) loss: 4.038531\n",
      "(Iteration 4721 / 7640) loss: 3.851439\n",
      "(Iteration 4731 / 7640) loss: 3.863910\n",
      "(Iteration 4741 / 7640) loss: 4.132495\n",
      "(Iteration 4751 / 7640) loss: 3.918554\n",
      "(Iteration 4761 / 7640) loss: 3.679338\n",
      "(Iteration 4771 / 7640) loss: 3.867675\n",
      "(Iteration 4781 / 7640) loss: 4.044273\n",
      "(Iteration 4791 / 7640) loss: 3.862628\n",
      "(Iteration 4801 / 7640) loss: 4.028474\n",
      "(Iteration 4811 / 7640) loss: 3.974756\n",
      "(Iteration 4821 / 7640) loss: 3.957641\n",
      "(Iteration 4831 / 7640) loss: 3.762728\n",
      "(Iteration 4841 / 7640) loss: 3.986883\n",
      "(Iteration 4851 / 7640) loss: 3.892663\n",
      "(Iteration 4861 / 7640) loss: 3.773373\n",
      "(Iteration 4871 / 7640) loss: 3.990816\n",
      "(Iteration 4881 / 7640) loss: 3.893389\n",
      "(Iteration 4891 / 7640) loss: 3.976473\n",
      "(Iteration 4901 / 7640) loss: 3.825188\n",
      "(Iteration 4911 / 7640) loss: 3.869124\n",
      "(Iteration 4921 / 7640) loss: 3.691821\n",
      "(Iteration 4931 / 7640) loss: 3.895431\n",
      "(Iteration 4941 / 7640) loss: 3.709993\n",
      "(Iteration 4951 / 7640) loss: 4.130137\n",
      "(Iteration 4961 / 7640) loss: 3.911863\n",
      "(Epoch 13 / 20) train acc: 0.642000; val_acc: 0.536000\n",
      "(Iteration 4971 / 7640) loss: 3.665539\n",
      "(Iteration 4981 / 7640) loss: 4.040922\n",
      "(Iteration 4991 / 7640) loss: 3.797915\n",
      "(Iteration 5001 / 7640) loss: 3.908945\n",
      "(Iteration 5011 / 7640) loss: 3.869313\n",
      "(Iteration 5021 / 7640) loss: 3.823102\n",
      "(Iteration 5031 / 7640) loss: 3.747829\n",
      "(Iteration 5041 / 7640) loss: 3.963751\n",
      "(Iteration 5051 / 7640) loss: 3.893624\n",
      "(Iteration 5061 / 7640) loss: 3.858026\n",
      "(Iteration 5071 / 7640) loss: 4.101149\n",
      "(Iteration 5081 / 7640) loss: 3.759063\n",
      "(Iteration 5091 / 7640) loss: 3.880039\n",
      "(Iteration 5101 / 7640) loss: 3.834171\n",
      "(Iteration 5111 / 7640) loss: 3.770997\n",
      "(Iteration 5121 / 7640) loss: 3.936917\n",
      "(Iteration 5131 / 7640) loss: 3.862885\n",
      "(Iteration 5141 / 7640) loss: 3.905173\n",
      "(Iteration 5151 / 7640) loss: 3.771650\n",
      "(Iteration 5161 / 7640) loss: 3.681130\n",
      "(Iteration 5171 / 7640) loss: 3.908340\n",
      "(Iteration 5181 / 7640) loss: 3.811883\n",
      "(Iteration 5191 / 7640) loss: 3.823489\n",
      "(Iteration 5201 / 7640) loss: 3.924521\n",
      "(Iteration 5211 / 7640) loss: 3.763978\n",
      "(Iteration 5221 / 7640) loss: 3.797674\n",
      "(Iteration 5231 / 7640) loss: 3.800488\n",
      "(Iteration 5241 / 7640) loss: 3.936870\n",
      "(Iteration 5251 / 7640) loss: 3.854951\n",
      "(Iteration 5261 / 7640) loss: 3.958069\n",
      "(Iteration 5271 / 7640) loss: 4.031187\n",
      "(Iteration 5281 / 7640) loss: 3.677812\n",
      "(Iteration 5291 / 7640) loss: 3.816528\n",
      "(Iteration 5301 / 7640) loss: 3.774887\n",
      "(Iteration 5311 / 7640) loss: 3.978794\n",
      "(Iteration 5321 / 7640) loss: 3.808487\n",
      "(Iteration 5331 / 7640) loss: 3.725797\n",
      "(Iteration 5341 / 7640) loss: 3.844092\n",
      "(Epoch 14 / 20) train acc: 0.679000; val_acc: 0.545000\n",
      "(Iteration 5351 / 7640) loss: 3.812754\n",
      "(Iteration 5361 / 7640) loss: 3.892289\n",
      "(Iteration 5371 / 7640) loss: 3.775164\n",
      "(Iteration 5381 / 7640) loss: 3.999001\n",
      "(Iteration 5391 / 7640) loss: 3.668059\n",
      "(Iteration 5401 / 7640) loss: 3.915475\n",
      "(Iteration 5411 / 7640) loss: 3.887983\n",
      "(Iteration 5421 / 7640) loss: 3.944221\n",
      "(Iteration 5431 / 7640) loss: 3.881935\n",
      "(Iteration 5441 / 7640) loss: 3.832880\n",
      "(Iteration 5451 / 7640) loss: 3.994943\n",
      "(Iteration 5461 / 7640) loss: 3.848772\n",
      "(Iteration 5471 / 7640) loss: 3.831838\n",
      "(Iteration 5481 / 7640) loss: 3.757318\n",
      "(Iteration 5491 / 7640) loss: 3.861671\n",
      "(Iteration 5501 / 7640) loss: 3.850449\n",
      "(Iteration 5511 / 7640) loss: 4.030473\n",
      "(Iteration 5521 / 7640) loss: 3.798549\n",
      "(Iteration 5531 / 7640) loss: 3.758593\n",
      "(Iteration 5541 / 7640) loss: 3.799247\n",
      "(Iteration 5551 / 7640) loss: 3.950167\n",
      "(Iteration 5561 / 7640) loss: 3.727501\n",
      "(Iteration 5571 / 7640) loss: 3.835386\n",
      "(Iteration 5581 / 7640) loss: 3.722050\n",
      "(Iteration 5591 / 7640) loss: 3.901592\n",
      "(Iteration 5601 / 7640) loss: 3.854663\n",
      "(Iteration 5611 / 7640) loss: 3.999753\n",
      "(Iteration 5621 / 7640) loss: 3.765719\n",
      "(Iteration 5631 / 7640) loss: 3.697050\n",
      "(Iteration 5641 / 7640) loss: 3.896933\n",
      "(Iteration 5651 / 7640) loss: 3.711670\n",
      "(Iteration 5661 / 7640) loss: 3.727438\n",
      "(Iteration 5671 / 7640) loss: 3.915907\n",
      "(Iteration 5681 / 7640) loss: 4.121099\n",
      "(Iteration 5691 / 7640) loss: 3.632314\n",
      "(Iteration 5701 / 7640) loss: 3.784162\n",
      "(Iteration 5711 / 7640) loss: 3.937406\n",
      "(Iteration 5721 / 7640) loss: 3.880233\n",
      "(Epoch 15 / 20) train acc: 0.677000; val_acc: 0.559000\n",
      "(Iteration 5731 / 7640) loss: 4.039267\n",
      "(Iteration 5741 / 7640) loss: 3.894851\n",
      "(Iteration 5751 / 7640) loss: 3.912214\n",
      "(Iteration 5761 / 7640) loss: 3.879221\n",
      "(Iteration 5771 / 7640) loss: 3.798417\n",
      "(Iteration 5781 / 7640) loss: 3.773726\n",
      "(Iteration 5791 / 7640) loss: 3.855609\n",
      "(Iteration 5801 / 7640) loss: 3.730598\n",
      "(Iteration 5811 / 7640) loss: 3.938309\n",
      "(Iteration 5821 / 7640) loss: 3.763105\n",
      "(Iteration 5831 / 7640) loss: 3.750607\n",
      "(Iteration 5841 / 7640) loss: 3.897703\n",
      "(Iteration 5851 / 7640) loss: 3.654139\n",
      "(Iteration 5861 / 7640) loss: 3.793570\n",
      "(Iteration 5871 / 7640) loss: 3.740635\n",
      "(Iteration 5881 / 7640) loss: 3.772003\n",
      "(Iteration 5891 / 7640) loss: 3.658025\n",
      "(Iteration 5901 / 7640) loss: 3.895784\n",
      "(Iteration 5911 / 7640) loss: 3.918668\n",
      "(Iteration 5921 / 7640) loss: 3.708765\n",
      "(Iteration 5931 / 7640) loss: 3.828312\n",
      "(Iteration 5941 / 7640) loss: 3.992619\n",
      "(Iteration 5951 / 7640) loss: 3.853725\n",
      "(Iteration 5961 / 7640) loss: 3.708764\n",
      "(Iteration 5971 / 7640) loss: 3.874326\n",
      "(Iteration 5981 / 7640) loss: 3.795389\n",
      "(Iteration 5991 / 7640) loss: 3.726637\n",
      "(Iteration 6001 / 7640) loss: 3.981051\n",
      "(Iteration 6011 / 7640) loss: 3.846829\n",
      "(Iteration 6021 / 7640) loss: 3.957013\n",
      "(Iteration 6031 / 7640) loss: 3.935615\n",
      "(Iteration 6041 / 7640) loss: 3.709378\n",
      "(Iteration 6051 / 7640) loss: 3.724796\n",
      "(Iteration 6061 / 7640) loss: 3.792214\n",
      "(Iteration 6071 / 7640) loss: 3.708237\n",
      "(Iteration 6081 / 7640) loss: 3.731766\n",
      "(Iteration 6091 / 7640) loss: 3.894035\n",
      "(Iteration 6101 / 7640) loss: 3.755826\n",
      "(Iteration 6111 / 7640) loss: 3.808966\n",
      "(Epoch 16 / 20) train acc: 0.672000; val_acc: 0.547000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 6121 / 7640) loss: 3.905311\n",
      "(Iteration 6131 / 7640) loss: 3.864321\n",
      "(Iteration 6141 / 7640) loss: 3.867075\n",
      "(Iteration 6151 / 7640) loss: 3.911498\n",
      "(Iteration 6161 / 7640) loss: 3.767213\n",
      "(Iteration 6171 / 7640) loss: 3.739729\n",
      "(Iteration 6181 / 7640) loss: 3.823261\n",
      "(Iteration 6191 / 7640) loss: 3.733659\n",
      "(Iteration 6201 / 7640) loss: 3.747791\n",
      "(Iteration 6211 / 7640) loss: 3.887680\n",
      "(Iteration 6221 / 7640) loss: 3.801760\n",
      "(Iteration 6231 / 7640) loss: 3.711970\n",
      "(Iteration 6241 / 7640) loss: 3.642928\n",
      "(Iteration 6251 / 7640) loss: 3.790629\n",
      "(Iteration 6261 / 7640) loss: 3.825347\n",
      "(Iteration 6271 / 7640) loss: 3.751364\n",
      "(Iteration 6281 / 7640) loss: 3.829956\n",
      "(Iteration 6291 / 7640) loss: 3.634571\n",
      "(Iteration 6301 / 7640) loss: 3.713797\n",
      "(Iteration 6311 / 7640) loss: 3.963539\n",
      "(Iteration 6321 / 7640) loss: 3.707591\n",
      "(Iteration 6331 / 7640) loss: 3.824837\n",
      "(Iteration 6341 / 7640) loss: 3.630195\n",
      "(Iteration 6351 / 7640) loss: 3.719026\n",
      "(Iteration 6361 / 7640) loss: 3.753996\n",
      "(Iteration 6371 / 7640) loss: 3.730258\n",
      "(Iteration 6381 / 7640) loss: 3.684693\n",
      "(Iteration 6391 / 7640) loss: 3.782181\n",
      "(Iteration 6401 / 7640) loss: 3.714690\n",
      "(Iteration 6411 / 7640) loss: 3.683796\n",
      "(Iteration 6421 / 7640) loss: 3.810061\n",
      "(Iteration 6431 / 7640) loss: 3.776107\n",
      "(Iteration 6441 / 7640) loss: 3.650697\n",
      "(Iteration 6451 / 7640) loss: 3.747701\n",
      "(Iteration 6461 / 7640) loss: 3.687047\n",
      "(Iteration 6471 / 7640) loss: 3.902651\n",
      "(Iteration 6481 / 7640) loss: 3.869099\n",
      "(Iteration 6491 / 7640) loss: 3.822265\n",
      "(Epoch 17 / 20) train acc: 0.689000; val_acc: 0.536000\n",
      "(Iteration 6501 / 7640) loss: 3.682573\n",
      "(Iteration 6511 / 7640) loss: 3.438963\n",
      "(Iteration 6521 / 7640) loss: 3.873435\n",
      "(Iteration 6531 / 7640) loss: 3.847055\n",
      "(Iteration 6541 / 7640) loss: 3.641499\n",
      "(Iteration 6551 / 7640) loss: 3.807996\n",
      "(Iteration 6561 / 7640) loss: 3.613259\n",
      "(Iteration 6571 / 7640) loss: 3.638433\n",
      "(Iteration 6581 / 7640) loss: 3.714931\n",
      "(Iteration 6591 / 7640) loss: 3.682620\n",
      "(Iteration 6601 / 7640) loss: 3.783569\n",
      "(Iteration 6611 / 7640) loss: 3.792036\n",
      "(Iteration 6621 / 7640) loss: 3.577690\n",
      "(Iteration 6631 / 7640) loss: 3.738676\n",
      "(Iteration 6641 / 7640) loss: 3.655378\n",
      "(Iteration 6651 / 7640) loss: 3.749051\n",
      "(Iteration 6661 / 7640) loss: 3.748178\n",
      "(Iteration 6671 / 7640) loss: 3.853572\n",
      "(Iteration 6681 / 7640) loss: 3.570016\n",
      "(Iteration 6691 / 7640) loss: 3.769189\n",
      "(Iteration 6701 / 7640) loss: 3.716479\n",
      "(Iteration 6711 / 7640) loss: 3.609445\n",
      "(Iteration 6721 / 7640) loss: 3.577348\n",
      "(Iteration 6731 / 7640) loss: 3.738854\n",
      "(Iteration 6741 / 7640) loss: 3.718210\n",
      "(Iteration 6751 / 7640) loss: 3.775969\n",
      "(Iteration 6761 / 7640) loss: 3.884933\n",
      "(Iteration 6771 / 7640) loss: 3.522348\n",
      "(Iteration 6781 / 7640) loss: 3.670728\n",
      "(Iteration 6791 / 7640) loss: 3.901861\n",
      "(Iteration 6801 / 7640) loss: 3.719170\n",
      "(Iteration 6811 / 7640) loss: 3.662484\n",
      "(Iteration 6821 / 7640) loss: 3.609630\n",
      "(Iteration 6831 / 7640) loss: 3.547892\n",
      "(Iteration 6841 / 7640) loss: 3.648803\n",
      "(Iteration 6851 / 7640) loss: 3.837134\n",
      "(Iteration 6861 / 7640) loss: 3.835260\n",
      "(Iteration 6871 / 7640) loss: 3.807427\n",
      "(Epoch 18 / 20) train acc: 0.685000; val_acc: 0.547000\n",
      "(Iteration 6881 / 7640) loss: 3.731812\n",
      "(Iteration 6891 / 7640) loss: 3.683213\n",
      "(Iteration 6901 / 7640) loss: 3.684342\n",
      "(Iteration 6911 / 7640) loss: 3.688990\n",
      "(Iteration 6921 / 7640) loss: 3.837936\n",
      "(Iteration 6931 / 7640) loss: 3.631064\n",
      "(Iteration 6941 / 7640) loss: 3.755310\n",
      "(Iteration 6951 / 7640) loss: 3.619348\n",
      "(Iteration 6961 / 7640) loss: 3.656600\n",
      "(Iteration 6971 / 7640) loss: 3.713002\n",
      "(Iteration 6981 / 7640) loss: 3.652300\n",
      "(Iteration 6991 / 7640) loss: 3.907663\n",
      "(Iteration 7001 / 7640) loss: 3.654220\n",
      "(Iteration 7011 / 7640) loss: 3.754843\n",
      "(Iteration 7021 / 7640) loss: 3.619476\n",
      "(Iteration 7031 / 7640) loss: 3.795187\n",
      "(Iteration 7041 / 7640) loss: 3.608649\n",
      "(Iteration 7051 / 7640) loss: 3.673806\n",
      "(Iteration 7061 / 7640) loss: 3.746231\n",
      "(Iteration 7071 / 7640) loss: 3.561176\n",
      "(Iteration 7081 / 7640) loss: 3.727836\n",
      "(Iteration 7091 / 7640) loss: 3.523737\n",
      "(Iteration 7101 / 7640) loss: 3.585962\n",
      "(Iteration 7111 / 7640) loss: 3.812154\n",
      "(Iteration 7121 / 7640) loss: 3.598112\n",
      "(Iteration 7131 / 7640) loss: 3.671860\n",
      "(Iteration 7141 / 7640) loss: 3.829600\n",
      "(Iteration 7151 / 7640) loss: 3.743558\n",
      "(Iteration 7161 / 7640) loss: 3.960684\n",
      "(Iteration 7171 / 7640) loss: 3.677851\n",
      "(Iteration 7181 / 7640) loss: 3.675850\n",
      "(Iteration 7191 / 7640) loss: 3.487586\n",
      "(Iteration 7201 / 7640) loss: 3.700794\n",
      "(Iteration 7211 / 7640) loss: 3.675309\n",
      "(Iteration 7221 / 7640) loss: 3.561634\n",
      "(Iteration 7231 / 7640) loss: 3.793947\n",
      "(Iteration 7241 / 7640) loss: 3.619980\n",
      "(Iteration 7251 / 7640) loss: 3.779695\n",
      "(Epoch 19 / 20) train acc: 0.697000; val_acc: 0.551000\n",
      "(Iteration 7261 / 7640) loss: 3.671466\n",
      "(Iteration 7271 / 7640) loss: 3.737434\n",
      "(Iteration 7281 / 7640) loss: 3.695567\n",
      "(Iteration 7291 / 7640) loss: 3.565178\n",
      "(Iteration 7301 / 7640) loss: 3.694526\n",
      "(Iteration 7311 / 7640) loss: 3.552139\n",
      "(Iteration 7321 / 7640) loss: 3.571887\n",
      "(Iteration 7331 / 7640) loss: 3.648945\n",
      "(Iteration 7341 / 7640) loss: 3.730033\n",
      "(Iteration 7351 / 7640) loss: 3.727459\n",
      "(Iteration 7361 / 7640) loss: 3.644819\n",
      "(Iteration 7371 / 7640) loss: 3.779619\n",
      "(Iteration 7381 / 7640) loss: 3.796860\n",
      "(Iteration 7391 / 7640) loss: 3.684288\n",
      "(Iteration 7401 / 7640) loss: 3.524346\n",
      "(Iteration 7411 / 7640) loss: 3.665521\n",
      "(Iteration 7421 / 7640) loss: 3.737644\n",
      "(Iteration 7431 / 7640) loss: 3.695697\n",
      "(Iteration 7441 / 7640) loss: 3.605846\n",
      "(Iteration 7451 / 7640) loss: 3.435989\n",
      "(Iteration 7461 / 7640) loss: 3.566885\n",
      "(Iteration 7471 / 7640) loss: 3.761698\n",
      "(Iteration 7481 / 7640) loss: 3.741171\n",
      "(Iteration 7491 / 7640) loss: 3.741593\n",
      "(Iteration 7501 / 7640) loss: 3.546135\n",
      "(Iteration 7511 / 7640) loss: 3.645990\n",
      "(Iteration 7521 / 7640) loss: 3.504182\n",
      "(Iteration 7531 / 7640) loss: 3.510258\n",
      "(Iteration 7541 / 7640) loss: 3.515956\n",
      "(Iteration 7551 / 7640) loss: 3.641303\n",
      "(Iteration 7561 / 7640) loss: 3.605436\n",
      "(Iteration 7571 / 7640) loss: 3.507202\n",
      "(Iteration 7581 / 7640) loss: 3.651502\n",
      "(Iteration 7591 / 7640) loss: 3.759426\n",
      "(Iteration 7601 / 7640) loss: 3.557307\n",
      "(Iteration 7611 / 7640) loss: 3.581411\n",
      "(Iteration 7621 / 7640) loss: 3.709913\n",
      "(Iteration 7631 / 7640) loss: 3.641246\n",
      "(Epoch 20 / 20) train acc: 0.723000; val_acc: 0.540000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.559000\n",
      "running with  nesterov_momentum\n",
      "(Iteration 1 / 7640) loss: 6.315302\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.113000\n",
      "(Iteration 11 / 7640) loss: 6.177902\n",
      "(Iteration 21 / 7640) loss: 6.205974\n",
      "(Iteration 31 / 7640) loss: 6.132755\n",
      "(Iteration 41 / 7640) loss: 6.194372\n",
      "(Iteration 51 / 7640) loss: 6.101364\n",
      "(Iteration 61 / 7640) loss: 6.050670\n",
      "(Iteration 71 / 7640) loss: 5.995424\n",
      "(Iteration 81 / 7640) loss: 5.920534\n",
      "(Iteration 91 / 7640) loss: 5.905677\n",
      "(Iteration 101 / 7640) loss: 5.731485\n",
      "(Iteration 111 / 7640) loss: 5.859728\n",
      "(Iteration 121 / 7640) loss: 5.669906\n",
      "(Iteration 131 / 7640) loss: 5.615639\n",
      "(Iteration 141 / 7640) loss: 5.521492\n",
      "(Iteration 151 / 7640) loss: 5.621921\n",
      "(Iteration 161 / 7640) loss: 5.636288\n",
      "(Iteration 171 / 7640) loss: 5.529523\n",
      "(Iteration 181 / 7640) loss: 5.528864\n",
      "(Iteration 191 / 7640) loss: 5.695002\n",
      "(Iteration 201 / 7640) loss: 5.479901\n",
      "(Iteration 211 / 7640) loss: 5.370651\n",
      "(Iteration 221 / 7640) loss: 5.315791\n",
      "(Iteration 231 / 7640) loss: 5.450486\n",
      "(Iteration 241 / 7640) loss: 5.472633\n",
      "(Iteration 251 / 7640) loss: 5.348199\n",
      "(Iteration 261 / 7640) loss: 5.428883\n",
      "(Iteration 271 / 7640) loss: 5.418012\n",
      "(Iteration 281 / 7640) loss: 5.281698\n",
      "(Iteration 291 / 7640) loss: 5.429232\n",
      "(Iteration 301 / 7640) loss: 5.453267\n",
      "(Iteration 311 / 7640) loss: 5.310193\n",
      "(Iteration 321 / 7640) loss: 5.205071\n",
      "(Iteration 331 / 7640) loss: 5.161891\n",
      "(Iteration 341 / 7640) loss: 5.083022\n",
      "(Iteration 351 / 7640) loss: 5.350549\n",
      "(Iteration 361 / 7640) loss: 5.099972\n",
      "(Iteration 371 / 7640) loss: 5.319357\n",
      "(Iteration 381 / 7640) loss: 5.175088\n",
      "(Epoch 1 / 20) train acc: 0.405000; val_acc: 0.424000\n",
      "(Iteration 391 / 7640) loss: 5.191397\n",
      "(Iteration 401 / 7640) loss: 5.154962\n",
      "(Iteration 411 / 7640) loss: 5.087269\n",
      "(Iteration 421 / 7640) loss: 5.186862\n",
      "(Iteration 431 / 7640) loss: 5.097193\n",
      "(Iteration 441 / 7640) loss: 5.265366\n",
      "(Iteration 451 / 7640) loss: 5.066786\n",
      "(Iteration 461 / 7640) loss: 5.032184\n",
      "(Iteration 471 / 7640) loss: 4.974957\n",
      "(Iteration 481 / 7640) loss: 5.163963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 491 / 7640) loss: 4.976082\n",
      "(Iteration 501 / 7640) loss: 5.178561\n",
      "(Iteration 511 / 7640) loss: 5.021385\n",
      "(Iteration 521 / 7640) loss: 4.878282\n",
      "(Iteration 531 / 7640) loss: 5.019150\n",
      "(Iteration 541 / 7640) loss: 4.954621\n",
      "(Iteration 551 / 7640) loss: 4.932543\n",
      "(Iteration 561 / 7640) loss: 4.941294\n",
      "(Iteration 571 / 7640) loss: 4.926579\n",
      "(Iteration 581 / 7640) loss: 4.911117\n",
      "(Iteration 591 / 7640) loss: 4.789061\n",
      "(Iteration 601 / 7640) loss: 4.964429\n",
      "(Iteration 611 / 7640) loss: 4.925410\n",
      "(Iteration 621 / 7640) loss: 4.944512\n",
      "(Iteration 631 / 7640) loss: 4.806427\n",
      "(Iteration 641 / 7640) loss: 4.678972\n",
      "(Iteration 651 / 7640) loss: 4.797991\n",
      "(Iteration 661 / 7640) loss: 4.786522\n",
      "(Iteration 671 / 7640) loss: 4.620381\n",
      "(Iteration 681 / 7640) loss: 4.914708\n",
      "(Iteration 691 / 7640) loss: 4.766641\n",
      "(Iteration 701 / 7640) loss: 4.932067\n",
      "(Iteration 711 / 7640) loss: 4.822179\n",
      "(Iteration 721 / 7640) loss: 4.891250\n",
      "(Iteration 731 / 7640) loss: 4.640585\n",
      "(Iteration 741 / 7640) loss: 4.913539\n",
      "(Iteration 751 / 7640) loss: 4.760010\n",
      "(Iteration 761 / 7640) loss: 4.811864\n",
      "(Epoch 2 / 20) train acc: 0.504000; val_acc: 0.468000\n",
      "(Iteration 771 / 7640) loss: 4.709046\n",
      "(Iteration 781 / 7640) loss: 4.696926\n",
      "(Iteration 791 / 7640) loss: 4.611852\n",
      "(Iteration 801 / 7640) loss: 4.755508\n",
      "(Iteration 811 / 7640) loss: 4.873887\n",
      "(Iteration 821 / 7640) loss: 4.740709\n",
      "(Iteration 831 / 7640) loss: 4.779980\n",
      "(Iteration 841 / 7640) loss: 4.610861\n",
      "(Iteration 851 / 7640) loss: 4.776323\n",
      "(Iteration 861 / 7640) loss: 4.767116\n",
      "(Iteration 871 / 7640) loss: 4.582464\n",
      "(Iteration 881 / 7640) loss: 4.580819\n",
      "(Iteration 891 / 7640) loss: 4.667188\n",
      "(Iteration 901 / 7640) loss: 4.661416\n",
      "(Iteration 911 / 7640) loss: 4.522254\n",
      "(Iteration 921 / 7640) loss: 4.388092\n",
      "(Iteration 931 / 7640) loss: 4.414759\n",
      "(Iteration 941 / 7640) loss: 4.623679\n",
      "(Iteration 951 / 7640) loss: 4.518509\n",
      "(Iteration 961 / 7640) loss: 4.490851\n",
      "(Iteration 971 / 7640) loss: 4.771703\n",
      "(Iteration 981 / 7640) loss: 4.652524\n",
      "(Iteration 991 / 7640) loss: 4.578698\n",
      "(Iteration 1001 / 7640) loss: 4.676859\n",
      "(Iteration 1011 / 7640) loss: 4.466028\n",
      "(Iteration 1021 / 7640) loss: 4.484775\n",
      "(Iteration 1031 / 7640) loss: 4.526858\n",
      "(Iteration 1041 / 7640) loss: 4.422397\n",
      "(Iteration 1051 / 7640) loss: 4.486586\n",
      "(Iteration 1061 / 7640) loss: 4.592659\n",
      "(Iteration 1071 / 7640) loss: 4.464479\n",
      "(Iteration 1081 / 7640) loss: 4.637046\n",
      "(Iteration 1091 / 7640) loss: 4.564198\n",
      "(Iteration 1101 / 7640) loss: 4.513213\n",
      "(Iteration 1111 / 7640) loss: 4.430854\n",
      "(Iteration 1121 / 7640) loss: 4.381304\n",
      "(Iteration 1131 / 7640) loss: 4.331824\n",
      "(Iteration 1141 / 7640) loss: 4.551285\n",
      "(Epoch 3 / 20) train acc: 0.540000; val_acc: 0.511000\n",
      "(Iteration 1151 / 7640) loss: 4.423131\n",
      "(Iteration 1161 / 7640) loss: 4.468346\n",
      "(Iteration 1171 / 7640) loss: 4.333552\n",
      "(Iteration 1181 / 7640) loss: 4.536274\n",
      "(Iteration 1191 / 7640) loss: 4.306399\n",
      "(Iteration 1201 / 7640) loss: 4.427351\n",
      "(Iteration 1211 / 7640) loss: 4.238831\n",
      "(Iteration 1221 / 7640) loss: 4.132659\n",
      "(Iteration 1231 / 7640) loss: 4.368538\n",
      "(Iteration 1241 / 7640) loss: 4.301658\n",
      "(Iteration 1251 / 7640) loss: 4.184104\n",
      "(Iteration 1261 / 7640) loss: 4.272324\n",
      "(Iteration 1271 / 7640) loss: 4.257640\n",
      "(Iteration 1281 / 7640) loss: 4.283274\n",
      "(Iteration 1291 / 7640) loss: 4.278421\n",
      "(Iteration 1301 / 7640) loss: 4.191969\n",
      "(Iteration 1311 / 7640) loss: 4.455260\n",
      "(Iteration 1321 / 7640) loss: 4.326127\n",
      "(Iteration 1331 / 7640) loss: 4.344713\n",
      "(Iteration 1341 / 7640) loss: 4.117480\n",
      "(Iteration 1351 / 7640) loss: 4.267013\n",
      "(Iteration 1361 / 7640) loss: 4.189584\n",
      "(Iteration 1371 / 7640) loss: 4.262163\n",
      "(Iteration 1381 / 7640) loss: 4.146730\n",
      "(Iteration 1391 / 7640) loss: 4.198635\n",
      "(Iteration 1401 / 7640) loss: 4.137527\n",
      "(Iteration 1411 / 7640) loss: 4.151031\n",
      "(Iteration 1421 / 7640) loss: 4.045353\n",
      "(Iteration 1431 / 7640) loss: 4.242300\n",
      "(Iteration 1441 / 7640) loss: 4.180361\n",
      "(Iteration 1451 / 7640) loss: 4.038106\n",
      "(Iteration 1461 / 7640) loss: 4.209967\n",
      "(Iteration 1471 / 7640) loss: 4.135371\n",
      "(Iteration 1481 / 7640) loss: 4.179552\n",
      "(Iteration 1491 / 7640) loss: 3.984282\n",
      "(Iteration 1501 / 7640) loss: 3.960741\n",
      "(Iteration 1511 / 7640) loss: 4.037900\n",
      "(Iteration 1521 / 7640) loss: 4.131740\n",
      "(Epoch 4 / 20) train acc: 0.586000; val_acc: 0.510000\n",
      "(Iteration 1531 / 7640) loss: 4.289962\n",
      "(Iteration 1541 / 7640) loss: 3.935011\n",
      "(Iteration 1551 / 7640) loss: 4.101921\n",
      "(Iteration 1561 / 7640) loss: 4.109723\n",
      "(Iteration 1571 / 7640) loss: 4.097656\n",
      "(Iteration 1581 / 7640) loss: 3.951744\n",
      "(Iteration 1591 / 7640) loss: 4.107028\n",
      "(Iteration 1601 / 7640) loss: 4.121361\n",
      "(Iteration 1611 / 7640) loss: 3.932122\n",
      "(Iteration 1621 / 7640) loss: 4.006149\n",
      "(Iteration 1631 / 7640) loss: 3.861950\n",
      "(Iteration 1641 / 7640) loss: 4.183551\n",
      "(Iteration 1651 / 7640) loss: 3.859078\n",
      "(Iteration 1661 / 7640) loss: 3.937565\n",
      "(Iteration 1671 / 7640) loss: 3.976935\n",
      "(Iteration 1681 / 7640) loss: 3.811262\n",
      "(Iteration 1691 / 7640) loss: 3.835546\n",
      "(Iteration 1701 / 7640) loss: 4.148801\n",
      "(Iteration 1711 / 7640) loss: 4.042619\n",
      "(Iteration 1721 / 7640) loss: 4.048088\n",
      "(Iteration 1731 / 7640) loss: 3.996874\n",
      "(Iteration 1741 / 7640) loss: 3.924535\n",
      "(Iteration 1751 / 7640) loss: 4.058136\n",
      "(Iteration 1761 / 7640) loss: 4.018653\n",
      "(Iteration 1771 / 7640) loss: 4.070116\n",
      "(Iteration 1781 / 7640) loss: 4.013409\n",
      "(Iteration 1791 / 7640) loss: 4.061716\n",
      "(Iteration 1801 / 7640) loss: 4.078161\n",
      "(Iteration 1811 / 7640) loss: 3.749122\n",
      "(Iteration 1821 / 7640) loss: 3.868363\n",
      "(Iteration 1831 / 7640) loss: 3.974457\n",
      "(Iteration 1841 / 7640) loss: 3.880327\n",
      "(Iteration 1851 / 7640) loss: 3.830752\n",
      "(Iteration 1861 / 7640) loss: 3.809118\n",
      "(Iteration 1871 / 7640) loss: 3.915848\n",
      "(Iteration 1881 / 7640) loss: 4.032459\n",
      "(Iteration 1891 / 7640) loss: 3.745708\n",
      "(Iteration 1901 / 7640) loss: 4.141390\n",
      "(Epoch 5 / 20) train acc: 0.581000; val_acc: 0.531000\n",
      "(Iteration 1911 / 7640) loss: 3.754155\n",
      "(Iteration 1921 / 7640) loss: 3.827395\n",
      "(Iteration 1931 / 7640) loss: 3.907612\n",
      "(Iteration 1941 / 7640) loss: 3.871453\n",
      "(Iteration 1951 / 7640) loss: 3.733017\n",
      "(Iteration 1961 / 7640) loss: 3.706664\n",
      "(Iteration 1971 / 7640) loss: 3.770958\n",
      "(Iteration 1981 / 7640) loss: 3.732698\n",
      "(Iteration 1991 / 7640) loss: 3.865941\n",
      "(Iteration 2001 / 7640) loss: 3.761140\n",
      "(Iteration 2011 / 7640) loss: 3.842572\n",
      "(Iteration 2021 / 7640) loss: 3.995000\n",
      "(Iteration 2031 / 7640) loss: 3.886748\n",
      "(Iteration 2041 / 7640) loss: 3.645659\n",
      "(Iteration 2051 / 7640) loss: 3.848947\n",
      "(Iteration 2061 / 7640) loss: 3.789097\n",
      "(Iteration 2071 / 7640) loss: 3.689471\n",
      "(Iteration 2081 / 7640) loss: 3.793057\n",
      "(Iteration 2091 / 7640) loss: 3.555708\n",
      "(Iteration 2101 / 7640) loss: 3.860162\n",
      "(Iteration 2111 / 7640) loss: 3.928312\n",
      "(Iteration 2121 / 7640) loss: 3.566107\n",
      "(Iteration 2131 / 7640) loss: 3.433860\n",
      "(Iteration 2141 / 7640) loss: 3.726590\n",
      "(Iteration 2151 / 7640) loss: 3.611344\n",
      "(Iteration 2161 / 7640) loss: 3.685391\n",
      "(Iteration 2171 / 7640) loss: 3.784565\n",
      "(Iteration 2181 / 7640) loss: 3.729990\n",
      "(Iteration 2191 / 7640) loss: 3.686771\n",
      "(Iteration 2201 / 7640) loss: 3.779384\n",
      "(Iteration 2211 / 7640) loss: 3.735748\n",
      "(Iteration 2221 / 7640) loss: 3.861767\n",
      "(Iteration 2231 / 7640) loss: 3.732209\n",
      "(Iteration 2241 / 7640) loss: 3.512552\n",
      "(Iteration 2251 / 7640) loss: 3.634135\n",
      "(Iteration 2261 / 7640) loss: 3.636589\n",
      "(Iteration 2271 / 7640) loss: 3.495205\n",
      "(Iteration 2281 / 7640) loss: 3.674558\n",
      "(Iteration 2291 / 7640) loss: 3.775645\n",
      "(Epoch 6 / 20) train acc: 0.599000; val_acc: 0.523000\n",
      "(Iteration 2301 / 7640) loss: 3.534122\n",
      "(Iteration 2311 / 7640) loss: 3.553419\n",
      "(Iteration 2321 / 7640) loss: 3.772427\n",
      "(Iteration 2331 / 7640) loss: 3.491704\n",
      "(Iteration 2341 / 7640) loss: 3.368145\n",
      "(Iteration 2351 / 7640) loss: 3.570097\n",
      "(Iteration 2361 / 7640) loss: 3.471140\n",
      "(Iteration 2371 / 7640) loss: 3.535202\n",
      "(Iteration 2381 / 7640) loss: 3.554009\n",
      "(Iteration 2391 / 7640) loss: 3.549493\n",
      "(Iteration 2401 / 7640) loss: 3.441791\n",
      "(Iteration 2411 / 7640) loss: 3.383010\n",
      "(Iteration 2421 / 7640) loss: 3.489780\n",
      "(Iteration 2431 / 7640) loss: 3.451093\n",
      "(Iteration 2441 / 7640) loss: 3.697266\n",
      "(Iteration 2451 / 7640) loss: 3.372656\n",
      "(Iteration 2461 / 7640) loss: 3.569695\n",
      "(Iteration 2471 / 7640) loss: 3.426881\n",
      "(Iteration 2481 / 7640) loss: 3.476640\n",
      "(Iteration 2491 / 7640) loss: 3.390686\n",
      "(Iteration 2501 / 7640) loss: 3.496948\n",
      "(Iteration 2511 / 7640) loss: 3.306593\n",
      "(Iteration 2521 / 7640) loss: 3.590125\n",
      "(Iteration 2531 / 7640) loss: 3.459143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2541 / 7640) loss: 3.464868\n",
      "(Iteration 2551 / 7640) loss: 3.348394\n",
      "(Iteration 2561 / 7640) loss: 3.386951\n",
      "(Iteration 2571 / 7640) loss: 3.398321\n",
      "(Iteration 2581 / 7640) loss: 3.529358\n",
      "(Iteration 2591 / 7640) loss: 3.391899\n",
      "(Iteration 2601 / 7640) loss: 3.292836\n",
      "(Iteration 2611 / 7640) loss: 3.301637\n",
      "(Iteration 2621 / 7640) loss: 3.501766\n",
      "(Iteration 2631 / 7640) loss: 3.401378\n",
      "(Iteration 2641 / 7640) loss: 3.485311\n",
      "(Iteration 2651 / 7640) loss: 3.313614\n",
      "(Iteration 2661 / 7640) loss: 3.342846\n",
      "(Iteration 2671 / 7640) loss: 3.269164\n",
      "(Epoch 7 / 20) train acc: 0.648000; val_acc: 0.527000\n",
      "(Iteration 2681 / 7640) loss: 3.388337\n",
      "(Iteration 2691 / 7640) loss: 3.328015\n",
      "(Iteration 2701 / 7640) loss: 3.352845\n",
      "(Iteration 2711 / 7640) loss: 3.267931\n",
      "(Iteration 2721 / 7640) loss: 3.448904\n",
      "(Iteration 2731 / 7640) loss: 3.343867\n",
      "(Iteration 2741 / 7640) loss: 3.174457\n",
      "(Iteration 2751 / 7640) loss: 3.279701\n",
      "(Iteration 2761 / 7640) loss: 3.223278\n",
      "(Iteration 2771 / 7640) loss: 3.229375\n",
      "(Iteration 2781 / 7640) loss: 3.286660\n",
      "(Iteration 2791 / 7640) loss: 3.361344\n",
      "(Iteration 2801 / 7640) loss: 3.288032\n",
      "(Iteration 2811 / 7640) loss: 3.308577\n",
      "(Iteration 2821 / 7640) loss: 3.288844\n",
      "(Iteration 2831 / 7640) loss: 3.380807\n",
      "(Iteration 2841 / 7640) loss: 3.266489\n",
      "(Iteration 2851 / 7640) loss: 3.273729\n",
      "(Iteration 2861 / 7640) loss: 3.199437\n",
      "(Iteration 2871 / 7640) loss: 3.254283\n",
      "(Iteration 2881 / 7640) loss: 3.032813\n",
      "(Iteration 2891 / 7640) loss: 3.264921\n",
      "(Iteration 2901 / 7640) loss: 3.232558\n",
      "(Iteration 2911 / 7640) loss: 3.268885\n",
      "(Iteration 2921 / 7640) loss: 3.309232\n",
      "(Iteration 2931 / 7640) loss: 3.296246\n",
      "(Iteration 2941 / 7640) loss: 3.218845\n",
      "(Iteration 2951 / 7640) loss: 3.159236\n",
      "(Iteration 2961 / 7640) loss: 3.319462\n",
      "(Iteration 2971 / 7640) loss: 3.220781\n",
      "(Iteration 2981 / 7640) loss: 3.189788\n",
      "(Iteration 2991 / 7640) loss: 3.261945\n",
      "(Iteration 3001 / 7640) loss: 3.149003\n",
      "(Iteration 3011 / 7640) loss: 3.313282\n",
      "(Iteration 3021 / 7640) loss: 3.075341\n",
      "(Iteration 3031 / 7640) loss: 3.209537\n",
      "(Iteration 3041 / 7640) loss: 3.144852\n",
      "(Iteration 3051 / 7640) loss: 3.170683\n",
      "(Epoch 8 / 20) train acc: 0.683000; val_acc: 0.543000\n",
      "(Iteration 3061 / 7640) loss: 3.183633\n",
      "(Iteration 3071 / 7640) loss: 3.275124\n",
      "(Iteration 3081 / 7640) loss: 3.097407\n",
      "(Iteration 3091 / 7640) loss: 3.138528\n",
      "(Iteration 3101 / 7640) loss: 3.140315\n",
      "(Iteration 3111 / 7640) loss: 3.084092\n",
      "(Iteration 3121 / 7640) loss: 3.190459\n",
      "(Iteration 3131 / 7640) loss: 3.111873\n",
      "(Iteration 3141 / 7640) loss: 3.331883\n",
      "(Iteration 3151 / 7640) loss: 2.990286\n",
      "(Iteration 3161 / 7640) loss: 3.137127\n",
      "(Iteration 3171 / 7640) loss: 3.180017\n",
      "(Iteration 3181 / 7640) loss: 3.175145\n",
      "(Iteration 3191 / 7640) loss: 3.083799\n",
      "(Iteration 3201 / 7640) loss: 3.107153\n",
      "(Iteration 3211 / 7640) loss: 3.115338\n",
      "(Iteration 3221 / 7640) loss: 3.004142\n",
      "(Iteration 3231 / 7640) loss: 3.288488\n",
      "(Iteration 3241 / 7640) loss: 3.059381\n",
      "(Iteration 3251 / 7640) loss: 3.059799\n",
      "(Iteration 3261 / 7640) loss: 3.109928\n",
      "(Iteration 3271 / 7640) loss: 3.101636\n",
      "(Iteration 3281 / 7640) loss: 2.928765\n",
      "(Iteration 3291 / 7640) loss: 3.147158\n",
      "(Iteration 3301 / 7640) loss: 2.991382\n",
      "(Iteration 3311 / 7640) loss: 3.079723\n",
      "(Iteration 3321 / 7640) loss: 2.904199\n",
      "(Iteration 3331 / 7640) loss: 3.048228\n",
      "(Iteration 3341 / 7640) loss: 3.057445\n",
      "(Iteration 3351 / 7640) loss: 3.010442\n",
      "(Iteration 3361 / 7640) loss: 3.010451\n",
      "(Iteration 3371 / 7640) loss: 3.041019\n",
      "(Iteration 3381 / 7640) loss: 3.077577\n",
      "(Iteration 3391 / 7640) loss: 2.981475\n",
      "(Iteration 3401 / 7640) loss: 3.046606\n",
      "(Iteration 3411 / 7640) loss: 2.949852\n",
      "(Iteration 3421 / 7640) loss: 2.973478\n",
      "(Iteration 3431 / 7640) loss: 3.125512\n",
      "(Epoch 9 / 20) train acc: 0.706000; val_acc: 0.540000\n",
      "(Iteration 3441 / 7640) loss: 3.054134\n",
      "(Iteration 3451 / 7640) loss: 3.032589\n",
      "(Iteration 3461 / 7640) loss: 2.941262\n",
      "(Iteration 3471 / 7640) loss: 2.963399\n",
      "(Iteration 3481 / 7640) loss: 2.913471\n",
      "(Iteration 3491 / 7640) loss: 3.044136\n",
      "(Iteration 3501 / 7640) loss: 3.071360\n",
      "(Iteration 3511 / 7640) loss: 2.899685\n",
      "(Iteration 3521 / 7640) loss: 2.780471\n",
      "(Iteration 3531 / 7640) loss: 3.133153\n",
      "(Iteration 3541 / 7640) loss: 2.922562\n",
      "(Iteration 3551 / 7640) loss: 2.955915\n",
      "(Iteration 3561 / 7640) loss: 2.996995\n",
      "(Iteration 3571 / 7640) loss: 2.946282\n",
      "(Iteration 3581 / 7640) loss: 2.981687\n",
      "(Iteration 3591 / 7640) loss: 3.034925\n",
      "(Iteration 3601 / 7640) loss: 2.873959\n",
      "(Iteration 3611 / 7640) loss: 2.905830\n",
      "(Iteration 3621 / 7640) loss: 2.912149\n",
      "(Iteration 3631 / 7640) loss: 2.703642\n",
      "(Iteration 3641 / 7640) loss: 2.812699\n",
      "(Iteration 3651 / 7640) loss: 2.862550\n",
      "(Iteration 3661 / 7640) loss: 2.811947\n",
      "(Iteration 3671 / 7640) loss: 2.880202\n",
      "(Iteration 3681 / 7640) loss: 2.830344\n",
      "(Iteration 3691 / 7640) loss: 2.714809\n",
      "(Iteration 3701 / 7640) loss: 2.825657\n",
      "(Iteration 3711 / 7640) loss: 2.814091\n",
      "(Iteration 3721 / 7640) loss: 3.043601\n",
      "(Iteration 3731 / 7640) loss: 2.844566\n",
      "(Iteration 3741 / 7640) loss: 2.909877\n",
      "(Iteration 3751 / 7640) loss: 2.782961\n",
      "(Iteration 3761 / 7640) loss: 3.065987\n",
      "(Iteration 3771 / 7640) loss: 2.641316\n",
      "(Iteration 3781 / 7640) loss: 3.056922\n",
      "(Iteration 3791 / 7640) loss: 2.752134\n",
      "(Iteration 3801 / 7640) loss: 2.933071\n",
      "(Iteration 3811 / 7640) loss: 2.710115\n",
      "(Epoch 10 / 20) train acc: 0.693000; val_acc: 0.533000\n",
      "(Iteration 3821 / 7640) loss: 2.809130\n",
      "(Iteration 3831 / 7640) loss: 2.796743\n",
      "(Iteration 3841 / 7640) loss: 2.932414\n",
      "(Iteration 3851 / 7640) loss: 2.665416\n",
      "(Iteration 3861 / 7640) loss: 2.841297\n",
      "(Iteration 3871 / 7640) loss: 2.781518\n",
      "(Iteration 3881 / 7640) loss: 2.701515\n",
      "(Iteration 3891 / 7640) loss: 2.850115\n",
      "(Iteration 3901 / 7640) loss: 2.812214\n",
      "(Iteration 3911 / 7640) loss: 2.640543\n",
      "(Iteration 3921 / 7640) loss: 2.979684\n",
      "(Iteration 3931 / 7640) loss: 2.796019\n",
      "(Iteration 3941 / 7640) loss: 2.855911\n",
      "(Iteration 3951 / 7640) loss: 2.689445\n",
      "(Iteration 3961 / 7640) loss: 2.765826\n",
      "(Iteration 3971 / 7640) loss: 2.510289\n",
      "(Iteration 3981 / 7640) loss: 2.798169\n",
      "(Iteration 3991 / 7640) loss: 2.671475\n",
      "(Iteration 4001 / 7640) loss: 2.871465\n",
      "(Iteration 4011 / 7640) loss: 2.708476\n",
      "(Iteration 4021 / 7640) loss: 2.680506\n",
      "(Iteration 4031 / 7640) loss: 2.580767\n",
      "(Iteration 4041 / 7640) loss: 2.802728\n",
      "(Iteration 4051 / 7640) loss: 2.624717\n",
      "(Iteration 4061 / 7640) loss: 2.573289\n",
      "(Iteration 4071 / 7640) loss: 2.647015\n",
      "(Iteration 4081 / 7640) loss: 2.768010\n",
      "(Iteration 4091 / 7640) loss: 2.701672\n",
      "(Iteration 4101 / 7640) loss: 2.680288\n",
      "(Iteration 4111 / 7640) loss: 2.602523\n",
      "(Iteration 4121 / 7640) loss: 2.679483\n",
      "(Iteration 4131 / 7640) loss: 2.573469\n",
      "(Iteration 4141 / 7640) loss: 2.777969\n",
      "(Iteration 4151 / 7640) loss: 2.555383\n",
      "(Iteration 4161 / 7640) loss: 2.663665\n",
      "(Iteration 4171 / 7640) loss: 2.705920\n",
      "(Iteration 4181 / 7640) loss: 2.692556\n",
      "(Iteration 4191 / 7640) loss: 2.632722\n",
      "(Iteration 4201 / 7640) loss: 2.620138\n",
      "(Epoch 11 / 20) train acc: 0.722000; val_acc: 0.545000\n",
      "(Iteration 4211 / 7640) loss: 2.594044\n",
      "(Iteration 4221 / 7640) loss: 2.709222\n",
      "(Iteration 4231 / 7640) loss: 2.675383\n",
      "(Iteration 4241 / 7640) loss: 2.689879\n",
      "(Iteration 4251 / 7640) loss: 2.437153\n",
      "(Iteration 4261 / 7640) loss: 2.710719\n",
      "(Iteration 4271 / 7640) loss: 2.638680\n",
      "(Iteration 4281 / 7640) loss: 2.556264\n",
      "(Iteration 4291 / 7640) loss: 2.657833\n",
      "(Iteration 4301 / 7640) loss: 2.604630\n",
      "(Iteration 4311 / 7640) loss: 2.765809\n",
      "(Iteration 4321 / 7640) loss: 2.630931\n",
      "(Iteration 4331 / 7640) loss: 2.646743\n",
      "(Iteration 4341 / 7640) loss: 2.595055\n",
      "(Iteration 4351 / 7640) loss: 2.513391\n",
      "(Iteration 4361 / 7640) loss: 2.567772\n",
      "(Iteration 4371 / 7640) loss: 2.585015\n",
      "(Iteration 4381 / 7640) loss: 2.750366\n",
      "(Iteration 4391 / 7640) loss: 2.488611\n",
      "(Iteration 4401 / 7640) loss: 2.473682\n",
      "(Iteration 4411 / 7640) loss: 2.521473\n",
      "(Iteration 4421 / 7640) loss: 2.617737\n",
      "(Iteration 4431 / 7640) loss: 2.496608\n",
      "(Iteration 4441 / 7640) loss: 2.448395\n",
      "(Iteration 4451 / 7640) loss: 2.775053\n",
      "(Iteration 4461 / 7640) loss: 2.505979\n",
      "(Iteration 4471 / 7640) loss: 2.541816\n",
      "(Iteration 4481 / 7640) loss: 2.470056\n",
      "(Iteration 4491 / 7640) loss: 2.448453\n",
      "(Iteration 4501 / 7640) loss: 2.500664\n",
      "(Iteration 4511 / 7640) loss: 2.582519\n",
      "(Iteration 4521 / 7640) loss: 2.347899\n",
      "(Iteration 4531 / 7640) loss: 2.507837\n",
      "(Iteration 4541 / 7640) loss: 2.527231\n",
      "(Iteration 4551 / 7640) loss: 2.612255\n",
      "(Iteration 4561 / 7640) loss: 2.504547\n",
      "(Iteration 4571 / 7640) loss: 2.401691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4581 / 7640) loss: 2.434023\n",
      "(Epoch 12 / 20) train acc: 0.742000; val_acc: 0.552000\n",
      "(Iteration 4591 / 7640) loss: 2.356329\n",
      "(Iteration 4601 / 7640) loss: 2.471594\n",
      "(Iteration 4611 / 7640) loss: 2.556267\n",
      "(Iteration 4621 / 7640) loss: 2.647965\n",
      "(Iteration 4631 / 7640) loss: 2.467304\n",
      "(Iteration 4641 / 7640) loss: 2.464501\n",
      "(Iteration 4651 / 7640) loss: 2.353879\n",
      "(Iteration 4661 / 7640) loss: 2.429004\n",
      "(Iteration 4671 / 7640) loss: 2.359185\n",
      "(Iteration 4681 / 7640) loss: 2.351060\n",
      "(Iteration 4691 / 7640) loss: 2.416506\n",
      "(Iteration 4701 / 7640) loss: 2.506217\n",
      "(Iteration 4711 / 7640) loss: 2.420207\n",
      "(Iteration 4721 / 7640) loss: 2.313902\n",
      "(Iteration 4731 / 7640) loss: 2.461256\n",
      "(Iteration 4741 / 7640) loss: 2.324298\n",
      "(Iteration 4751 / 7640) loss: 2.368481\n",
      "(Iteration 4761 / 7640) loss: 2.387644\n",
      "(Iteration 4771 / 7640) loss: 2.431766\n",
      "(Iteration 4781 / 7640) loss: 2.458821\n",
      "(Iteration 4791 / 7640) loss: 2.370990\n",
      "(Iteration 4801 / 7640) loss: 2.516245\n",
      "(Iteration 4811 / 7640) loss: 2.389144\n",
      "(Iteration 4821 / 7640) loss: 2.322141\n",
      "(Iteration 4831 / 7640) loss: 2.236725\n",
      "(Iteration 4841 / 7640) loss: 2.577497\n",
      "(Iteration 4851 / 7640) loss: 2.420298\n",
      "(Iteration 4861 / 7640) loss: 2.530113\n",
      "(Iteration 4871 / 7640) loss: 2.467926\n",
      "(Iteration 4881 / 7640) loss: 2.327061\n",
      "(Iteration 4891 / 7640) loss: 2.344210\n",
      "(Iteration 4901 / 7640) loss: 2.278588\n",
      "(Iteration 4911 / 7640) loss: 2.336727\n",
      "(Iteration 4921 / 7640) loss: 2.400312\n",
      "(Iteration 4931 / 7640) loss: 2.282286\n",
      "(Iteration 4941 / 7640) loss: 2.316484\n",
      "(Iteration 4951 / 7640) loss: 2.199321\n",
      "(Iteration 4961 / 7640) loss: 2.288176\n",
      "(Epoch 13 / 20) train acc: 0.745000; val_acc: 0.547000\n",
      "(Iteration 4971 / 7640) loss: 2.318664\n",
      "(Iteration 4981 / 7640) loss: 2.283879\n",
      "(Iteration 4991 / 7640) loss: 2.386680\n",
      "(Iteration 5001 / 7640) loss: 2.328715\n",
      "(Iteration 5011 / 7640) loss: 2.325567\n",
      "(Iteration 5021 / 7640) loss: 2.280247\n",
      "(Iteration 5031 / 7640) loss: 2.421999\n",
      "(Iteration 5041 / 7640) loss: 2.299737\n",
      "(Iteration 5051 / 7640) loss: 2.159736\n",
      "(Iteration 5061 / 7640) loss: 2.294165\n",
      "(Iteration 5071 / 7640) loss: 2.263578\n",
      "(Iteration 5081 / 7640) loss: 2.264655\n",
      "(Iteration 5091 / 7640) loss: 2.302937\n",
      "(Iteration 5101 / 7640) loss: 2.331715\n",
      "(Iteration 5111 / 7640) loss: 2.140011\n",
      "(Iteration 5121 / 7640) loss: 2.299658\n",
      "(Iteration 5131 / 7640) loss: 2.312425\n",
      "(Iteration 5141 / 7640) loss: 2.306015\n",
      "(Iteration 5151 / 7640) loss: 2.251805\n",
      "(Iteration 5161 / 7640) loss: 2.256169\n",
      "(Iteration 5171 / 7640) loss: 2.297039\n",
      "(Iteration 5181 / 7640) loss: 2.215905\n",
      "(Iteration 5191 / 7640) loss: 2.189605\n",
      "(Iteration 5201 / 7640) loss: 2.215681\n",
      "(Iteration 5211 / 7640) loss: 2.400045\n",
      "(Iteration 5221 / 7640) loss: 2.264672\n",
      "(Iteration 5231 / 7640) loss: 2.361052\n",
      "(Iteration 5241 / 7640) loss: 2.230184\n",
      "(Iteration 5251 / 7640) loss: 2.202205\n",
      "(Iteration 5261 / 7640) loss: 2.325129\n",
      "(Iteration 5271 / 7640) loss: 2.208125\n",
      "(Iteration 5281 / 7640) loss: 2.115549\n",
      "(Iteration 5291 / 7640) loss: 2.193917\n",
      "(Iteration 5301 / 7640) loss: 2.355162\n",
      "(Iteration 5311 / 7640) loss: 2.336941\n",
      "(Iteration 5321 / 7640) loss: 2.202870\n",
      "(Iteration 5331 / 7640) loss: 2.253206\n",
      "(Iteration 5341 / 7640) loss: 2.089649\n",
      "(Epoch 14 / 20) train acc: 0.760000; val_acc: 0.552000\n",
      "(Iteration 5351 / 7640) loss: 2.102870\n",
      "(Iteration 5361 / 7640) loss: 2.227325\n",
      "(Iteration 5371 / 7640) loss: 2.290328\n",
      "(Iteration 5381 / 7640) loss: 2.283736\n",
      "(Iteration 5391 / 7640) loss: 2.087860\n",
      "(Iteration 5401 / 7640) loss: 1.993427\n",
      "(Iteration 5411 / 7640) loss: 2.321174\n",
      "(Iteration 5421 / 7640) loss: 2.198850\n",
      "(Iteration 5431 / 7640) loss: 2.213624\n",
      "(Iteration 5441 / 7640) loss: 2.202699\n",
      "(Iteration 5451 / 7640) loss: 2.280230\n",
      "(Iteration 5461 / 7640) loss: 2.077544\n",
      "(Iteration 5471 / 7640) loss: 2.207834\n",
      "(Iteration 5481 / 7640) loss: 2.099725\n",
      "(Iteration 5491 / 7640) loss: 2.199120\n",
      "(Iteration 5501 / 7640) loss: 2.127382\n",
      "(Iteration 5511 / 7640) loss: 2.011123\n",
      "(Iteration 5521 / 7640) loss: 2.049148\n",
      "(Iteration 5531 / 7640) loss: 2.101493\n",
      "(Iteration 5541 / 7640) loss: 1.997121\n",
      "(Iteration 5551 / 7640) loss: 2.019874\n",
      "(Iteration 5561 / 7640) loss: 2.164444\n",
      "(Iteration 5571 / 7640) loss: 2.074534\n",
      "(Iteration 5581 / 7640) loss: 2.044883\n",
      "(Iteration 5591 / 7640) loss: 2.148841\n",
      "(Iteration 5601 / 7640) loss: 2.107886\n",
      "(Iteration 5611 / 7640) loss: 2.055173\n",
      "(Iteration 5621 / 7640) loss: 2.272226\n",
      "(Iteration 5631 / 7640) loss: 2.132743\n",
      "(Iteration 5641 / 7640) loss: 2.237606\n",
      "(Iteration 5651 / 7640) loss: 2.062306\n",
      "(Iteration 5661 / 7640) loss: 2.008071\n",
      "(Iteration 5671 / 7640) loss: 2.227854\n",
      "(Iteration 5681 / 7640) loss: 2.100039\n",
      "(Iteration 5691 / 7640) loss: 1.940088\n",
      "(Iteration 5701 / 7640) loss: 2.125854\n",
      "(Iteration 5711 / 7640) loss: 2.032919\n",
      "(Iteration 5721 / 7640) loss: 2.105655\n",
      "(Epoch 15 / 20) train acc: 0.769000; val_acc: 0.558000\n",
      "(Iteration 5731 / 7640) loss: 2.179018\n",
      "(Iteration 5741 / 7640) loss: 2.096789\n",
      "(Iteration 5751 / 7640) loss: 2.005649\n",
      "(Iteration 5761 / 7640) loss: 2.294284\n",
      "(Iteration 5771 / 7640) loss: 1.975386\n",
      "(Iteration 5781 / 7640) loss: 1.973942\n",
      "(Iteration 5791 / 7640) loss: 1.998121\n",
      "(Iteration 5801 / 7640) loss: 2.047449\n",
      "(Iteration 5811 / 7640) loss: 2.123642\n",
      "(Iteration 5821 / 7640) loss: 2.062351\n",
      "(Iteration 5831 / 7640) loss: 2.029694\n",
      "(Iteration 5841 / 7640) loss: 1.949554\n",
      "(Iteration 5851 / 7640) loss: 2.196810\n",
      "(Iteration 5861 / 7640) loss: 2.117687\n",
      "(Iteration 5871 / 7640) loss: 2.194579\n",
      "(Iteration 5881 / 7640) loss: 2.166544\n",
      "(Iteration 5891 / 7640) loss: 2.038712\n",
      "(Iteration 5901 / 7640) loss: 1.983392\n",
      "(Iteration 5911 / 7640) loss: 2.107810\n",
      "(Iteration 5921 / 7640) loss: 2.091321\n",
      "(Iteration 5931 / 7640) loss: 1.944503\n",
      "(Iteration 5941 / 7640) loss: 2.170926\n",
      "(Iteration 5951 / 7640) loss: 2.105753\n",
      "(Iteration 5961 / 7640) loss: 1.945892\n",
      "(Iteration 5971 / 7640) loss: 1.967796\n",
      "(Iteration 5981 / 7640) loss: 2.099246\n",
      "(Iteration 5991 / 7640) loss: 2.154709\n",
      "(Iteration 6001 / 7640) loss: 2.232620\n",
      "(Iteration 6011 / 7640) loss: 2.082002\n",
      "(Iteration 6021 / 7640) loss: 2.147638\n",
      "(Iteration 6031 / 7640) loss: 1.736558\n",
      "(Iteration 6041 / 7640) loss: 1.846649\n",
      "(Iteration 6051 / 7640) loss: 2.044235\n",
      "(Iteration 6061 / 7640) loss: 2.124599\n",
      "(Iteration 6071 / 7640) loss: 1.943107\n",
      "(Iteration 6081 / 7640) loss: 2.078694\n",
      "(Iteration 6091 / 7640) loss: 2.070382\n",
      "(Iteration 6101 / 7640) loss: 2.093199\n",
      "(Iteration 6111 / 7640) loss: 2.092868\n",
      "(Epoch 16 / 20) train acc: 0.810000; val_acc: 0.541000\n",
      "(Iteration 6121 / 7640) loss: 1.879536\n",
      "(Iteration 6131 / 7640) loss: 1.956013\n",
      "(Iteration 6141 / 7640) loss: 1.947269\n",
      "(Iteration 6151 / 7640) loss: 1.970340\n",
      "(Iteration 6161 / 7640) loss: 1.958653\n",
      "(Iteration 6171 / 7640) loss: 2.007049\n",
      "(Iteration 6181 / 7640) loss: 1.975816\n",
      "(Iteration 6191 / 7640) loss: 1.987846\n",
      "(Iteration 6201 / 7640) loss: 1.880902\n",
      "(Iteration 6211 / 7640) loss: 1.857553\n",
      "(Iteration 6221 / 7640) loss: 2.068910\n",
      "(Iteration 6231 / 7640) loss: 1.977856\n",
      "(Iteration 6241 / 7640) loss: 2.000691\n",
      "(Iteration 6251 / 7640) loss: 1.962627\n",
      "(Iteration 6261 / 7640) loss: 1.924736\n",
      "(Iteration 6271 / 7640) loss: 1.887306\n",
      "(Iteration 6281 / 7640) loss: 1.843441\n",
      "(Iteration 6291 / 7640) loss: 1.892916\n",
      "(Iteration 6301 / 7640) loss: 1.959856\n",
      "(Iteration 6311 / 7640) loss: 1.838188\n",
      "(Iteration 6321 / 7640) loss: 1.988342\n",
      "(Iteration 6331 / 7640) loss: 1.989145\n",
      "(Iteration 6341 / 7640) loss: 1.958783\n",
      "(Iteration 6351 / 7640) loss: 1.850192\n",
      "(Iteration 6361 / 7640) loss: 1.893655\n",
      "(Iteration 6371 / 7640) loss: 1.847609\n",
      "(Iteration 6381 / 7640) loss: 1.763120\n",
      "(Iteration 6391 / 7640) loss: 1.924538\n",
      "(Iteration 6401 / 7640) loss: 1.945711\n",
      "(Iteration 6411 / 7640) loss: 1.886830\n",
      "(Iteration 6421 / 7640) loss: 2.031192\n",
      "(Iteration 6431 / 7640) loss: 1.966604\n",
      "(Iteration 6441 / 7640) loss: 1.816886\n",
      "(Iteration 6451 / 7640) loss: 1.941047\n",
      "(Iteration 6461 / 7640) loss: 1.851441\n",
      "(Iteration 6471 / 7640) loss: 1.815875\n",
      "(Iteration 6481 / 7640) loss: 1.966012\n",
      "(Iteration 6491 / 7640) loss: 1.924219\n",
      "(Epoch 17 / 20) train acc: 0.810000; val_acc: 0.554000\n",
      "(Iteration 6501 / 7640) loss: 1.876034\n",
      "(Iteration 6511 / 7640) loss: 2.038259\n",
      "(Iteration 6521 / 7640) loss: 1.777005\n",
      "(Iteration 6531 / 7640) loss: 2.007322\n",
      "(Iteration 6541 / 7640) loss: 1.864251\n",
      "(Iteration 6551 / 7640) loss: 1.941489\n",
      "(Iteration 6561 / 7640) loss: 1.834819\n",
      "(Iteration 6571 / 7640) loss: 1.848767\n",
      "(Iteration 6581 / 7640) loss: 1.967162\n",
      "(Iteration 6591 / 7640) loss: 1.926618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 6601 / 7640) loss: 1.868216\n",
      "(Iteration 6611 / 7640) loss: 1.904660\n",
      "(Iteration 6621 / 7640) loss: 1.861895\n",
      "(Iteration 6631 / 7640) loss: 1.959174\n",
      "(Iteration 6641 / 7640) loss: 1.789584\n",
      "(Iteration 6651 / 7640) loss: 1.883300\n",
      "(Iteration 6661 / 7640) loss: 1.725552\n",
      "(Iteration 6671 / 7640) loss: 2.034858\n",
      "(Iteration 6681 / 7640) loss: 2.009724\n",
      "(Iteration 6691 / 7640) loss: 1.984830\n",
      "(Iteration 6701 / 7640) loss: 1.772920\n",
      "(Iteration 6711 / 7640) loss: 2.017500\n",
      "(Iteration 6721 / 7640) loss: 1.799062\n",
      "(Iteration 6731 / 7640) loss: 1.797762\n",
      "(Iteration 6741 / 7640) loss: 1.837359\n",
      "(Iteration 6751 / 7640) loss: 1.934533\n",
      "(Iteration 6761 / 7640) loss: 1.958180\n",
      "(Iteration 6771 / 7640) loss: 1.829992\n",
      "(Iteration 6781 / 7640) loss: 1.845961\n",
      "(Iteration 6791 / 7640) loss: 1.802033\n",
      "(Iteration 6801 / 7640) loss: 1.933169\n",
      "(Iteration 6811 / 7640) loss: 1.745561\n",
      "(Iteration 6821 / 7640) loss: 1.839423\n",
      "(Iteration 6831 / 7640) loss: 1.721593\n",
      "(Iteration 6841 / 7640) loss: 1.874783\n",
      "(Iteration 6851 / 7640) loss: 1.759346\n",
      "(Iteration 6861 / 7640) loss: 1.683946\n",
      "(Iteration 6871 / 7640) loss: 1.828460\n",
      "(Epoch 18 / 20) train acc: 0.817000; val_acc: 0.553000\n",
      "(Iteration 6881 / 7640) loss: 1.812977\n",
      "(Iteration 6891 / 7640) loss: 1.801032\n",
      "(Iteration 6901 / 7640) loss: 1.757241\n",
      "(Iteration 6911 / 7640) loss: 1.791924\n",
      "(Iteration 6921 / 7640) loss: 1.840421\n",
      "(Iteration 6931 / 7640) loss: 1.730426\n",
      "(Iteration 6941 / 7640) loss: 1.797900\n",
      "(Iteration 6951 / 7640) loss: 1.743301\n",
      "(Iteration 6961 / 7640) loss: 1.906388\n",
      "(Iteration 6971 / 7640) loss: 1.766510\n",
      "(Iteration 6981 / 7640) loss: 1.781755\n",
      "(Iteration 6991 / 7640) loss: 1.976824\n",
      "(Iteration 7001 / 7640) loss: 1.833743\n",
      "(Iteration 7011 / 7640) loss: 1.753298\n",
      "(Iteration 7021 / 7640) loss: 1.797751\n",
      "(Iteration 7031 / 7640) loss: 1.838633\n",
      "(Iteration 7041 / 7640) loss: 1.583282\n",
      "(Iteration 7051 / 7640) loss: 1.776989\n",
      "(Iteration 7061 / 7640) loss: 1.722872\n",
      "(Iteration 7071 / 7640) loss: 1.786327\n",
      "(Iteration 7081 / 7640) loss: 1.754553\n",
      "(Iteration 7091 / 7640) loss: 1.756126\n",
      "(Iteration 7101 / 7640) loss: 1.735358\n",
      "(Iteration 7111 / 7640) loss: 1.825747\n",
      "(Iteration 7121 / 7640) loss: 1.707763\n",
      "(Iteration 7131 / 7640) loss: 1.716008\n",
      "(Iteration 7141 / 7640) loss: 1.725670\n",
      "(Iteration 7151 / 7640) loss: 1.706051\n",
      "(Iteration 7161 / 7640) loss: 1.831049\n",
      "(Iteration 7171 / 7640) loss: 1.898935\n",
      "(Iteration 7181 / 7640) loss: 1.748148\n",
      "(Iteration 7191 / 7640) loss: 1.902208\n",
      "(Iteration 7201 / 7640) loss: 1.737236\n",
      "(Iteration 7211 / 7640) loss: 1.712741\n",
      "(Iteration 7221 / 7640) loss: 1.619504\n",
      "(Iteration 7231 / 7640) loss: 1.734804\n",
      "(Iteration 7241 / 7640) loss: 1.759947\n",
      "(Iteration 7251 / 7640) loss: 1.627776\n",
      "(Epoch 19 / 20) train acc: 0.828000; val_acc: 0.542000\n",
      "(Iteration 7261 / 7640) loss: 1.812360\n",
      "(Iteration 7271 / 7640) loss: 1.740714\n",
      "(Iteration 7281 / 7640) loss: 2.028781\n",
      "(Iteration 7291 / 7640) loss: 1.700280\n",
      "(Iteration 7301 / 7640) loss: 1.667712\n",
      "(Iteration 7311 / 7640) loss: 1.734169\n",
      "(Iteration 7321 / 7640) loss: 1.859421\n",
      "(Iteration 7331 / 7640) loss: 1.681531\n",
      "(Iteration 7341 / 7640) loss: 1.628454\n",
      "(Iteration 7351 / 7640) loss: 1.771070\n",
      "(Iteration 7361 / 7640) loss: 1.603720\n",
      "(Iteration 7371 / 7640) loss: 1.803311\n",
      "(Iteration 7381 / 7640) loss: 1.642193\n",
      "(Iteration 7391 / 7640) loss: 1.574127\n",
      "(Iteration 7401 / 7640) loss: 1.730833\n",
      "(Iteration 7411 / 7640) loss: 1.499018\n",
      "(Iteration 7421 / 7640) loss: 1.799018\n",
      "(Iteration 7431 / 7640) loss: 1.669145\n",
      "(Iteration 7441 / 7640) loss: 1.736761\n",
      "(Iteration 7451 / 7640) loss: 1.646105\n",
      "(Iteration 7461 / 7640) loss: 1.701678\n",
      "(Iteration 7471 / 7640) loss: 1.640745\n",
      "(Iteration 7481 / 7640) loss: 1.604920\n",
      "(Iteration 7491 / 7640) loss: 1.603474\n",
      "(Iteration 7501 / 7640) loss: 1.572816\n",
      "(Iteration 7511 / 7640) loss: 1.725946\n",
      "(Iteration 7521 / 7640) loss: 1.616914\n",
      "(Iteration 7531 / 7640) loss: 1.641977\n",
      "(Iteration 7541 / 7640) loss: 1.546376\n",
      "(Iteration 7551 / 7640) loss: 1.684452\n",
      "(Iteration 7561 / 7640) loss: 1.550090\n",
      "(Iteration 7571 / 7640) loss: 1.660965\n",
      "(Iteration 7581 / 7640) loss: 1.564705\n",
      "(Iteration 7591 / 7640) loss: 1.660826\n",
      "(Iteration 7601 / 7640) loss: 1.678678\n",
      "(Iteration 7611 / 7640) loss: 1.585420\n",
      "(Iteration 7621 / 7640) loss: 1.812037\n",
      "(Iteration 7631 / 7640) loss: 1.569270\n",
      "(Epoch 20 / 20) train acc: 0.843000; val_acc: 0.556000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.558000\n",
      "running with  adamopt\n",
      "(Iteration 1 / 7640) loss: 6.641057\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.114000\n",
      "(Iteration 11 / 7640) loss: 5.888642\n",
      "(Iteration 21 / 7640) loss: 5.663342\n",
      "(Iteration 31 / 7640) loss: 5.404955\n",
      "(Iteration 41 / 7640) loss: 5.146619\n",
      "(Iteration 51 / 7640) loss: 5.051680\n",
      "(Iteration 61 / 7640) loss: 5.004074\n",
      "(Iteration 71 / 7640) loss: 4.790078\n",
      "(Iteration 81 / 7640) loss: 4.920855\n",
      "(Iteration 91 / 7640) loss: 4.694186\n",
      "(Iteration 101 / 7640) loss: 4.657476\n",
      "(Iteration 111 / 7640) loss: 4.464933\n",
      "(Iteration 121 / 7640) loss: 4.583558\n",
      "(Iteration 131 / 7640) loss: 4.630592\n",
      "(Iteration 141 / 7640) loss: 4.645895\n",
      "(Iteration 151 / 7640) loss: 4.483019\n",
      "(Iteration 161 / 7640) loss: 4.501680\n",
      "(Iteration 171 / 7640) loss: 4.335469\n",
      "(Iteration 181 / 7640) loss: 4.290178\n",
      "(Iteration 191 / 7640) loss: 4.200967\n",
      "(Iteration 201 / 7640) loss: 4.352758\n",
      "(Iteration 211 / 7640) loss: 4.337666\n",
      "(Iteration 221 / 7640) loss: 4.074348\n",
      "(Iteration 231 / 7640) loss: 4.150541\n",
      "(Iteration 241 / 7640) loss: 4.155166\n",
      "(Iteration 251 / 7640) loss: 4.291085\n",
      "(Iteration 261 / 7640) loss: 3.962044\n",
      "(Iteration 271 / 7640) loss: 3.893289\n",
      "(Iteration 281 / 7640) loss: 4.027882\n",
      "(Iteration 291 / 7640) loss: 4.122620\n",
      "(Iteration 301 / 7640) loss: 3.837111\n",
      "(Iteration 311 / 7640) loss: 3.899767\n",
      "(Iteration 321 / 7640) loss: 3.962264\n",
      "(Iteration 331 / 7640) loss: 3.990078\n",
      "(Iteration 341 / 7640) loss: 3.950819\n",
      "(Iteration 351 / 7640) loss: 3.761009\n",
      "(Iteration 361 / 7640) loss: 3.736296\n",
      "(Iteration 371 / 7640) loss: 3.714739\n",
      "(Iteration 381 / 7640) loss: 3.794829\n",
      "(Epoch 1 / 20) train acc: 0.427000; val_acc: 0.432000\n",
      "(Iteration 391 / 7640) loss: 3.865496\n",
      "(Iteration 401 / 7640) loss: 3.698146\n",
      "(Iteration 411 / 7640) loss: 3.701746\n",
      "(Iteration 421 / 7640) loss: 3.594912\n",
      "(Iteration 431 / 7640) loss: 3.778743\n",
      "(Iteration 441 / 7640) loss: 3.679865\n",
      "(Iteration 451 / 7640) loss: 3.479321\n",
      "(Iteration 461 / 7640) loss: 3.517790\n",
      "(Iteration 471 / 7640) loss: 3.542182\n",
      "(Iteration 481 / 7640) loss: 3.592255\n",
      "(Iteration 491 / 7640) loss: 3.512627\n",
      "(Iteration 501 / 7640) loss: 3.511437\n",
      "(Iteration 511 / 7640) loss: 3.375973\n",
      "(Iteration 521 / 7640) loss: 3.476326\n",
      "(Iteration 531 / 7640) loss: 3.701465\n",
      "(Iteration 541 / 7640) loss: 3.639709\n",
      "(Iteration 551 / 7640) loss: 3.453509\n",
      "(Iteration 561 / 7640) loss: 3.466403\n",
      "(Iteration 571 / 7640) loss: 3.389301\n",
      "(Iteration 581 / 7640) loss: 3.373331\n",
      "(Iteration 591 / 7640) loss: 3.411564\n",
      "(Iteration 601 / 7640) loss: 3.306384\n",
      "(Iteration 611 / 7640) loss: 3.187777\n",
      "(Iteration 621 / 7640) loss: 3.319398\n",
      "(Iteration 631 / 7640) loss: 3.245133\n",
      "(Iteration 641 / 7640) loss: 3.263037\n",
      "(Iteration 651 / 7640) loss: 3.330532\n",
      "(Iteration 661 / 7640) loss: 3.299573\n",
      "(Iteration 671 / 7640) loss: 3.397604\n",
      "(Iteration 681 / 7640) loss: 3.300762\n",
      "(Iteration 691 / 7640) loss: 3.226806\n",
      "(Iteration 701 / 7640) loss: 3.301294\n",
      "(Iteration 711 / 7640) loss: 3.232398\n",
      "(Iteration 721 / 7640) loss: 3.199914\n",
      "(Iteration 731 / 7640) loss: 3.223271\n",
      "(Iteration 741 / 7640) loss: 3.289382\n",
      "(Iteration 751 / 7640) loss: 3.177788\n",
      "(Iteration 761 / 7640) loss: 3.237808\n",
      "(Epoch 2 / 20) train acc: 0.429000; val_acc: 0.448000\n",
      "(Iteration 771 / 7640) loss: 3.076184\n",
      "(Iteration 781 / 7640) loss: 3.139536\n",
      "(Iteration 791 / 7640) loss: 3.099143\n",
      "(Iteration 801 / 7640) loss: 3.202023\n",
      "(Iteration 811 / 7640) loss: 2.976153\n",
      "(Iteration 821 / 7640) loss: 3.196600\n",
      "(Iteration 831 / 7640) loss: 3.078772\n",
      "(Iteration 841 / 7640) loss: 2.892794\n",
      "(Iteration 851 / 7640) loss: 3.051460\n",
      "(Iteration 861 / 7640) loss: 3.206001\n",
      "(Iteration 871 / 7640) loss: 3.298485\n",
      "(Iteration 881 / 7640) loss: 3.136888\n",
      "(Iteration 891 / 7640) loss: 2.866180\n",
      "(Iteration 901 / 7640) loss: 2.965731\n",
      "(Iteration 911 / 7640) loss: 3.196028\n",
      "(Iteration 921 / 7640) loss: 3.074196\n",
      "(Iteration 931 / 7640) loss: 2.900143\n",
      "(Iteration 941 / 7640) loss: 2.923961\n",
      "(Iteration 951 / 7640) loss: 2.847445\n",
      "(Iteration 961 / 7640) loss: 2.870674\n",
      "(Iteration 971 / 7640) loss: 2.966035\n",
      "(Iteration 981 / 7640) loss: 2.814299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 991 / 7640) loss: 2.931492\n",
      "(Iteration 1001 / 7640) loss: 2.940277\n",
      "(Iteration 1011 / 7640) loss: 2.937212\n",
      "(Iteration 1021 / 7640) loss: 3.090137\n",
      "(Iteration 1031 / 7640) loss: 2.883272\n",
      "(Iteration 1041 / 7640) loss: 2.791452\n",
      "(Iteration 1051 / 7640) loss: 2.885078\n",
      "(Iteration 1061 / 7640) loss: 2.770093\n",
      "(Iteration 1071 / 7640) loss: 2.934364\n",
      "(Iteration 1081 / 7640) loss: 2.844690\n",
      "(Iteration 1091 / 7640) loss: 2.693328\n",
      "(Iteration 1101 / 7640) loss: 2.888517\n",
      "(Iteration 1111 / 7640) loss: 2.956722\n",
      "(Iteration 1121 / 7640) loss: 2.875877\n",
      "(Iteration 1131 / 7640) loss: 2.892804\n",
      "(Iteration 1141 / 7640) loss: 2.828728\n",
      "(Epoch 3 / 20) train acc: 0.486000; val_acc: 0.480000\n",
      "(Iteration 1151 / 7640) loss: 2.928272\n",
      "(Iteration 1161 / 7640) loss: 2.951884\n",
      "(Iteration 1171 / 7640) loss: 2.893198\n",
      "(Iteration 1181 / 7640) loss: 2.774049\n",
      "(Iteration 1191 / 7640) loss: 2.727378\n",
      "(Iteration 1201 / 7640) loss: 2.892512\n",
      "(Iteration 1211 / 7640) loss: 2.955255\n",
      "(Iteration 1221 / 7640) loss: 2.778093\n",
      "(Iteration 1231 / 7640) loss: 2.626370\n",
      "(Iteration 1241 / 7640) loss: 2.716517\n",
      "(Iteration 1251 / 7640) loss: 2.708951\n",
      "(Iteration 1261 / 7640) loss: 2.761801\n",
      "(Iteration 1271 / 7640) loss: 2.712828\n",
      "(Iteration 1281 / 7640) loss: 2.657071\n",
      "(Iteration 1291 / 7640) loss: 2.658938\n",
      "(Iteration 1301 / 7640) loss: 2.799030\n",
      "(Iteration 1311 / 7640) loss: 2.704356\n",
      "(Iteration 1321 / 7640) loss: 2.529499\n",
      "(Iteration 1331 / 7640) loss: 2.490941\n",
      "(Iteration 1341 / 7640) loss: 2.759509\n",
      "(Iteration 1351 / 7640) loss: 2.782001\n",
      "(Iteration 1361 / 7640) loss: 2.479992\n",
      "(Iteration 1371 / 7640) loss: 2.696055\n",
      "(Iteration 1381 / 7640) loss: 2.764234\n",
      "(Iteration 1391 / 7640) loss: 2.790297\n",
      "(Iteration 1401 / 7640) loss: 2.603412\n",
      "(Iteration 1411 / 7640) loss: 2.492177\n",
      "(Iteration 1421 / 7640) loss: 2.768155\n",
      "(Iteration 1431 / 7640) loss: 2.646636\n",
      "(Iteration 1441 / 7640) loss: 2.526444\n",
      "(Iteration 1451 / 7640) loss: 2.743951\n",
      "(Iteration 1461 / 7640) loss: 2.533672\n",
      "(Iteration 1471 / 7640) loss: 2.575287\n",
      "(Iteration 1481 / 7640) loss: 2.636758\n",
      "(Iteration 1491 / 7640) loss: 2.442812\n",
      "(Iteration 1501 / 7640) loss: 2.683088\n",
      "(Iteration 1511 / 7640) loss: 2.584767\n",
      "(Iteration 1521 / 7640) loss: 2.563418\n",
      "(Epoch 4 / 20) train acc: 0.518000; val_acc: 0.491000\n",
      "(Iteration 1531 / 7640) loss: 2.448469\n",
      "(Iteration 1541 / 7640) loss: 2.514811\n",
      "(Iteration 1551 / 7640) loss: 2.582834\n",
      "(Iteration 1561 / 7640) loss: 2.591194\n",
      "(Iteration 1571 / 7640) loss: 2.613415\n",
      "(Iteration 1581 / 7640) loss: 2.383052\n",
      "(Iteration 1591 / 7640) loss: 2.434361\n",
      "(Iteration 1601 / 7640) loss: 2.556951\n",
      "(Iteration 1611 / 7640) loss: 2.523480\n",
      "(Iteration 1621 / 7640) loss: 2.667506\n",
      "(Iteration 1631 / 7640) loss: 2.427128\n",
      "(Iteration 1641 / 7640) loss: 2.696591\n",
      "(Iteration 1651 / 7640) loss: 2.451068\n",
      "(Iteration 1661 / 7640) loss: 2.437947\n",
      "(Iteration 1671 / 7640) loss: 2.336041\n",
      "(Iteration 1681 / 7640) loss: 2.511290\n",
      "(Iteration 1691 / 7640) loss: 2.635855\n",
      "(Iteration 1701 / 7640) loss: 2.480921\n",
      "(Iteration 1711 / 7640) loss: 2.416343\n",
      "(Iteration 1721 / 7640) loss: 2.464589\n",
      "(Iteration 1731 / 7640) loss: 2.474762\n",
      "(Iteration 1741 / 7640) loss: 2.251505\n",
      "(Iteration 1751 / 7640) loss: 2.292243\n",
      "(Iteration 1761 / 7640) loss: 2.472575\n",
      "(Iteration 1771 / 7640) loss: 2.427078\n",
      "(Iteration 1781 / 7640) loss: 2.544016\n",
      "(Iteration 1791 / 7640) loss: 2.405686\n",
      "(Iteration 1801 / 7640) loss: 2.473459\n",
      "(Iteration 1811 / 7640) loss: 2.542045\n",
      "(Iteration 1821 / 7640) loss: 2.455822\n",
      "(Iteration 1831 / 7640) loss: 2.468064\n",
      "(Iteration 1841 / 7640) loss: 2.306314\n",
      "(Iteration 1851 / 7640) loss: 2.477926\n",
      "(Iteration 1861 / 7640) loss: 2.187960\n",
      "(Iteration 1871 / 7640) loss: 2.621169\n",
      "(Iteration 1881 / 7640) loss: 2.570811\n",
      "(Iteration 1891 / 7640) loss: 2.343874\n",
      "(Iteration 1901 / 7640) loss: 2.233332\n",
      "(Epoch 5 / 20) train acc: 0.536000; val_acc: 0.470000\n",
      "(Iteration 1911 / 7640) loss: 2.253465\n",
      "(Iteration 1921 / 7640) loss: 2.207389\n",
      "(Iteration 1931 / 7640) loss: 2.394620\n",
      "(Iteration 1941 / 7640) loss: 2.383131\n",
      "(Iteration 1951 / 7640) loss: 2.223272\n",
      "(Iteration 1961 / 7640) loss: 2.251155\n",
      "(Iteration 1971 / 7640) loss: 2.332772\n",
      "(Iteration 1981 / 7640) loss: 2.395708\n",
      "(Iteration 1991 / 7640) loss: 2.117879\n",
      "(Iteration 2001 / 7640) loss: 2.340661\n",
      "(Iteration 2011 / 7640) loss: 2.342317\n",
      "(Iteration 2021 / 7640) loss: 2.605368\n",
      "(Iteration 2031 / 7640) loss: 2.416895\n",
      "(Iteration 2041 / 7640) loss: 2.268896\n",
      "(Iteration 2051 / 7640) loss: 2.230930\n",
      "(Iteration 2061 / 7640) loss: 2.215837\n",
      "(Iteration 2071 / 7640) loss: 2.221419\n",
      "(Iteration 2081 / 7640) loss: 2.268297\n",
      "(Iteration 2091 / 7640) loss: 2.448680\n",
      "(Iteration 2101 / 7640) loss: 2.168385\n",
      "(Iteration 2111 / 7640) loss: 2.190374\n",
      "(Iteration 2121 / 7640) loss: 2.223312\n",
      "(Iteration 2131 / 7640) loss: 2.240733\n",
      "(Iteration 2141 / 7640) loss: 2.323183\n",
      "(Iteration 2151 / 7640) loss: 2.245072\n",
      "(Iteration 2161 / 7640) loss: 2.368298\n",
      "(Iteration 2171 / 7640) loss: 2.119125\n",
      "(Iteration 2181 / 7640) loss: 2.279596\n",
      "(Iteration 2191 / 7640) loss: 2.098572\n",
      "(Iteration 2201 / 7640) loss: 2.066866\n",
      "(Iteration 2211 / 7640) loss: 2.138251\n",
      "(Iteration 2221 / 7640) loss: 2.210445\n",
      "(Iteration 2231 / 7640) loss: 2.178185\n",
      "(Iteration 2241 / 7640) loss: 2.240287\n",
      "(Iteration 2251 / 7640) loss: 2.169192\n",
      "(Iteration 2261 / 7640) loss: 2.151195\n",
      "(Iteration 2271 / 7640) loss: 2.311758\n",
      "(Iteration 2281 / 7640) loss: 2.144520\n",
      "(Iteration 2291 / 7640) loss: 2.339395\n",
      "(Epoch 6 / 20) train acc: 0.514000; val_acc: 0.485000\n",
      "(Iteration 2301 / 7640) loss: 2.135837\n",
      "(Iteration 2311 / 7640) loss: 2.270248\n",
      "(Iteration 2321 / 7640) loss: 2.209059\n",
      "(Iteration 2331 / 7640) loss: 2.073401\n",
      "(Iteration 2341 / 7640) loss: 2.162376\n",
      "(Iteration 2351 / 7640) loss: 2.011190\n",
      "(Iteration 2361 / 7640) loss: 2.151417\n",
      "(Iteration 2371 / 7640) loss: 1.978108\n",
      "(Iteration 2381 / 7640) loss: 2.287750\n",
      "(Iteration 2391 / 7640) loss: 2.113954\n",
      "(Iteration 2401 / 7640) loss: 2.298753\n",
      "(Iteration 2411 / 7640) loss: 2.195374\n",
      "(Iteration 2421 / 7640) loss: 1.994456\n",
      "(Iteration 2431 / 7640) loss: 2.096486\n",
      "(Iteration 2441 / 7640) loss: 2.174138\n",
      "(Iteration 2451 / 7640) loss: 2.164069\n",
      "(Iteration 2461 / 7640) loss: 2.094269\n",
      "(Iteration 2471 / 7640) loss: 2.102708\n",
      "(Iteration 2481 / 7640) loss: 2.088947\n",
      "(Iteration 2491 / 7640) loss: 2.059687\n",
      "(Iteration 2501 / 7640) loss: 2.196280\n",
      "(Iteration 2511 / 7640) loss: 2.190890\n",
      "(Iteration 2521 / 7640) loss: 2.014001\n",
      "(Iteration 2531 / 7640) loss: 2.259958\n",
      "(Iteration 2541 / 7640) loss: 2.010852\n",
      "(Iteration 2551 / 7640) loss: 2.123153\n",
      "(Iteration 2561 / 7640) loss: 2.089944\n",
      "(Iteration 2571 / 7640) loss: 2.234925\n",
      "(Iteration 2581 / 7640) loss: 2.160062\n",
      "(Iteration 2591 / 7640) loss: 2.135320\n",
      "(Iteration 2601 / 7640) loss: 2.034611\n",
      "(Iteration 2611 / 7640) loss: 2.042434\n",
      "(Iteration 2621 / 7640) loss: 1.896628\n",
      "(Iteration 2631 / 7640) loss: 2.140524\n",
      "(Iteration 2641 / 7640) loss: 1.893891\n",
      "(Iteration 2651 / 7640) loss: 2.167932\n",
      "(Iteration 2661 / 7640) loss: 2.187268\n",
      "(Iteration 2671 / 7640) loss: 2.181135\n",
      "(Epoch 7 / 20) train acc: 0.552000; val_acc: 0.515000\n",
      "(Iteration 2681 / 7640) loss: 2.097865\n",
      "(Iteration 2691 / 7640) loss: 1.976462\n",
      "(Iteration 2701 / 7640) loss: 2.015969\n",
      "(Iteration 2711 / 7640) loss: 2.090192\n",
      "(Iteration 2721 / 7640) loss: 1.975917\n",
      "(Iteration 2731 / 7640) loss: 1.933526\n",
      "(Iteration 2741 / 7640) loss: 2.035703\n",
      "(Iteration 2751 / 7640) loss: 2.110376\n",
      "(Iteration 2761 / 7640) loss: 1.833511\n",
      "(Iteration 2771 / 7640) loss: 1.997114\n",
      "(Iteration 2781 / 7640) loss: 2.063306\n",
      "(Iteration 2791 / 7640) loss: 2.029351\n",
      "(Iteration 2801 / 7640) loss: 2.099168\n",
      "(Iteration 2811 / 7640) loss: 2.147049\n",
      "(Iteration 2821 / 7640) loss: 2.138889\n",
      "(Iteration 2831 / 7640) loss: 2.039061\n",
      "(Iteration 2841 / 7640) loss: 2.103944\n",
      "(Iteration 2851 / 7640) loss: 1.991034\n",
      "(Iteration 2861 / 7640) loss: 2.126663\n",
      "(Iteration 2871 / 7640) loss: 2.072959\n",
      "(Iteration 2881 / 7640) loss: 1.925584\n",
      "(Iteration 2891 / 7640) loss: 2.022092\n",
      "(Iteration 2901 / 7640) loss: 1.896636\n",
      "(Iteration 2911 / 7640) loss: 2.059336\n",
      "(Iteration 2921 / 7640) loss: 1.748106\n",
      "(Iteration 2931 / 7640) loss: 2.066687\n",
      "(Iteration 2941 / 7640) loss: 1.871792\n",
      "(Iteration 2951 / 7640) loss: 2.064427\n",
      "(Iteration 2961 / 7640) loss: 1.952502\n",
      "(Iteration 2971 / 7640) loss: 1.945752\n",
      "(Iteration 2981 / 7640) loss: 2.093899\n",
      "(Iteration 2991 / 7640) loss: 1.875509\n",
      "(Iteration 3001 / 7640) loss: 2.014490\n",
      "(Iteration 3011 / 7640) loss: 1.932559\n",
      "(Iteration 3021 / 7640) loss: 1.880796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3031 / 7640) loss: 1.793718\n",
      "(Iteration 3041 / 7640) loss: 1.949156\n",
      "(Iteration 3051 / 7640) loss: 1.973623\n",
      "(Epoch 8 / 20) train acc: 0.597000; val_acc: 0.527000\n",
      "(Iteration 3061 / 7640) loss: 2.024501\n",
      "(Iteration 3071 / 7640) loss: 1.868064\n",
      "(Iteration 3081 / 7640) loss: 1.579084\n",
      "(Iteration 3091 / 7640) loss: 1.855086\n",
      "(Iteration 3101 / 7640) loss: 1.889819\n",
      "(Iteration 3111 / 7640) loss: 1.927815\n",
      "(Iteration 3121 / 7640) loss: 1.914040\n",
      "(Iteration 3131 / 7640) loss: 2.008240\n",
      "(Iteration 3141 / 7640) loss: 1.917902\n",
      "(Iteration 3151 / 7640) loss: 1.862490\n",
      "(Iteration 3161 / 7640) loss: 1.858243\n",
      "(Iteration 3171 / 7640) loss: 1.813197\n",
      "(Iteration 3181 / 7640) loss: 1.775945\n",
      "(Iteration 3191 / 7640) loss: 2.008576\n",
      "(Iteration 3201 / 7640) loss: 1.851880\n",
      "(Iteration 3211 / 7640) loss: 1.874890\n",
      "(Iteration 3221 / 7640) loss: 1.954540\n",
      "(Iteration 3231 / 7640) loss: 1.964666\n",
      "(Iteration 3241 / 7640) loss: 2.019892\n",
      "(Iteration 3251 / 7640) loss: 1.998184\n",
      "(Iteration 3261 / 7640) loss: 1.902153\n",
      "(Iteration 3271 / 7640) loss: 1.976334\n",
      "(Iteration 3281 / 7640) loss: 1.777274\n",
      "(Iteration 3291 / 7640) loss: 1.985774\n",
      "(Iteration 3301 / 7640) loss: 1.912420\n",
      "(Iteration 3311 / 7640) loss: 1.833486\n",
      "(Iteration 3321 / 7640) loss: 1.845659\n",
      "(Iteration 3331 / 7640) loss: 1.805108\n",
      "(Iteration 3341 / 7640) loss: 1.920621\n",
      "(Iteration 3351 / 7640) loss: 2.010499\n",
      "(Iteration 3361 / 7640) loss: 1.881200\n",
      "(Iteration 3371 / 7640) loss: 1.752010\n",
      "(Iteration 3381 / 7640) loss: 1.813483\n",
      "(Iteration 3391 / 7640) loss: 1.946895\n",
      "(Iteration 3401 / 7640) loss: 1.816231\n",
      "(Iteration 3411 / 7640) loss: 1.996757\n",
      "(Iteration 3421 / 7640) loss: 2.112329\n",
      "(Iteration 3431 / 7640) loss: 1.794399\n",
      "(Epoch 9 / 20) train acc: 0.607000; val_acc: 0.525000\n",
      "(Iteration 3441 / 7640) loss: 1.788365\n",
      "(Iteration 3451 / 7640) loss: 1.759683\n",
      "(Iteration 3461 / 7640) loss: 1.727503\n",
      "(Iteration 3471 / 7640) loss: 1.768015\n",
      "(Iteration 3481 / 7640) loss: 1.821361\n",
      "(Iteration 3491 / 7640) loss: 1.896274\n",
      "(Iteration 3501 / 7640) loss: 1.868019\n",
      "(Iteration 3511 / 7640) loss: 1.752442\n",
      "(Iteration 3521 / 7640) loss: 1.825614\n",
      "(Iteration 3531 / 7640) loss: 1.870273\n",
      "(Iteration 3541 / 7640) loss: 1.891038\n",
      "(Iteration 3551 / 7640) loss: 1.684803\n",
      "(Iteration 3561 / 7640) loss: 1.758992\n",
      "(Iteration 3571 / 7640) loss: 1.763927\n",
      "(Iteration 3581 / 7640) loss: 1.927811\n",
      "(Iteration 3591 / 7640) loss: 1.780350\n",
      "(Iteration 3601 / 7640) loss: 1.709931\n",
      "(Iteration 3611 / 7640) loss: 1.796515\n",
      "(Iteration 3621 / 7640) loss: 1.949015\n",
      "(Iteration 3631 / 7640) loss: 1.949943\n",
      "(Iteration 3641 / 7640) loss: 1.901484\n",
      "(Iteration 3651 / 7640) loss: 1.861961\n",
      "(Iteration 3661 / 7640) loss: 1.771337\n",
      "(Iteration 3671 / 7640) loss: 1.690501\n",
      "(Iteration 3681 / 7640) loss: 1.916138\n",
      "(Iteration 3691 / 7640) loss: 1.742596\n",
      "(Iteration 3701 / 7640) loss: 1.931759\n",
      "(Iteration 3711 / 7640) loss: 1.909547\n",
      "(Iteration 3721 / 7640) loss: 1.892723\n",
      "(Iteration 3731 / 7640) loss: 1.876485\n",
      "(Iteration 3741 / 7640) loss: 1.930091\n",
      "(Iteration 3751 / 7640) loss: 1.923077\n",
      "(Iteration 3761 / 7640) loss: 1.801143\n",
      "(Iteration 3771 / 7640) loss: 1.720756\n",
      "(Iteration 3781 / 7640) loss: 1.970462\n",
      "(Iteration 3791 / 7640) loss: 1.705914\n",
      "(Iteration 3801 / 7640) loss: 1.640267\n",
      "(Iteration 3811 / 7640) loss: 1.869430\n",
      "(Epoch 10 / 20) train acc: 0.567000; val_acc: 0.533000\n",
      "(Iteration 3821 / 7640) loss: 1.720745\n",
      "(Iteration 3831 / 7640) loss: 1.739138\n",
      "(Iteration 3841 / 7640) loss: 1.730228\n",
      "(Iteration 3851 / 7640) loss: 1.896995\n",
      "(Iteration 3861 / 7640) loss: 1.726173\n",
      "(Iteration 3871 / 7640) loss: 1.973060\n",
      "(Iteration 3881 / 7640) loss: 1.668821\n",
      "(Iteration 3891 / 7640) loss: 1.832015\n",
      "(Iteration 3901 / 7640) loss: 1.770872\n",
      "(Iteration 3911 / 7640) loss: 1.737976\n",
      "(Iteration 3921 / 7640) loss: 1.709729\n",
      "(Iteration 3931 / 7640) loss: 1.707645\n",
      "(Iteration 3941 / 7640) loss: 1.719682\n",
      "(Iteration 3951 / 7640) loss: 1.713618\n",
      "(Iteration 3961 / 7640) loss: 1.770330\n",
      "(Iteration 3971 / 7640) loss: 1.822321\n",
      "(Iteration 3981 / 7640) loss: 1.905909\n",
      "(Iteration 3991 / 7640) loss: 1.872774\n",
      "(Iteration 4001 / 7640) loss: 1.771741\n",
      "(Iteration 4011 / 7640) loss: 1.771515\n",
      "(Iteration 4021 / 7640) loss: 1.667392\n",
      "(Iteration 4031 / 7640) loss: 1.643974\n",
      "(Iteration 4041 / 7640) loss: 1.583795\n",
      "(Iteration 4051 / 7640) loss: 1.680150\n",
      "(Iteration 4061 / 7640) loss: 1.794802\n",
      "(Iteration 4071 / 7640) loss: 1.714121\n",
      "(Iteration 4081 / 7640) loss: 1.664062\n",
      "(Iteration 4091 / 7640) loss: 1.863534\n",
      "(Iteration 4101 / 7640) loss: 1.715091\n",
      "(Iteration 4111 / 7640) loss: 1.682448\n",
      "(Iteration 4121 / 7640) loss: 1.706747\n",
      "(Iteration 4131 / 7640) loss: 1.779677\n",
      "(Iteration 4141 / 7640) loss: 1.556929\n",
      "(Iteration 4151 / 7640) loss: 1.690293\n",
      "(Iteration 4161 / 7640) loss: 1.921436\n",
      "(Iteration 4171 / 7640) loss: 1.584170\n",
      "(Iteration 4181 / 7640) loss: 1.734403\n",
      "(Iteration 4191 / 7640) loss: 1.917716\n",
      "(Iteration 4201 / 7640) loss: 1.507778\n",
      "(Epoch 11 / 20) train acc: 0.614000; val_acc: 0.534000\n",
      "(Iteration 4211 / 7640) loss: 1.734125\n",
      "(Iteration 4221 / 7640) loss: 1.817965\n",
      "(Iteration 4231 / 7640) loss: 1.826690\n",
      "(Iteration 4241 / 7640) loss: 1.726846\n",
      "(Iteration 4251 / 7640) loss: 1.807578\n",
      "(Iteration 4261 / 7640) loss: 1.779465\n",
      "(Iteration 4271 / 7640) loss: 1.652912\n",
      "(Iteration 4281 / 7640) loss: 1.748313\n",
      "(Iteration 4291 / 7640) loss: 1.879257\n",
      "(Iteration 4301 / 7640) loss: 1.574713\n",
      "(Iteration 4311 / 7640) loss: 1.922091\n",
      "(Iteration 4321 / 7640) loss: 1.531925\n",
      "(Iteration 4331 / 7640) loss: 1.567462\n",
      "(Iteration 4341 / 7640) loss: 1.791451\n",
      "(Iteration 4351 / 7640) loss: 1.632217\n",
      "(Iteration 4361 / 7640) loss: 1.908916\n",
      "(Iteration 4371 / 7640) loss: 1.673071\n",
      "(Iteration 4381 / 7640) loss: 1.593561\n",
      "(Iteration 4391 / 7640) loss: 1.649279\n",
      "(Iteration 4401 / 7640) loss: 1.590772\n",
      "(Iteration 4411 / 7640) loss: 1.656019\n",
      "(Iteration 4421 / 7640) loss: 1.770244\n",
      "(Iteration 4431 / 7640) loss: 1.751992\n",
      "(Iteration 4441 / 7640) loss: 1.809678\n",
      "(Iteration 4451 / 7640) loss: 1.683739\n",
      "(Iteration 4461 / 7640) loss: 1.708916\n",
      "(Iteration 4471 / 7640) loss: 1.783290\n",
      "(Iteration 4481 / 7640) loss: 1.581340\n",
      "(Iteration 4491 / 7640) loss: 1.866056\n",
      "(Iteration 4501 / 7640) loss: 1.583354\n",
      "(Iteration 4511 / 7640) loss: 1.780201\n",
      "(Iteration 4521 / 7640) loss: 1.606878\n",
      "(Iteration 4531 / 7640) loss: 1.801074\n",
      "(Iteration 4541 / 7640) loss: 1.720041\n",
      "(Iteration 4551 / 7640) loss: 1.805321\n",
      "(Iteration 4561 / 7640) loss: 1.685028\n",
      "(Iteration 4571 / 7640) loss: 1.678093\n",
      "(Iteration 4581 / 7640) loss: 1.591282\n",
      "(Epoch 12 / 20) train acc: 0.592000; val_acc: 0.518000\n",
      "(Iteration 4591 / 7640) loss: 1.856697\n",
      "(Iteration 4601 / 7640) loss: 1.583539\n",
      "(Iteration 4611 / 7640) loss: 1.742245\n",
      "(Iteration 4621 / 7640) loss: 1.647527\n",
      "(Iteration 4631 / 7640) loss: 1.623662\n",
      "(Iteration 4641 / 7640) loss: 1.548150\n",
      "(Iteration 4651 / 7640) loss: 1.568974\n",
      "(Iteration 4661 / 7640) loss: 1.679594\n",
      "(Iteration 4671 / 7640) loss: 1.629379\n",
      "(Iteration 4681 / 7640) loss: 1.701552\n",
      "(Iteration 4691 / 7640) loss: 1.663985\n",
      "(Iteration 4701 / 7640) loss: 1.582866\n",
      "(Iteration 4711 / 7640) loss: 1.729522\n",
      "(Iteration 4721 / 7640) loss: 1.602498\n",
      "(Iteration 4731 / 7640) loss: 1.578313\n",
      "(Iteration 4741 / 7640) loss: 1.654572\n",
      "(Iteration 4751 / 7640) loss: 1.703473\n",
      "(Iteration 4761 / 7640) loss: 1.658448\n",
      "(Iteration 4771 / 7640) loss: 1.558819\n",
      "(Iteration 4781 / 7640) loss: 1.729952\n",
      "(Iteration 4791 / 7640) loss: 1.508774\n",
      "(Iteration 4801 / 7640) loss: 1.621489\n",
      "(Iteration 4811 / 7640) loss: 1.484315\n",
      "(Iteration 4821 / 7640) loss: 1.637681\n",
      "(Iteration 4831 / 7640) loss: 1.738200\n",
      "(Iteration 4841 / 7640) loss: 1.691611\n",
      "(Iteration 4851 / 7640) loss: 1.698051\n",
      "(Iteration 4861 / 7640) loss: 1.506847\n",
      "(Iteration 4871 / 7640) loss: 1.822700\n",
      "(Iteration 4881 / 7640) loss: 1.464948\n",
      "(Iteration 4891 / 7640) loss: 1.683953\n",
      "(Iteration 4901 / 7640) loss: 1.574939\n",
      "(Iteration 4911 / 7640) loss: 1.863749\n",
      "(Iteration 4921 / 7640) loss: 1.692809\n",
      "(Iteration 4931 / 7640) loss: 1.597058\n",
      "(Iteration 4941 / 7640) loss: 1.548455\n",
      "(Iteration 4951 / 7640) loss: 1.589556\n",
      "(Iteration 4961 / 7640) loss: 1.608879\n",
      "(Epoch 13 / 20) train acc: 0.581000; val_acc: 0.532000\n",
      "(Iteration 4971 / 7640) loss: 1.664370\n",
      "(Iteration 4981 / 7640) loss: 1.706281\n",
      "(Iteration 4991 / 7640) loss: 1.782426\n",
      "(Iteration 5001 / 7640) loss: 1.749950\n",
      "(Iteration 5011 / 7640) loss: 1.669043\n",
      "(Iteration 5021 / 7640) loss: 1.623551\n",
      "(Iteration 5031 / 7640) loss: 1.643220\n",
      "(Iteration 5041 / 7640) loss: 1.688263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 5051 / 7640) loss: 1.583782\n",
      "(Iteration 5061 / 7640) loss: 1.592100\n",
      "(Iteration 5071 / 7640) loss: 1.621668\n",
      "(Iteration 5081 / 7640) loss: 1.572896\n",
      "(Iteration 5091 / 7640) loss: 1.641832\n",
      "(Iteration 5101 / 7640) loss: 1.730724\n",
      "(Iteration 5111 / 7640) loss: 1.668458\n",
      "(Iteration 5121 / 7640) loss: 1.561084\n",
      "(Iteration 5131 / 7640) loss: 1.699465\n",
      "(Iteration 5141 / 7640) loss: 1.715254\n",
      "(Iteration 5151 / 7640) loss: 1.648139\n",
      "(Iteration 5161 / 7640) loss: 1.730095\n",
      "(Iteration 5171 / 7640) loss: 1.590787\n",
      "(Iteration 5181 / 7640) loss: 1.697561\n",
      "(Iteration 5191 / 7640) loss: 1.479945\n",
      "(Iteration 5201 / 7640) loss: 1.570007\n",
      "(Iteration 5211 / 7640) loss: 1.498979\n",
      "(Iteration 5221 / 7640) loss: 1.650576\n",
      "(Iteration 5231 / 7640) loss: 1.444085\n",
      "(Iteration 5241 / 7640) loss: 1.510905\n",
      "(Iteration 5251 / 7640) loss: 1.596157\n",
      "(Iteration 5261 / 7640) loss: 1.659223\n",
      "(Iteration 5271 / 7640) loss: 1.347358\n",
      "(Iteration 5281 / 7640) loss: 1.618039\n",
      "(Iteration 5291 / 7640) loss: 1.538658\n",
      "(Iteration 5301 / 7640) loss: 1.468820\n",
      "(Iteration 5311 / 7640) loss: 1.647476\n",
      "(Iteration 5321 / 7640) loss: 1.557304\n",
      "(Iteration 5331 / 7640) loss: 1.601116\n",
      "(Iteration 5341 / 7640) loss: 1.613260\n",
      "(Epoch 14 / 20) train acc: 0.638000; val_acc: 0.521000\n",
      "(Iteration 5351 / 7640) loss: 1.685152\n",
      "(Iteration 5361 / 7640) loss: 1.615696\n",
      "(Iteration 5371 / 7640) loss: 1.593107\n",
      "(Iteration 5381 / 7640) loss: 1.619872\n",
      "(Iteration 5391 / 7640) loss: 1.622084\n",
      "(Iteration 5401 / 7640) loss: 1.672011\n",
      "(Iteration 5411 / 7640) loss: 1.427978\n",
      "(Iteration 5421 / 7640) loss: 1.555121\n",
      "(Iteration 5431 / 7640) loss: 1.531983\n",
      "(Iteration 5441 / 7640) loss: 1.602143\n",
      "(Iteration 5451 / 7640) loss: 1.600661\n",
      "(Iteration 5461 / 7640) loss: 1.488674\n",
      "(Iteration 5471 / 7640) loss: 1.528156\n",
      "(Iteration 5481 / 7640) loss: 1.671479\n",
      "(Iteration 5491 / 7640) loss: 1.608457\n",
      "(Iteration 5501 / 7640) loss: 1.519492\n",
      "(Iteration 5511 / 7640) loss: 1.537871\n",
      "(Iteration 5521 / 7640) loss: 1.551784\n",
      "(Iteration 5531 / 7640) loss: 1.707065\n",
      "(Iteration 5541 / 7640) loss: 1.503843\n",
      "(Iteration 5551 / 7640) loss: 1.588832\n",
      "(Iteration 5561 / 7640) loss: 1.632807\n",
      "(Iteration 5571 / 7640) loss: 1.743626\n",
      "(Iteration 5581 / 7640) loss: 1.589500\n",
      "(Iteration 5591 / 7640) loss: 1.449653\n",
      "(Iteration 5601 / 7640) loss: 1.568802\n",
      "(Iteration 5611 / 7640) loss: 1.645022\n",
      "(Iteration 5621 / 7640) loss: 1.506165\n",
      "(Iteration 5631 / 7640) loss: 1.636125\n",
      "(Iteration 5641 / 7640) loss: 1.596120\n",
      "(Iteration 5651 / 7640) loss: 1.647091\n",
      "(Iteration 5661 / 7640) loss: 1.446163\n",
      "(Iteration 5671 / 7640) loss: 1.503402\n",
      "(Iteration 5681 / 7640) loss: 1.411961\n",
      "(Iteration 5691 / 7640) loss: 1.661723\n",
      "(Iteration 5701 / 7640) loss: 1.655854\n",
      "(Iteration 5711 / 7640) loss: 1.647613\n",
      "(Iteration 5721 / 7640) loss: 1.523678\n",
      "(Epoch 15 / 20) train acc: 0.622000; val_acc: 0.530000\n",
      "(Iteration 5731 / 7640) loss: 1.657256\n",
      "(Iteration 5741 / 7640) loss: 1.547739\n",
      "(Iteration 5751 / 7640) loss: 1.661192\n",
      "(Iteration 5761 / 7640) loss: 1.706361\n",
      "(Iteration 5771 / 7640) loss: 1.457674\n",
      "(Iteration 5781 / 7640) loss: 1.530319\n",
      "(Iteration 5791 / 7640) loss: 1.620271\n",
      "(Iteration 5801 / 7640) loss: 1.539908\n",
      "(Iteration 5811 / 7640) loss: 1.675783\n",
      "(Iteration 5821 / 7640) loss: 1.499598\n",
      "(Iteration 5831 / 7640) loss: 1.600491\n",
      "(Iteration 5841 / 7640) loss: 1.524728\n",
      "(Iteration 5851 / 7640) loss: 1.463093\n",
      "(Iteration 5861 / 7640) loss: 1.613718\n",
      "(Iteration 5871 / 7640) loss: 1.482578\n",
      "(Iteration 5881 / 7640) loss: 1.368415\n",
      "(Iteration 5891 / 7640) loss: 1.516769\n",
      "(Iteration 5901 / 7640) loss: 1.636712\n",
      "(Iteration 5911 / 7640) loss: 1.644466\n",
      "(Iteration 5921 / 7640) loss: 1.597173\n",
      "(Iteration 5931 / 7640) loss: 1.726089\n",
      "(Iteration 5941 / 7640) loss: 1.502775\n",
      "(Iteration 5951 / 7640) loss: 1.729666\n",
      "(Iteration 5961 / 7640) loss: 1.590989\n",
      "(Iteration 5971 / 7640) loss: 1.708620\n",
      "(Iteration 5981 / 7640) loss: 1.392975\n",
      "(Iteration 5991 / 7640) loss: 1.319327\n",
      "(Iteration 6001 / 7640) loss: 1.374374\n",
      "(Iteration 6011 / 7640) loss: 1.579210\n",
      "(Iteration 6021 / 7640) loss: 1.600143\n",
      "(Iteration 6031 / 7640) loss: 1.521964\n",
      "(Iteration 6041 / 7640) loss: 1.499759\n",
      "(Iteration 6051 / 7640) loss: 1.411339\n",
      "(Iteration 6061 / 7640) loss: 1.667595\n",
      "(Iteration 6071 / 7640) loss: 1.626749\n",
      "(Iteration 6081 / 7640) loss: 1.516395\n",
      "(Iteration 6091 / 7640) loss: 1.594021\n",
      "(Iteration 6101 / 7640) loss: 1.568295\n",
      "(Iteration 6111 / 7640) loss: 1.685449\n",
      "(Epoch 16 / 20) train acc: 0.613000; val_acc: 0.532000\n",
      "(Iteration 6121 / 7640) loss: 1.400541\n",
      "(Iteration 6131 / 7640) loss: 1.649731\n",
      "(Iteration 6141 / 7640) loss: 1.483648\n",
      "(Iteration 6151 / 7640) loss: 1.491414\n",
      "(Iteration 6161 / 7640) loss: 1.708360\n",
      "(Iteration 6171 / 7640) loss: 1.390666\n",
      "(Iteration 6181 / 7640) loss: 1.531378\n",
      "(Iteration 6191 / 7640) loss: 1.350109\n",
      "(Iteration 6201 / 7640) loss: 1.501754\n",
      "(Iteration 6211 / 7640) loss: 1.464786\n",
      "(Iteration 6221 / 7640) loss: 1.542710\n",
      "(Iteration 6231 / 7640) loss: 1.745509\n",
      "(Iteration 6241 / 7640) loss: 1.542275\n",
      "(Iteration 6251 / 7640) loss: 1.511583\n",
      "(Iteration 6261 / 7640) loss: 1.579407\n",
      "(Iteration 6271 / 7640) loss: 1.449927\n",
      "(Iteration 6281 / 7640) loss: 1.449678\n",
      "(Iteration 6291 / 7640) loss: 1.635638\n",
      "(Iteration 6301 / 7640) loss: 1.559311\n",
      "(Iteration 6311 / 7640) loss: 1.386955\n",
      "(Iteration 6321 / 7640) loss: 1.573321\n",
      "(Iteration 6331 / 7640) loss: 1.563634\n",
      "(Iteration 6341 / 7640) loss: 1.381372\n",
      "(Iteration 6351 / 7640) loss: 1.442170\n",
      "(Iteration 6361 / 7640) loss: 1.405555\n",
      "(Iteration 6371 / 7640) loss: 1.600392\n",
      "(Iteration 6381 / 7640) loss: 1.513183\n",
      "(Iteration 6391 / 7640) loss: 1.607661\n",
      "(Iteration 6401 / 7640) loss: 1.506728\n",
      "(Iteration 6411 / 7640) loss: 1.609434\n",
      "(Iteration 6421 / 7640) loss: 1.481287\n",
      "(Iteration 6431 / 7640) loss: 1.555699\n",
      "(Iteration 6441 / 7640) loss: 1.595252\n",
      "(Iteration 6451 / 7640) loss: 1.692191\n",
      "(Iteration 6461 / 7640) loss: 1.602864\n",
      "(Iteration 6471 / 7640) loss: 1.415174\n",
      "(Iteration 6481 / 7640) loss: 1.356866\n",
      "(Iteration 6491 / 7640) loss: 1.510776\n",
      "(Epoch 17 / 20) train acc: 0.614000; val_acc: 0.522000\n",
      "(Iteration 6501 / 7640) loss: 1.471540\n",
      "(Iteration 6511 / 7640) loss: 1.608991\n",
      "(Iteration 6521 / 7640) loss: 1.431748\n",
      "(Iteration 6531 / 7640) loss: 1.362568\n",
      "(Iteration 6541 / 7640) loss: 1.692923\n",
      "(Iteration 6551 / 7640) loss: 1.460522\n",
      "(Iteration 6561 / 7640) loss: 1.358073\n",
      "(Iteration 6571 / 7640) loss: 1.342821\n",
      "(Iteration 6581 / 7640) loss: 1.432229\n",
      "(Iteration 6591 / 7640) loss: 1.625468\n",
      "(Iteration 6601 / 7640) loss: 1.483851\n",
      "(Iteration 6611 / 7640) loss: 1.412300\n",
      "(Iteration 6621 / 7640) loss: 1.496957\n",
      "(Iteration 6631 / 7640) loss: 1.542924\n",
      "(Iteration 6641 / 7640) loss: 1.329019\n",
      "(Iteration 6651 / 7640) loss: 1.437113\n",
      "(Iteration 6661 / 7640) loss: 1.404219\n",
      "(Iteration 6671 / 7640) loss: 1.387913\n",
      "(Iteration 6681 / 7640) loss: 1.456614\n",
      "(Iteration 6691 / 7640) loss: 1.341638\n",
      "(Iteration 6701 / 7640) loss: 1.384627\n",
      "(Iteration 6711 / 7640) loss: 1.499243\n",
      "(Iteration 6721 / 7640) loss: 1.488418\n",
      "(Iteration 6731 / 7640) loss: 1.507623\n",
      "(Iteration 6741 / 7640) loss: 1.533700\n",
      "(Iteration 6751 / 7640) loss: 1.423269\n",
      "(Iteration 6761 / 7640) loss: 1.488164\n",
      "(Iteration 6771 / 7640) loss: 1.549472\n",
      "(Iteration 6781 / 7640) loss: 1.624147\n",
      "(Iteration 6791 / 7640) loss: 1.538551\n",
      "(Iteration 6801 / 7640) loss: 1.397072\n",
      "(Iteration 6811 / 7640) loss: 1.602791\n",
      "(Iteration 6821 / 7640) loss: 1.403358\n",
      "(Iteration 6831 / 7640) loss: 1.404156\n",
      "(Iteration 6841 / 7640) loss: 1.545560\n",
      "(Iteration 6851 / 7640) loss: 1.557018\n",
      "(Iteration 6861 / 7640) loss: 1.631514\n",
      "(Iteration 6871 / 7640) loss: 1.505880\n",
      "(Epoch 18 / 20) train acc: 0.589000; val_acc: 0.529000\n",
      "(Iteration 6881 / 7640) loss: 1.379736\n",
      "(Iteration 6891 / 7640) loss: 1.329375\n",
      "(Iteration 6901 / 7640) loss: 1.469387\n",
      "(Iteration 6911 / 7640) loss: 1.592805\n",
      "(Iteration 6921 / 7640) loss: 1.514271\n",
      "(Iteration 6931 / 7640) loss: 1.463883\n",
      "(Iteration 6941 / 7640) loss: 1.445919\n",
      "(Iteration 6951 / 7640) loss: 1.543643\n",
      "(Iteration 6961 / 7640) loss: 1.554711\n",
      "(Iteration 6971 / 7640) loss: 1.446768\n",
      "(Iteration 6981 / 7640) loss: 1.518089\n",
      "(Iteration 6991 / 7640) loss: 1.494775\n",
      "(Iteration 7001 / 7640) loss: 1.441081\n",
      "(Iteration 7011 / 7640) loss: 1.563427\n",
      "(Iteration 7021 / 7640) loss: 1.709186\n",
      "(Iteration 7031 / 7640) loss: 1.580562\n",
      "(Iteration 7041 / 7640) loss: 1.555192\n",
      "(Iteration 7051 / 7640) loss: 1.421227\n",
      "(Iteration 7061 / 7640) loss: 1.599899\n",
      "(Iteration 7071 / 7640) loss: 1.540153\n",
      "(Iteration 7081 / 7640) loss: 1.600763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7091 / 7640) loss: 1.474399\n",
      "(Iteration 7101 / 7640) loss: 1.402513\n",
      "(Iteration 7111 / 7640) loss: 1.435979\n",
      "(Iteration 7121 / 7640) loss: 1.337330\n",
      "(Iteration 7131 / 7640) loss: 1.545095\n",
      "(Iteration 7141 / 7640) loss: 1.408270\n",
      "(Iteration 7151 / 7640) loss: 1.549151\n",
      "(Iteration 7161 / 7640) loss: 1.511505\n",
      "(Iteration 7171 / 7640) loss: 1.485533\n",
      "(Iteration 7181 / 7640) loss: 1.381635\n",
      "(Iteration 7191 / 7640) loss: 1.409941\n",
      "(Iteration 7201 / 7640) loss: 1.526763\n",
      "(Iteration 7211 / 7640) loss: 1.486994\n",
      "(Iteration 7221 / 7640) loss: 1.389845\n",
      "(Iteration 7231 / 7640) loss: 1.394072\n",
      "(Iteration 7241 / 7640) loss: 1.308451\n",
      "(Iteration 7251 / 7640) loss: 1.625702\n",
      "(Epoch 19 / 20) train acc: 0.637000; val_acc: 0.529000\n",
      "(Iteration 7261 / 7640) loss: 1.455101\n",
      "(Iteration 7271 / 7640) loss: 1.331713\n",
      "(Iteration 7281 / 7640) loss: 1.359114\n",
      "(Iteration 7291 / 7640) loss: 1.306611\n",
      "(Iteration 7301 / 7640) loss: 1.423306\n",
      "(Iteration 7311 / 7640) loss: 1.338010\n",
      "(Iteration 7321 / 7640) loss: 1.323395\n",
      "(Iteration 7331 / 7640) loss: 1.422762\n",
      "(Iteration 7341 / 7640) loss: 1.430605\n",
      "(Iteration 7351 / 7640) loss: 1.506298\n",
      "(Iteration 7361 / 7640) loss: 1.335169\n",
      "(Iteration 7371 / 7640) loss: 1.489947\n",
      "(Iteration 7381 / 7640) loss: 1.495782\n",
      "(Iteration 7391 / 7640) loss: 1.489978\n",
      "(Iteration 7401 / 7640) loss: 1.457770\n",
      "(Iteration 7411 / 7640) loss: 1.429453\n",
      "(Iteration 7421 / 7640) loss: 1.362304\n",
      "(Iteration 7431 / 7640) loss: 1.354435\n",
      "(Iteration 7441 / 7640) loss: 1.390894\n",
      "(Iteration 7451 / 7640) loss: 1.449040\n",
      "(Iteration 7461 / 7640) loss: 1.286034\n",
      "(Iteration 7471 / 7640) loss: 1.572340\n",
      "(Iteration 7481 / 7640) loss: 1.406278\n",
      "(Iteration 7491 / 7640) loss: 1.464603\n",
      "(Iteration 7501 / 7640) loss: 1.616569\n",
      "(Iteration 7511 / 7640) loss: 1.586532\n",
      "(Iteration 7521 / 7640) loss: 1.475862\n",
      "(Iteration 7531 / 7640) loss: 1.324144\n",
      "(Iteration 7541 / 7640) loss: 1.483218\n",
      "(Iteration 7551 / 7640) loss: 1.521861\n",
      "(Iteration 7561 / 7640) loss: 1.491259\n",
      "(Iteration 7571 / 7640) loss: 1.510608\n",
      "(Iteration 7581 / 7640) loss: 1.459408\n",
      "(Iteration 7591 / 7640) loss: 1.474125\n",
      "(Iteration 7601 / 7640) loss: 1.379463\n",
      "(Iteration 7611 / 7640) loss: 1.480142\n",
      "(Iteration 7621 / 7640) loss: 1.445796\n",
      "(Iteration 7631 / 7640) loss: 1.470251\n",
      "(Epoch 20 / 20) train acc: 0.610000; val_acc: 0.540000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.540000\n",
      "running with  sgd\n",
      "(Iteration 1 / 7640) loss: 6.226417\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.107000\n",
      "(Iteration 11 / 7640) loss: 6.052525\n",
      "(Iteration 21 / 7640) loss: 6.230297\n",
      "(Iteration 31 / 7640) loss: 6.001738\n",
      "(Iteration 41 / 7640) loss: 6.199550\n",
      "(Iteration 51 / 7640) loss: 5.987185\n",
      "(Iteration 61 / 7640) loss: 6.195691\n",
      "(Iteration 71 / 7640) loss: 6.157533\n",
      "(Iteration 81 / 7640) loss: 6.181963\n",
      "(Iteration 91 / 7640) loss: 6.119754\n",
      "(Iteration 101 / 7640) loss: 5.954912\n",
      "(Iteration 111 / 7640) loss: 6.213841\n",
      "(Iteration 121 / 7640) loss: 6.067947\n",
      "(Iteration 131 / 7640) loss: 5.866969\n",
      "(Iteration 141 / 7640) loss: 5.974679\n",
      "(Iteration 151 / 7640) loss: 5.844023\n",
      "(Iteration 161 / 7640) loss: 6.042211\n",
      "(Iteration 171 / 7640) loss: 5.960243\n",
      "(Iteration 181 / 7640) loss: 5.894109\n",
      "(Iteration 191 / 7640) loss: 5.898116\n",
      "(Iteration 201 / 7640) loss: 5.818347\n",
      "(Iteration 211 / 7640) loss: 6.014912\n",
      "(Iteration 221 / 7640) loss: 5.933736\n",
      "(Iteration 231 / 7640) loss: 5.994754\n",
      "(Iteration 241 / 7640) loss: 5.844002\n",
      "(Iteration 251 / 7640) loss: 5.776847\n",
      "(Iteration 261 / 7640) loss: 5.817286\n",
      "(Iteration 271 / 7640) loss: 5.828042\n",
      "(Iteration 281 / 7640) loss: 5.799151\n",
      "(Iteration 291 / 7640) loss: 5.788081\n",
      "(Iteration 301 / 7640) loss: 5.673181\n",
      "(Iteration 311 / 7640) loss: 5.710782\n",
      "(Iteration 321 / 7640) loss: 5.642408\n",
      "(Iteration 331 / 7640) loss: 5.591584\n",
      "(Iteration 341 / 7640) loss: 5.616292\n",
      "(Iteration 351 / 7640) loss: 5.588712\n",
      "(Iteration 361 / 7640) loss: 5.604527\n",
      "(Iteration 371 / 7640) loss: 5.586695\n",
      "(Iteration 381 / 7640) loss: 5.552408\n",
      "(Epoch 1 / 20) train acc: 0.172000; val_acc: 0.178000\n",
      "(Iteration 391 / 7640) loss: 5.592542\n",
      "(Iteration 401 / 7640) loss: 5.510628\n",
      "(Iteration 411 / 7640) loss: 5.581621\n",
      "(Iteration 421 / 7640) loss: 5.585631\n",
      "(Iteration 431 / 7640) loss: 5.485204\n",
      "(Iteration 441 / 7640) loss: 5.511264\n",
      "(Iteration 451 / 7640) loss: 5.551565\n",
      "(Iteration 461 / 7640) loss: 5.540603\n",
      "(Iteration 471 / 7640) loss: 5.493287\n",
      "(Iteration 481 / 7640) loss: 5.582396\n",
      "(Iteration 491 / 7640) loss: 5.471239\n",
      "(Iteration 501 / 7640) loss: 5.478433\n",
      "(Iteration 511 / 7640) loss: 5.435825\n",
      "(Iteration 521 / 7640) loss: 5.503753\n",
      "(Iteration 531 / 7640) loss: 5.465268\n",
      "(Iteration 541 / 7640) loss: 5.426288\n",
      "(Iteration 551 / 7640) loss: 5.380284\n",
      "(Iteration 561 / 7640) loss: 5.385436\n",
      "(Iteration 571 / 7640) loss: 5.403789\n",
      "(Iteration 581 / 7640) loss: 5.378741\n",
      "(Iteration 591 / 7640) loss: 5.435476\n",
      "(Iteration 601 / 7640) loss: 5.413418\n",
      "(Iteration 611 / 7640) loss: 5.402705\n",
      "(Iteration 621 / 7640) loss: 5.415774\n",
      "(Iteration 631 / 7640) loss: 5.244063\n",
      "(Iteration 641 / 7640) loss: 5.475688\n",
      "(Iteration 651 / 7640) loss: 5.344512\n",
      "(Iteration 661 / 7640) loss: 5.475780\n",
      "(Iteration 671 / 7640) loss: 5.378076\n",
      "(Iteration 681 / 7640) loss: 5.357472\n",
      "(Iteration 691 / 7640) loss: 5.375994\n",
      "(Iteration 701 / 7640) loss: 5.417329\n",
      "(Iteration 711 / 7640) loss: 5.248698\n",
      "(Iteration 721 / 7640) loss: 5.310861\n",
      "(Iteration 731 / 7640) loss: 5.299218\n",
      "(Iteration 741 / 7640) loss: 5.357836\n",
      "(Iteration 751 / 7640) loss: 5.342918\n",
      "(Iteration 761 / 7640) loss: 5.304230\n",
      "(Epoch 2 / 20) train acc: 0.274000; val_acc: 0.285000\n",
      "(Iteration 771 / 7640) loss: 5.373577\n",
      "(Iteration 781 / 7640) loss: 5.312680\n",
      "(Iteration 791 / 7640) loss: 5.339965\n",
      "(Iteration 801 / 7640) loss: 5.285919\n",
      "(Iteration 811 / 7640) loss: 5.245269\n",
      "(Iteration 821 / 7640) loss: 5.333719\n",
      "(Iteration 831 / 7640) loss: 5.362026\n",
      "(Iteration 841 / 7640) loss: 5.371745\n",
      "(Iteration 851 / 7640) loss: 5.422892\n",
      "(Iteration 861 / 7640) loss: 5.304149\n",
      "(Iteration 871 / 7640) loss: 5.446567\n",
      "(Iteration 881 / 7640) loss: 5.290863\n",
      "(Iteration 891 / 7640) loss: 5.376572\n",
      "(Iteration 901 / 7640) loss: 5.379546\n",
      "(Iteration 911 / 7640) loss: 5.365828\n",
      "(Iteration 921 / 7640) loss: 5.260962\n",
      "(Iteration 931 / 7640) loss: 5.276731\n",
      "(Iteration 941 / 7640) loss: 5.343593\n",
      "(Iteration 951 / 7640) loss: 5.230687\n",
      "(Iteration 961 / 7640) loss: 5.248630\n",
      "(Iteration 971 / 7640) loss: 5.252664\n",
      "(Iteration 981 / 7640) loss: 5.380785\n",
      "(Iteration 991 / 7640) loss: 5.246124\n",
      "(Iteration 1001 / 7640) loss: 5.280979\n",
      "(Iteration 1011 / 7640) loss: 5.225032\n",
      "(Iteration 1021 / 7640) loss: 5.270024\n",
      "(Iteration 1031 / 7640) loss: 5.230004\n",
      "(Iteration 1041 / 7640) loss: 5.282671\n",
      "(Iteration 1051 / 7640) loss: 5.235337\n",
      "(Iteration 1061 / 7640) loss: 5.175841\n",
      "(Iteration 1071 / 7640) loss: 5.172820\n",
      "(Iteration 1081 / 7640) loss: 5.197172\n",
      "(Iteration 1091 / 7640) loss: 5.218139\n",
      "(Iteration 1101 / 7640) loss: 5.315422\n",
      "(Iteration 1111 / 7640) loss: 5.165926\n",
      "(Iteration 1121 / 7640) loss: 5.134925\n",
      "(Iteration 1131 / 7640) loss: 5.363480\n",
      "(Iteration 1141 / 7640) loss: 5.266812\n",
      "(Epoch 3 / 20) train acc: 0.327000; val_acc: 0.334000\n",
      "(Iteration 1151 / 7640) loss: 5.247145\n",
      "(Iteration 1161 / 7640) loss: 5.102590\n",
      "(Iteration 1171 / 7640) loss: 5.111906\n",
      "(Iteration 1181 / 7640) loss: 5.148421\n",
      "(Iteration 1191 / 7640) loss: 5.293079\n",
      "(Iteration 1201 / 7640) loss: 5.193525\n",
      "(Iteration 1211 / 7640) loss: 5.201050\n",
      "(Iteration 1221 / 7640) loss: 5.238529\n",
      "(Iteration 1231 / 7640) loss: 5.397517\n",
      "(Iteration 1241 / 7640) loss: 5.146821\n",
      "(Iteration 1251 / 7640) loss: 5.199167\n",
      "(Iteration 1261 / 7640) loss: 5.202732\n",
      "(Iteration 1271 / 7640) loss: 5.148802\n",
      "(Iteration 1281 / 7640) loss: 5.018383\n",
      "(Iteration 1291 / 7640) loss: 5.117329\n",
      "(Iteration 1301 / 7640) loss: 5.157084\n",
      "(Iteration 1311 / 7640) loss: 5.230903\n",
      "(Iteration 1321 / 7640) loss: 5.170485\n",
      "(Iteration 1331 / 7640) loss: 5.029609\n",
      "(Iteration 1341 / 7640) loss: 5.093014\n",
      "(Iteration 1351 / 7640) loss: 5.179891\n",
      "(Iteration 1361 / 7640) loss: 5.105618\n",
      "(Iteration 1371 / 7640) loss: 5.046676\n",
      "(Iteration 1381 / 7640) loss: 5.135337\n",
      "(Iteration 1391 / 7640) loss: 5.145440\n",
      "(Iteration 1401 / 7640) loss: 4.974321\n",
      "(Iteration 1411 / 7640) loss: 5.098849\n",
      "(Iteration 1421 / 7640) loss: 5.111524\n",
      "(Iteration 1431 / 7640) loss: 5.077182\n",
      "(Iteration 1441 / 7640) loss: 5.107034\n",
      "(Iteration 1451 / 7640) loss: 5.150191\n",
      "(Iteration 1461 / 7640) loss: 5.207479\n",
      "(Iteration 1471 / 7640) loss: 4.969051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1481 / 7640) loss: 5.035863\n",
      "(Iteration 1491 / 7640) loss: 5.025498\n",
      "(Iteration 1501 / 7640) loss: 5.199024\n",
      "(Iteration 1511 / 7640) loss: 4.979665\n",
      "(Iteration 1521 / 7640) loss: 5.126280\n",
      "(Epoch 4 / 20) train acc: 0.348000; val_acc: 0.382000\n",
      "(Iteration 1531 / 7640) loss: 5.153242\n",
      "(Iteration 1541 / 7640) loss: 5.026244\n",
      "(Iteration 1551 / 7640) loss: 5.004106\n",
      "(Iteration 1561 / 7640) loss: 4.916711\n",
      "(Iteration 1571 / 7640) loss: 4.989812\n",
      "(Iteration 1581 / 7640) loss: 4.973961\n",
      "(Iteration 1591 / 7640) loss: 4.924611\n",
      "(Iteration 1601 / 7640) loss: 5.122659\n",
      "(Iteration 1611 / 7640) loss: 5.148330\n",
      "(Iteration 1621 / 7640) loss: 4.989244\n",
      "(Iteration 1631 / 7640) loss: 5.123497\n",
      "(Iteration 1641 / 7640) loss: 4.999735\n",
      "(Iteration 1651 / 7640) loss: 5.117968\n",
      "(Iteration 1661 / 7640) loss: 4.957050\n",
      "(Iteration 1671 / 7640) loss: 4.989620\n",
      "(Iteration 1681 / 7640) loss: 5.103168\n",
      "(Iteration 1691 / 7640) loss: 5.213200\n",
      "(Iteration 1701 / 7640) loss: 5.198410\n",
      "(Iteration 1711 / 7640) loss: 5.074697\n",
      "(Iteration 1721 / 7640) loss: 5.064610\n",
      "(Iteration 1731 / 7640) loss: 5.180127\n",
      "(Iteration 1741 / 7640) loss: 4.999663\n",
      "(Iteration 1751 / 7640) loss: 5.018376\n",
      "(Iteration 1761 / 7640) loss: 4.976327\n",
      "(Iteration 1771 / 7640) loss: 5.010202\n",
      "(Iteration 1781 / 7640) loss: 4.999089\n",
      "(Iteration 1791 / 7640) loss: 5.018040\n",
      "(Iteration 1801 / 7640) loss: 4.905638\n",
      "(Iteration 1811 / 7640) loss: 5.041673\n",
      "(Iteration 1821 / 7640) loss: 5.114456\n",
      "(Iteration 1831 / 7640) loss: 4.924146\n",
      "(Iteration 1841 / 7640) loss: 5.120872\n",
      "(Iteration 1851 / 7640) loss: 5.011521\n",
      "(Iteration 1861 / 7640) loss: 4.910726\n",
      "(Iteration 1871 / 7640) loss: 4.911065\n",
      "(Iteration 1881 / 7640) loss: 4.971505\n",
      "(Iteration 1891 / 7640) loss: 5.075196\n",
      "(Iteration 1901 / 7640) loss: 4.983501\n",
      "(Epoch 5 / 20) train acc: 0.373000; val_acc: 0.394000\n",
      "(Iteration 1911 / 7640) loss: 4.873063\n",
      "(Iteration 1921 / 7640) loss: 4.972634\n",
      "(Iteration 1931 / 7640) loss: 4.855639\n",
      "(Iteration 1941 / 7640) loss: 4.904680\n",
      "(Iteration 1951 / 7640) loss: 4.997063\n",
      "(Iteration 1961 / 7640) loss: 5.057780\n",
      "(Iteration 1971 / 7640) loss: 4.929048\n",
      "(Iteration 1981 / 7640) loss: 4.866617\n",
      "(Iteration 1991 / 7640) loss: 4.935414\n",
      "(Iteration 2001 / 7640) loss: 4.908419\n",
      "(Iteration 2011 / 7640) loss: 4.899241\n",
      "(Iteration 2021 / 7640) loss: 4.918795\n",
      "(Iteration 2031 / 7640) loss: 4.962007\n",
      "(Iteration 2041 / 7640) loss: 5.018084\n",
      "(Iteration 2051 / 7640) loss: 4.841195\n",
      "(Iteration 2061 / 7640) loss: 5.103416\n",
      "(Iteration 2071 / 7640) loss: 4.853132\n",
      "(Iteration 2081 / 7640) loss: 4.963130\n",
      "(Iteration 2091 / 7640) loss: 4.925467\n",
      "(Iteration 2101 / 7640) loss: 4.909783\n",
      "(Iteration 2111 / 7640) loss: 4.988125\n",
      "(Iteration 2121 / 7640) loss: 4.969631\n",
      "(Iteration 2131 / 7640) loss: 4.976052\n",
      "(Iteration 2141 / 7640) loss: 4.951552\n",
      "(Iteration 2151 / 7640) loss: 4.879335\n",
      "(Iteration 2161 / 7640) loss: 4.757118\n",
      "(Iteration 2171 / 7640) loss: 5.019142\n",
      "(Iteration 2181 / 7640) loss: 4.895254\n",
      "(Iteration 2191 / 7640) loss: 5.083996\n",
      "(Iteration 2201 / 7640) loss: 4.820353\n",
      "(Iteration 2211 / 7640) loss: 4.801428\n",
      "(Iteration 2221 / 7640) loss: 5.000909\n",
      "(Iteration 2231 / 7640) loss: 4.895551\n",
      "(Iteration 2241 / 7640) loss: 4.971859\n",
      "(Iteration 2251 / 7640) loss: 4.945859\n",
      "(Iteration 2261 / 7640) loss: 4.913215\n",
      "(Iteration 2271 / 7640) loss: 4.872774\n",
      "(Iteration 2281 / 7640) loss: 4.936566\n",
      "(Iteration 2291 / 7640) loss: 4.943972\n",
      "(Epoch 6 / 20) train acc: 0.412000; val_acc: 0.415000\n",
      "(Iteration 2301 / 7640) loss: 4.836320\n",
      "(Iteration 2311 / 7640) loss: 4.875756\n",
      "(Iteration 2321 / 7640) loss: 4.843835\n",
      "(Iteration 2331 / 7640) loss: 4.886432\n",
      "(Iteration 2341 / 7640) loss: 4.857034\n",
      "(Iteration 2351 / 7640) loss: 4.972042\n",
      "(Iteration 2361 / 7640) loss: 5.040186\n",
      "(Iteration 2371 / 7640) loss: 4.936869\n",
      "(Iteration 2381 / 7640) loss: 4.974435\n",
      "(Iteration 2391 / 7640) loss: 4.903986\n",
      "(Iteration 2401 / 7640) loss: 4.873247\n",
      "(Iteration 2411 / 7640) loss: 4.866296\n",
      "(Iteration 2421 / 7640) loss: 4.918814\n",
      "(Iteration 2431 / 7640) loss: 5.024490\n",
      "(Iteration 2441 / 7640) loss: 4.684046\n",
      "(Iteration 2451 / 7640) loss: 4.739871\n",
      "(Iteration 2461 / 7640) loss: 5.065899\n",
      "(Iteration 2471 / 7640) loss: 4.742418\n",
      "(Iteration 2481 / 7640) loss: 4.888827\n",
      "(Iteration 2491 / 7640) loss: 4.869380\n",
      "(Iteration 2501 / 7640) loss: 4.874101\n",
      "(Iteration 2511 / 7640) loss: 5.012257\n",
      "(Iteration 2521 / 7640) loss: 4.875018\n",
      "(Iteration 2531 / 7640) loss: 4.826769\n",
      "(Iteration 2541 / 7640) loss: 4.861588\n",
      "(Iteration 2551 / 7640) loss: 4.886891\n",
      "(Iteration 2561 / 7640) loss: 4.932847\n",
      "(Iteration 2571 / 7640) loss: 4.850262\n",
      "(Iteration 2581 / 7640) loss: 4.931739\n",
      "(Iteration 2591 / 7640) loss: 4.896874\n",
      "(Iteration 2601 / 7640) loss: 4.820418\n",
      "(Iteration 2611 / 7640) loss: 4.680762\n",
      "(Iteration 2621 / 7640) loss: 4.915923\n",
      "(Iteration 2631 / 7640) loss: 4.695016\n",
      "(Iteration 2641 / 7640) loss: 4.896563\n",
      "(Iteration 2651 / 7640) loss: 4.831050\n",
      "(Iteration 2661 / 7640) loss: 4.788701\n",
      "(Iteration 2671 / 7640) loss: 4.774880\n",
      "(Epoch 7 / 20) train acc: 0.442000; val_acc: 0.428000\n",
      "(Iteration 2681 / 7640) loss: 4.758913\n",
      "(Iteration 2691 / 7640) loss: 4.979330\n",
      "(Iteration 2701 / 7640) loss: 4.717480\n",
      "(Iteration 2711 / 7640) loss: 4.885963\n",
      "(Iteration 2721 / 7640) loss: 4.944401\n",
      "(Iteration 2731 / 7640) loss: 4.863007\n",
      "(Iteration 2741 / 7640) loss: 4.743436\n",
      "(Iteration 2751 / 7640) loss: 5.004305\n",
      "(Iteration 2761 / 7640) loss: 4.713973\n",
      "(Iteration 2771 / 7640) loss: 4.861413\n",
      "(Iteration 2781 / 7640) loss: 4.927800\n",
      "(Iteration 2791 / 7640) loss: 4.845373\n",
      "(Iteration 2801 / 7640) loss: 4.675874\n",
      "(Iteration 2811 / 7640) loss: 4.776479\n",
      "(Iteration 2821 / 7640) loss: 4.815146\n",
      "(Iteration 2831 / 7640) loss: 4.757867\n",
      "(Iteration 2841 / 7640) loss: 4.798155\n",
      "(Iteration 2851 / 7640) loss: 4.813866\n",
      "(Iteration 2861 / 7640) loss: 4.791023\n",
      "(Iteration 2871 / 7640) loss: 4.755184\n",
      "(Iteration 2881 / 7640) loss: 4.753681\n",
      "(Iteration 2891 / 7640) loss: 4.803004\n",
      "(Iteration 2901 / 7640) loss: 4.763746\n",
      "(Iteration 2911 / 7640) loss: 4.655240\n",
      "(Iteration 2921 / 7640) loss: 4.748562\n",
      "(Iteration 2931 / 7640) loss: 4.761843\n",
      "(Iteration 2941 / 7640) loss: 4.672059\n",
      "(Iteration 2951 / 7640) loss: 4.784843\n",
      "(Iteration 2961 / 7640) loss: 4.799310\n",
      "(Iteration 2971 / 7640) loss: 4.818465\n",
      "(Iteration 2981 / 7640) loss: 4.662728\n",
      "(Iteration 2991 / 7640) loss: 4.776145\n",
      "(Iteration 3001 / 7640) loss: 4.829090\n",
      "(Iteration 3011 / 7640) loss: 4.675134\n",
      "(Iteration 3021 / 7640) loss: 4.762661\n",
      "(Iteration 3031 / 7640) loss: 4.702880\n",
      "(Iteration 3041 / 7640) loss: 4.718879\n",
      "(Iteration 3051 / 7640) loss: 4.744379\n",
      "(Epoch 8 / 20) train acc: 0.458000; val_acc: 0.453000\n",
      "(Iteration 3061 / 7640) loss: 4.759325\n",
      "(Iteration 3071 / 7640) loss: 4.644397\n",
      "(Iteration 3081 / 7640) loss: 4.798555\n",
      "(Iteration 3091 / 7640) loss: 4.818499\n",
      "(Iteration 3101 / 7640) loss: 4.795252\n",
      "(Iteration 3111 / 7640) loss: 4.767627\n",
      "(Iteration 3121 / 7640) loss: 4.814313\n",
      "(Iteration 3131 / 7640) loss: 4.722834\n",
      "(Iteration 3141 / 7640) loss: 4.826939\n",
      "(Iteration 3151 / 7640) loss: 4.780009\n",
      "(Iteration 3161 / 7640) loss: 4.569088\n",
      "(Iteration 3171 / 7640) loss: 4.650529\n",
      "(Iteration 3181 / 7640) loss: 4.822171\n",
      "(Iteration 3191 / 7640) loss: 4.729342\n",
      "(Iteration 3201 / 7640) loss: 4.836422\n",
      "(Iteration 3211 / 7640) loss: 4.731910\n",
      "(Iteration 3221 / 7640) loss: 4.648364\n",
      "(Iteration 3231 / 7640) loss: 4.600774\n",
      "(Iteration 3241 / 7640) loss: 4.692039\n",
      "(Iteration 3251 / 7640) loss: 4.624364\n",
      "(Iteration 3261 / 7640) loss: 4.766098\n",
      "(Iteration 3271 / 7640) loss: 4.866581\n",
      "(Iteration 3281 / 7640) loss: 4.753503\n",
      "(Iteration 3291 / 7640) loss: 4.729094\n",
      "(Iteration 3301 / 7640) loss: 4.642545\n",
      "(Iteration 3311 / 7640) loss: 4.779549\n",
      "(Iteration 3321 / 7640) loss: 4.654046\n",
      "(Iteration 3331 / 7640) loss: 4.726507\n",
      "(Iteration 3341 / 7640) loss: 4.826722\n",
      "(Iteration 3351 / 7640) loss: 4.667061\n",
      "(Iteration 3361 / 7640) loss: 4.684894\n",
      "(Iteration 3371 / 7640) loss: 4.740696\n",
      "(Iteration 3381 / 7640) loss: 4.689137\n",
      "(Iteration 3391 / 7640) loss: 4.625184\n",
      "(Iteration 3401 / 7640) loss: 4.525521\n",
      "(Iteration 3411 / 7640) loss: 4.709170\n",
      "(Iteration 3421 / 7640) loss: 4.777451\n",
      "(Iteration 3431 / 7640) loss: 4.942730\n",
      "(Epoch 9 / 20) train acc: 0.477000; val_acc: 0.452000\n",
      "(Iteration 3441 / 7640) loss: 4.721877\n",
      "(Iteration 3451 / 7640) loss: 4.671566\n",
      "(Iteration 3461 / 7640) loss: 4.694619\n",
      "(Iteration 3471 / 7640) loss: 4.665195\n",
      "(Iteration 3481 / 7640) loss: 4.670716\n",
      "(Iteration 3491 / 7640) loss: 4.681753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3501 / 7640) loss: 4.838049\n",
      "(Iteration 3511 / 7640) loss: 4.536962\n",
      "(Iteration 3521 / 7640) loss: 4.787865\n",
      "(Iteration 3531 / 7640) loss: 4.619304\n",
      "(Iteration 3541 / 7640) loss: 4.627867\n",
      "(Iteration 3551 / 7640) loss: 4.866726\n",
      "(Iteration 3561 / 7640) loss: 4.697410\n",
      "(Iteration 3571 / 7640) loss: 4.697537\n",
      "(Iteration 3581 / 7640) loss: 4.624747\n",
      "(Iteration 3591 / 7640) loss: 4.771763\n",
      "(Iteration 3601 / 7640) loss: 4.650489\n",
      "(Iteration 3611 / 7640) loss: 4.553597\n",
      "(Iteration 3621 / 7640) loss: 4.731841\n",
      "(Iteration 3631 / 7640) loss: 4.567850\n",
      "(Iteration 3641 / 7640) loss: 4.746845\n",
      "(Iteration 3651 / 7640) loss: 4.635536\n",
      "(Iteration 3661 / 7640) loss: 4.679550\n",
      "(Iteration 3671 / 7640) loss: 4.622432\n",
      "(Iteration 3681 / 7640) loss: 4.583270\n",
      "(Iteration 3691 / 7640) loss: 4.675495\n",
      "(Iteration 3701 / 7640) loss: 4.673859\n",
      "(Iteration 3711 / 7640) loss: 4.744101\n",
      "(Iteration 3721 / 7640) loss: 4.519820\n",
      "(Iteration 3731 / 7640) loss: 4.582392\n",
      "(Iteration 3741 / 7640) loss: 4.721850\n",
      "(Iteration 3751 / 7640) loss: 4.729454\n",
      "(Iteration 3761 / 7640) loss: 4.576433\n",
      "(Iteration 3771 / 7640) loss: 4.608120\n",
      "(Iteration 3781 / 7640) loss: 4.697129\n",
      "(Iteration 3791 / 7640) loss: 4.414494\n",
      "(Iteration 3801 / 7640) loss: 4.703816\n",
      "(Iteration 3811 / 7640) loss: 4.609149\n",
      "(Epoch 10 / 20) train acc: 0.494000; val_acc: 0.465000\n",
      "(Iteration 3821 / 7640) loss: 4.614520\n",
      "(Iteration 3831 / 7640) loss: 4.688431\n",
      "(Iteration 3841 / 7640) loss: 4.566392\n",
      "(Iteration 3851 / 7640) loss: 4.652433\n",
      "(Iteration 3861 / 7640) loss: 4.677344\n",
      "(Iteration 3871 / 7640) loss: 4.692680\n",
      "(Iteration 3881 / 7640) loss: 4.618941\n",
      "(Iteration 3891 / 7640) loss: 4.781730\n",
      "(Iteration 3901 / 7640) loss: 4.663245\n",
      "(Iteration 3911 / 7640) loss: 4.688194\n",
      "(Iteration 3921 / 7640) loss: 4.402045\n",
      "(Iteration 3931 / 7640) loss: 4.788641\n",
      "(Iteration 3941 / 7640) loss: 4.689660\n",
      "(Iteration 3951 / 7640) loss: 4.707626\n",
      "(Iteration 3961 / 7640) loss: 4.567344\n",
      "(Iteration 3971 / 7640) loss: 4.559348\n",
      "(Iteration 3981 / 7640) loss: 4.415029\n",
      "(Iteration 3991 / 7640) loss: 4.727238\n",
      "(Iteration 4001 / 7640) loss: 4.696156\n",
      "(Iteration 4011 / 7640) loss: 4.536576\n",
      "(Iteration 4021 / 7640) loss: 4.561127\n",
      "(Iteration 4031 / 7640) loss: 4.616698\n",
      "(Iteration 4041 / 7640) loss: 4.640004\n",
      "(Iteration 4051 / 7640) loss: 4.618719\n",
      "(Iteration 4061 / 7640) loss: 4.724803\n",
      "(Iteration 4071 / 7640) loss: 4.548141\n",
      "(Iteration 4081 / 7640) loss: 4.552628\n",
      "(Iteration 4091 / 7640) loss: 4.393067\n",
      "(Iteration 4101 / 7640) loss: 4.471347\n",
      "(Iteration 4111 / 7640) loss: 4.550257\n",
      "(Iteration 4121 / 7640) loss: 4.575092\n",
      "(Iteration 4131 / 7640) loss: 4.515212\n",
      "(Iteration 4141 / 7640) loss: 4.499188\n",
      "(Iteration 4151 / 7640) loss: 4.632699\n",
      "(Iteration 4161 / 7640) loss: 4.561585\n",
      "(Iteration 4171 / 7640) loss: 4.605381\n",
      "(Iteration 4181 / 7640) loss: 4.451651\n",
      "(Iteration 4191 / 7640) loss: 4.780470\n",
      "(Iteration 4201 / 7640) loss: 4.667238\n",
      "(Epoch 11 / 20) train acc: 0.522000; val_acc: 0.476000\n",
      "(Iteration 4211 / 7640) loss: 4.620646\n",
      "(Iteration 4221 / 7640) loss: 4.474484\n",
      "(Iteration 4231 / 7640) loss: 4.581888\n",
      "(Iteration 4241 / 7640) loss: 4.616188\n",
      "(Iteration 4251 / 7640) loss: 4.605973\n",
      "(Iteration 4261 / 7640) loss: 4.482234\n",
      "(Iteration 4271 / 7640) loss: 4.489307\n",
      "(Iteration 4281 / 7640) loss: 4.635172\n",
      "(Iteration 4291 / 7640) loss: 4.609552\n",
      "(Iteration 4301 / 7640) loss: 4.574223\n",
      "(Iteration 4311 / 7640) loss: 4.567016\n",
      "(Iteration 4321 / 7640) loss: 4.618843\n",
      "(Iteration 4331 / 7640) loss: 4.526091\n",
      "(Iteration 4341 / 7640) loss: 4.485273\n",
      "(Iteration 4351 / 7640) loss: 4.399945\n",
      "(Iteration 4361 / 7640) loss: 4.686557\n",
      "(Iteration 4371 / 7640) loss: 4.516967\n",
      "(Iteration 4381 / 7640) loss: 4.596621\n",
      "(Iteration 4391 / 7640) loss: 4.553273\n",
      "(Iteration 4401 / 7640) loss: 4.509699\n",
      "(Iteration 4411 / 7640) loss: 4.494953\n",
      "(Iteration 4421 / 7640) loss: 4.454017\n",
      "(Iteration 4431 / 7640) loss: 4.542163\n",
      "(Iteration 4441 / 7640) loss: 4.527800\n",
      "(Iteration 4451 / 7640) loss: 4.491760\n",
      "(Iteration 4461 / 7640) loss: 4.644711\n",
      "(Iteration 4471 / 7640) loss: 4.615568\n",
      "(Iteration 4481 / 7640) loss: 4.520030\n",
      "(Iteration 4491 / 7640) loss: 4.576110\n",
      "(Iteration 4501 / 7640) loss: 4.446207\n",
      "(Iteration 4511 / 7640) loss: 4.709751\n",
      "(Iteration 4521 / 7640) loss: 4.660576\n",
      "(Iteration 4531 / 7640) loss: 4.390881\n",
      "(Iteration 4541 / 7640) loss: 4.504769\n",
      "(Iteration 4551 / 7640) loss: 4.574655\n",
      "(Iteration 4561 / 7640) loss: 4.523392\n",
      "(Iteration 4571 / 7640) loss: 4.607277\n",
      "(Iteration 4581 / 7640) loss: 4.590766\n",
      "(Epoch 12 / 20) train acc: 0.506000; val_acc: 0.472000\n",
      "(Iteration 4591 / 7640) loss: 4.495462\n",
      "(Iteration 4601 / 7640) loss: 4.596053\n",
      "(Iteration 4611 / 7640) loss: 4.452096\n",
      "(Iteration 4621 / 7640) loss: 4.526555\n",
      "(Iteration 4631 / 7640) loss: 4.441794\n",
      "(Iteration 4641 / 7640) loss: 4.571079\n",
      "(Iteration 4651 / 7640) loss: 4.625893\n",
      "(Iteration 4661 / 7640) loss: 4.568703\n",
      "(Iteration 4671 / 7640) loss: 4.625906\n",
      "(Iteration 4681 / 7640) loss: 4.447106\n",
      "(Iteration 4691 / 7640) loss: 4.538102\n",
      "(Iteration 4701 / 7640) loss: 4.525839\n",
      "(Iteration 4711 / 7640) loss: 4.509726\n",
      "(Iteration 4721 / 7640) loss: 4.385031\n",
      "(Iteration 4731 / 7640) loss: 4.320876\n",
      "(Iteration 4741 / 7640) loss: 4.348049\n",
      "(Iteration 4751 / 7640) loss: 4.603369\n",
      "(Iteration 4761 / 7640) loss: 4.522018\n",
      "(Iteration 4771 / 7640) loss: 4.472924\n",
      "(Iteration 4781 / 7640) loss: 4.449347\n",
      "(Iteration 4791 / 7640) loss: 4.599955\n",
      "(Iteration 4801 / 7640) loss: 4.573284\n",
      "(Iteration 4811 / 7640) loss: 4.581103\n",
      "(Iteration 4821 / 7640) loss: 4.525800\n",
      "(Iteration 4831 / 7640) loss: 4.575945\n",
      "(Iteration 4841 / 7640) loss: 4.563066\n",
      "(Iteration 4851 / 7640) loss: 4.602289\n",
      "(Iteration 4861 / 7640) loss: 4.432580\n",
      "(Iteration 4871 / 7640) loss: 4.384149\n",
      "(Iteration 4881 / 7640) loss: 4.478859\n",
      "(Iteration 4891 / 7640) loss: 4.590223\n",
      "(Iteration 4901 / 7640) loss: 4.520722\n",
      "(Iteration 4911 / 7640) loss: 4.564811\n",
      "(Iteration 4921 / 7640) loss: 4.526603\n",
      "(Iteration 4931 / 7640) loss: 4.345061\n",
      "(Iteration 4941 / 7640) loss: 4.427814\n",
      "(Iteration 4951 / 7640) loss: 4.554384\n",
      "(Iteration 4961 / 7640) loss: 4.508582\n",
      "(Epoch 13 / 20) train acc: 0.474000; val_acc: 0.490000\n",
      "(Iteration 4971 / 7640) loss: 4.476902\n",
      "(Iteration 4981 / 7640) loss: 4.461298\n",
      "(Iteration 4991 / 7640) loss: 4.438493\n",
      "(Iteration 5001 / 7640) loss: 4.625685\n",
      "(Iteration 5011 / 7640) loss: 4.446777\n",
      "(Iteration 5021 / 7640) loss: 4.453538\n",
      "(Iteration 5031 / 7640) loss: 4.445806\n",
      "(Iteration 5041 / 7640) loss: 4.576708\n",
      "(Iteration 5051 / 7640) loss: 4.459811\n",
      "(Iteration 5061 / 7640) loss: 4.475885\n",
      "(Iteration 5071 / 7640) loss: 4.419417\n",
      "(Iteration 5081 / 7640) loss: 4.469562\n",
      "(Iteration 5091 / 7640) loss: 4.540678\n",
      "(Iteration 5101 / 7640) loss: 4.310361\n",
      "(Iteration 5111 / 7640) loss: 4.555616\n",
      "(Iteration 5121 / 7640) loss: 4.375222\n",
      "(Iteration 5131 / 7640) loss: 4.371758\n",
      "(Iteration 5141 / 7640) loss: 4.293437\n",
      "(Iteration 5151 / 7640) loss: 4.384883\n",
      "(Iteration 5161 / 7640) loss: 4.652455\n",
      "(Iteration 5171 / 7640) loss: 4.553970\n",
      "(Iteration 5181 / 7640) loss: 4.319940\n",
      "(Iteration 5191 / 7640) loss: 4.339983\n",
      "(Iteration 5201 / 7640) loss: 4.537066\n",
      "(Iteration 5211 / 7640) loss: 4.379185\n",
      "(Iteration 5221 / 7640) loss: 4.474772\n",
      "(Iteration 5231 / 7640) loss: 4.468297\n",
      "(Iteration 5241 / 7640) loss: 4.503588\n",
      "(Iteration 5251 / 7640) loss: 4.416343\n",
      "(Iteration 5261 / 7640) loss: 4.257514\n",
      "(Iteration 5271 / 7640) loss: 4.320656\n",
      "(Iteration 5281 / 7640) loss: 4.455574\n",
      "(Iteration 5291 / 7640) loss: 4.350480\n",
      "(Iteration 5301 / 7640) loss: 4.441948\n",
      "(Iteration 5311 / 7640) loss: 4.294073\n",
      "(Iteration 5321 / 7640) loss: 4.497963\n",
      "(Iteration 5331 / 7640) loss: 4.283859\n",
      "(Iteration 5341 / 7640) loss: 4.287837\n",
      "(Epoch 14 / 20) train acc: 0.511000; val_acc: 0.486000\n",
      "(Iteration 5351 / 7640) loss: 4.564439\n",
      "(Iteration 5361 / 7640) loss: 4.421047\n",
      "(Iteration 5371 / 7640) loss: 4.566564\n",
      "(Iteration 5381 / 7640) loss: 4.327060\n",
      "(Iteration 5391 / 7640) loss: 4.331021\n",
      "(Iteration 5401 / 7640) loss: 4.396464\n",
      "(Iteration 5411 / 7640) loss: 4.452321\n",
      "(Iteration 5421 / 7640) loss: 4.322011\n",
      "(Iteration 5431 / 7640) loss: 4.361691\n",
      "(Iteration 5441 / 7640) loss: 4.349236\n",
      "(Iteration 5451 / 7640) loss: 4.265252\n",
      "(Iteration 5461 / 7640) loss: 4.372227\n",
      "(Iteration 5471 / 7640) loss: 4.417887\n",
      "(Iteration 5481 / 7640) loss: 4.394575\n",
      "(Iteration 5491 / 7640) loss: 4.442219\n",
      "(Iteration 5501 / 7640) loss: 4.289904\n",
      "(Iteration 5511 / 7640) loss: 4.394408\n",
      "(Iteration 5521 / 7640) loss: 4.348516\n",
      "(Iteration 5531 / 7640) loss: 4.427058\n",
      "(Iteration 5541 / 7640) loss: 4.339885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 5551 / 7640) loss: 4.460886\n",
      "(Iteration 5561 / 7640) loss: 4.211720\n",
      "(Iteration 5571 / 7640) loss: 4.469740\n",
      "(Iteration 5581 / 7640) loss: 4.376575\n",
      "(Iteration 5591 / 7640) loss: 4.345534\n",
      "(Iteration 5601 / 7640) loss: 4.350123\n",
      "(Iteration 5611 / 7640) loss: 4.206828\n",
      "(Iteration 5621 / 7640) loss: 4.494373\n",
      "(Iteration 5631 / 7640) loss: 4.389198\n",
      "(Iteration 5641 / 7640) loss: 4.308266\n",
      "(Iteration 5651 / 7640) loss: 4.380286\n",
      "(Iteration 5661 / 7640) loss: 4.477584\n",
      "(Iteration 5671 / 7640) loss: 4.337708\n",
      "(Iteration 5681 / 7640) loss: 4.361718\n",
      "(Iteration 5691 / 7640) loss: 4.274068\n",
      "(Iteration 5701 / 7640) loss: 4.434887\n",
      "(Iteration 5711 / 7640) loss: 4.396776\n",
      "(Iteration 5721 / 7640) loss: 4.452023\n",
      "(Epoch 15 / 20) train acc: 0.529000; val_acc: 0.491000\n",
      "(Iteration 5731 / 7640) loss: 4.431558\n",
      "(Iteration 5741 / 7640) loss: 4.406528\n",
      "(Iteration 5751 / 7640) loss: 4.526988\n",
      "(Iteration 5761 / 7640) loss: 4.328625\n",
      "(Iteration 5771 / 7640) loss: 4.381889\n",
      "(Iteration 5781 / 7640) loss: 4.327862\n",
      "(Iteration 5791 / 7640) loss: 4.233789\n",
      "(Iteration 5801 / 7640) loss: 4.218141\n",
      "(Iteration 5811 / 7640) loss: 4.356915\n",
      "(Iteration 5821 / 7640) loss: 4.294300\n",
      "(Iteration 5831 / 7640) loss: 4.231719\n",
      "(Iteration 5841 / 7640) loss: 4.314453\n",
      "(Iteration 5851 / 7640) loss: 4.223553\n",
      "(Iteration 5861 / 7640) loss: 4.329217\n",
      "(Iteration 5871 / 7640) loss: 4.375788\n",
      "(Iteration 5881 / 7640) loss: 4.281723\n",
      "(Iteration 5891 / 7640) loss: 4.273159\n",
      "(Iteration 5901 / 7640) loss: 4.525830\n",
      "(Iteration 5911 / 7640) loss: 4.472323\n",
      "(Iteration 5921 / 7640) loss: 4.234154\n",
      "(Iteration 5931 / 7640) loss: 4.184988\n",
      "(Iteration 5941 / 7640) loss: 4.191523\n",
      "(Iteration 5951 / 7640) loss: 4.304054\n",
      "(Iteration 5961 / 7640) loss: 4.427750\n",
      "(Iteration 5971 / 7640) loss: 4.229296\n",
      "(Iteration 5981 / 7640) loss: 4.353854\n",
      "(Iteration 5991 / 7640) loss: 4.348243\n",
      "(Iteration 6001 / 7640) loss: 4.335876\n",
      "(Iteration 6011 / 7640) loss: 4.124286\n",
      "(Iteration 6021 / 7640) loss: 4.441786\n",
      "(Iteration 6031 / 7640) loss: 4.384016\n",
      "(Iteration 6041 / 7640) loss: 4.421937\n",
      "(Iteration 6051 / 7640) loss: 4.409118\n",
      "(Iteration 6061 / 7640) loss: 4.420604\n",
      "(Iteration 6071 / 7640) loss: 4.315446\n",
      "(Iteration 6081 / 7640) loss: 4.281960\n",
      "(Iteration 6091 / 7640) loss: 4.329011\n",
      "(Iteration 6101 / 7640) loss: 4.316590\n",
      "(Iteration 6111 / 7640) loss: 4.257534\n",
      "(Epoch 16 / 20) train acc: 0.544000; val_acc: 0.516000\n",
      "(Iteration 6121 / 7640) loss: 4.409949\n",
      "(Iteration 6131 / 7640) loss: 4.211161\n",
      "(Iteration 6141 / 7640) loss: 4.203543\n",
      "(Iteration 6151 / 7640) loss: 4.361645\n",
      "(Iteration 6161 / 7640) loss: 4.427267\n",
      "(Iteration 6171 / 7640) loss: 4.223516\n",
      "(Iteration 6181 / 7640) loss: 4.239304\n",
      "(Iteration 6191 / 7640) loss: 4.298995\n",
      "(Iteration 6201 / 7640) loss: 4.311665\n",
      "(Iteration 6211 / 7640) loss: 4.466911\n",
      "(Iteration 6221 / 7640) loss: 4.290818\n",
      "(Iteration 6231 / 7640) loss: 4.143436\n",
      "(Iteration 6241 / 7640) loss: 4.198671\n",
      "(Iteration 6251 / 7640) loss: 4.357959\n",
      "(Iteration 6261 / 7640) loss: 4.144173\n",
      "(Iteration 6271 / 7640) loss: 4.220082\n",
      "(Iteration 6281 / 7640) loss: 4.258301\n",
      "(Iteration 6291 / 7640) loss: 4.410781\n",
      "(Iteration 6301 / 7640) loss: 4.318355\n",
      "(Iteration 6311 / 7640) loss: 4.262493\n",
      "(Iteration 6321 / 7640) loss: 4.352426\n",
      "(Iteration 6331 / 7640) loss: 4.316523\n",
      "(Iteration 6341 / 7640) loss: 4.487787\n",
      "(Iteration 6351 / 7640) loss: 4.120858\n",
      "(Iteration 6361 / 7640) loss: 4.191233\n",
      "(Iteration 6371 / 7640) loss: 4.232770\n",
      "(Iteration 6381 / 7640) loss: 4.295984\n",
      "(Iteration 6391 / 7640) loss: 4.332756\n",
      "(Iteration 6401 / 7640) loss: 4.123577\n",
      "(Iteration 6411 / 7640) loss: 4.347985\n",
      "(Iteration 6421 / 7640) loss: 4.373765\n",
      "(Iteration 6431 / 7640) loss: 4.152988\n",
      "(Iteration 6441 / 7640) loss: 4.231177\n",
      "(Iteration 6451 / 7640) loss: 4.251395\n",
      "(Iteration 6461 / 7640) loss: 4.293247\n",
      "(Iteration 6471 / 7640) loss: 4.135300\n",
      "(Iteration 6481 / 7640) loss: 4.161256\n",
      "(Iteration 6491 / 7640) loss: 4.333190\n",
      "(Epoch 17 / 20) train acc: 0.561000; val_acc: 0.511000\n",
      "(Iteration 6501 / 7640) loss: 4.140957\n",
      "(Iteration 6511 / 7640) loss: 4.263938\n",
      "(Iteration 6521 / 7640) loss: 4.242692\n",
      "(Iteration 6531 / 7640) loss: 4.220343\n",
      "(Iteration 6541 / 7640) loss: 4.039708\n",
      "(Iteration 6551 / 7640) loss: 4.294010\n",
      "(Iteration 6561 / 7640) loss: 4.193832\n",
      "(Iteration 6571 / 7640) loss: 4.338231\n",
      "(Iteration 6581 / 7640) loss: 4.525791\n",
      "(Iteration 6591 / 7640) loss: 4.170774\n",
      "(Iteration 6601 / 7640) loss: 4.270813\n",
      "(Iteration 6611 / 7640) loss: 4.228413\n",
      "(Iteration 6621 / 7640) loss: 4.090977\n",
      "(Iteration 6631 / 7640) loss: 4.194132\n",
      "(Iteration 6641 / 7640) loss: 4.206935\n",
      "(Iteration 6651 / 7640) loss: 4.230887\n",
      "(Iteration 6661 / 7640) loss: 4.124422\n",
      "(Iteration 6671 / 7640) loss: 4.239028\n",
      "(Iteration 6681 / 7640) loss: 4.330315\n",
      "(Iteration 6691 / 7640) loss: 4.235545\n",
      "(Iteration 6701 / 7640) loss: 4.227128\n",
      "(Iteration 6711 / 7640) loss: 4.283221\n",
      "(Iteration 6721 / 7640) loss: 4.393313\n",
      "(Iteration 6731 / 7640) loss: 4.237756\n",
      "(Iteration 6741 / 7640) loss: 4.072338\n",
      "(Iteration 6751 / 7640) loss: 4.230129\n",
      "(Iteration 6761 / 7640) loss: 4.322218\n",
      "(Iteration 6771 / 7640) loss: 4.232878\n",
      "(Iteration 6781 / 7640) loss: 4.017202\n",
      "(Iteration 6791 / 7640) loss: 4.155153\n",
      "(Iteration 6801 / 7640) loss: 4.172248\n",
      "(Iteration 6811 / 7640) loss: 4.078096\n",
      "(Iteration 6821 / 7640) loss: 4.142631\n",
      "(Iteration 6831 / 7640) loss: 4.382518\n",
      "(Iteration 6841 / 7640) loss: 4.389172\n",
      "(Iteration 6851 / 7640) loss: 4.233436\n",
      "(Iteration 6861 / 7640) loss: 4.348533\n",
      "(Iteration 6871 / 7640) loss: 4.196646\n",
      "(Epoch 18 / 20) train acc: 0.543000; val_acc: 0.515000\n",
      "(Iteration 6881 / 7640) loss: 4.200060\n",
      "(Iteration 6891 / 7640) loss: 4.284342\n",
      "(Iteration 6901 / 7640) loss: 4.104987\n",
      "(Iteration 6911 / 7640) loss: 4.341201\n",
      "(Iteration 6921 / 7640) loss: 4.046882\n",
      "(Iteration 6931 / 7640) loss: 4.161739\n",
      "(Iteration 6941 / 7640) loss: 4.118423\n",
      "(Iteration 6951 / 7640) loss: 4.302684\n",
      "(Iteration 6961 / 7640) loss: 4.200455\n",
      "(Iteration 6971 / 7640) loss: 4.461881\n",
      "(Iteration 6981 / 7640) loss: 4.256921\n",
      "(Iteration 6991 / 7640) loss: 4.440416\n",
      "(Iteration 7001 / 7640) loss: 4.129543\n",
      "(Iteration 7011 / 7640) loss: 4.104093\n",
      "(Iteration 7021 / 7640) loss: 4.290342\n",
      "(Iteration 7031 / 7640) loss: 4.261234\n",
      "(Iteration 7041 / 7640) loss: 4.138631\n",
      "(Iteration 7051 / 7640) loss: 4.221071\n",
      "(Iteration 7061 / 7640) loss: 4.154251\n",
      "(Iteration 7071 / 7640) loss: 4.331985\n",
      "(Iteration 7081 / 7640) loss: 4.083889\n",
      "(Iteration 7091 / 7640) loss: 4.109292\n",
      "(Iteration 7101 / 7640) loss: 4.208773\n",
      "(Iteration 7111 / 7640) loss: 4.264226\n",
      "(Iteration 7121 / 7640) loss: 4.219694\n",
      "(Iteration 7131 / 7640) loss: 4.205329\n",
      "(Iteration 7141 / 7640) loss: 4.224737\n",
      "(Iteration 7151 / 7640) loss: 4.299694\n",
      "(Iteration 7161 / 7640) loss: 4.346659\n",
      "(Iteration 7171 / 7640) loss: 4.114973\n",
      "(Iteration 7181 / 7640) loss: 4.275437\n",
      "(Iteration 7191 / 7640) loss: 4.143071\n",
      "(Iteration 7201 / 7640) loss: 4.162015\n",
      "(Iteration 7211 / 7640) loss: 4.168505\n",
      "(Iteration 7221 / 7640) loss: 4.160772\n",
      "(Iteration 7231 / 7640) loss: 4.217989\n",
      "(Iteration 7241 / 7640) loss: 4.224502\n",
      "(Iteration 7251 / 7640) loss: 4.110336\n",
      "(Epoch 19 / 20) train acc: 0.573000; val_acc: 0.501000\n",
      "(Iteration 7261 / 7640) loss: 4.162378\n",
      "(Iteration 7271 / 7640) loss: 4.046064\n",
      "(Iteration 7281 / 7640) loss: 4.264036\n",
      "(Iteration 7291 / 7640) loss: 4.074948\n",
      "(Iteration 7301 / 7640) loss: 4.276202\n",
      "(Iteration 7311 / 7640) loss: 4.183518\n",
      "(Iteration 7321 / 7640) loss: 4.262787\n",
      "(Iteration 7331 / 7640) loss: 4.079909\n",
      "(Iteration 7341 / 7640) loss: 4.204447\n",
      "(Iteration 7351 / 7640) loss: 4.061314\n",
      "(Iteration 7361 / 7640) loss: 4.179232\n",
      "(Iteration 7371 / 7640) loss: 4.071894\n",
      "(Iteration 7381 / 7640) loss: 4.294983\n",
      "(Iteration 7391 / 7640) loss: 4.077856\n",
      "(Iteration 7401 / 7640) loss: 4.149865\n",
      "(Iteration 7411 / 7640) loss: 4.190806\n",
      "(Iteration 7421 / 7640) loss: 3.970593\n",
      "(Iteration 7431 / 7640) loss: 4.280702\n",
      "(Iteration 7441 / 7640) loss: 4.113404\n",
      "(Iteration 7451 / 7640) loss: 4.344990\n",
      "(Iteration 7461 / 7640) loss: 4.090012\n",
      "(Iteration 7471 / 7640) loss: 4.210550\n",
      "(Iteration 7481 / 7640) loss: 4.344772\n",
      "(Iteration 7491 / 7640) loss: 4.110886\n",
      "(Iteration 7501 / 7640) loss: 4.125753\n",
      "(Iteration 7511 / 7640) loss: 4.219279\n",
      "(Iteration 7521 / 7640) loss: 4.376400\n",
      "(Iteration 7531 / 7640) loss: 4.126350\n",
      "(Iteration 7541 / 7640) loss: 4.142134\n",
      "(Iteration 7551 / 7640) loss: 4.351715\n",
      "(Iteration 7561 / 7640) loss: 4.041968\n",
      "(Iteration 7571 / 7640) loss: 4.152356\n",
      "(Iteration 7581 / 7640) loss: 4.012058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7591 / 7640) loss: 4.133639\n",
      "(Iteration 7601 / 7640) loss: 4.264532\n",
      "(Iteration 7611 / 7640) loss: 4.182169\n",
      "(Iteration 7621 / 7640) loss: 4.099981\n",
      "(Iteration 7631 / 7640) loss: 4.143907\n",
      "(Epoch 20 / 20) train acc: 0.594000; val_acc: 0.505000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.516000\n",
      "running with  sgd_momentum\n",
      "(Iteration 1 / 7640) loss: 6.509876\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 11 / 7640) loss: 6.468231\n",
      "(Iteration 21 / 7640) loss: 6.345715\n",
      "(Iteration 31 / 7640) loss: 6.244817\n",
      "(Iteration 41 / 7640) loss: 6.201848\n",
      "(Iteration 51 / 7640) loss: 6.129910\n",
      "(Iteration 61 / 7640) loss: 6.056646\n",
      "(Iteration 71 / 7640) loss: 6.022939\n",
      "(Iteration 81 / 7640) loss: 6.028519\n",
      "(Iteration 91 / 7640) loss: 5.929732\n",
      "(Iteration 101 / 7640) loss: 5.788520\n",
      "(Iteration 111 / 7640) loss: 5.828610\n",
      "(Iteration 121 / 7640) loss: 5.824266\n",
      "(Iteration 131 / 7640) loss: 5.769546\n",
      "(Iteration 141 / 7640) loss: 5.708364\n",
      "(Iteration 151 / 7640) loss: 5.589085\n",
      "(Iteration 161 / 7640) loss: 5.555932\n",
      "(Iteration 171 / 7640) loss: 5.644318\n",
      "(Iteration 181 / 7640) loss: 5.567793\n",
      "(Iteration 191 / 7640) loss: 5.504013\n",
      "(Iteration 201 / 7640) loss: 5.481062\n",
      "(Iteration 211 / 7640) loss: 5.531780\n",
      "(Iteration 221 / 7640) loss: 5.479557\n",
      "(Iteration 231 / 7640) loss: 5.515620\n",
      "(Iteration 241 / 7640) loss: 5.338339\n",
      "(Iteration 251 / 7640) loss: 5.411862\n",
      "(Iteration 261 / 7640) loss: 5.355561\n",
      "(Iteration 271 / 7640) loss: 5.283170\n",
      "(Iteration 281 / 7640) loss: 5.284491\n",
      "(Iteration 291 / 7640) loss: 5.283077\n",
      "(Iteration 301 / 7640) loss: 5.320790\n",
      "(Iteration 311 / 7640) loss: 5.163151\n",
      "(Iteration 321 / 7640) loss: 5.250722\n",
      "(Iteration 331 / 7640) loss: 5.215195\n",
      "(Iteration 341 / 7640) loss: 5.378275\n",
      "(Iteration 351 / 7640) loss: 5.246180\n",
      "(Iteration 361 / 7640) loss: 5.356775\n",
      "(Iteration 371 / 7640) loss: 5.151196\n",
      "(Iteration 381 / 7640) loss: 5.436857\n",
      "(Epoch 1 / 20) train acc: 0.427000; val_acc: 0.424000\n",
      "(Iteration 391 / 7640) loss: 5.181895\n",
      "(Iteration 401 / 7640) loss: 5.347617\n",
      "(Iteration 411 / 7640) loss: 5.211612\n",
      "(Iteration 421 / 7640) loss: 5.092155\n",
      "(Iteration 431 / 7640) loss: 5.006998\n",
      "(Iteration 441 / 7640) loss: 5.174288\n",
      "(Iteration 451 / 7640) loss: 5.212175\n",
      "(Iteration 461 / 7640) loss: 5.111234\n",
      "(Iteration 471 / 7640) loss: 5.048251\n",
      "(Iteration 481 / 7640) loss: 5.095697\n",
      "(Iteration 491 / 7640) loss: 5.131569\n",
      "(Iteration 501 / 7640) loss: 4.905914\n",
      "(Iteration 511 / 7640) loss: 5.007541\n",
      "(Iteration 521 / 7640) loss: 5.078742\n",
      "(Iteration 531 / 7640) loss: 4.918235\n",
      "(Iteration 541 / 7640) loss: 4.996207\n",
      "(Iteration 551 / 7640) loss: 5.125534\n",
      "(Iteration 561 / 7640) loss: 5.174527\n",
      "(Iteration 571 / 7640) loss: 4.927247\n",
      "(Iteration 581 / 7640) loss: 5.027301\n",
      "(Iteration 591 / 7640) loss: 5.035218\n",
      "(Iteration 601 / 7640) loss: 4.779803\n",
      "(Iteration 611 / 7640) loss: 5.018404\n",
      "(Iteration 621 / 7640) loss: 4.798072\n",
      "(Iteration 631 / 7640) loss: 4.809504\n",
      "(Iteration 641 / 7640) loss: 4.932166\n",
      "(Iteration 651 / 7640) loss: 5.042067\n",
      "(Iteration 661 / 7640) loss: 4.945069\n",
      "(Iteration 671 / 7640) loss: 4.820624\n",
      "(Iteration 681 / 7640) loss: 4.728470\n",
      "(Iteration 691 / 7640) loss: 4.937997\n",
      "(Iteration 701 / 7640) loss: 4.784193\n",
      "(Iteration 711 / 7640) loss: 4.888350\n",
      "(Iteration 721 / 7640) loss: 4.977351\n",
      "(Iteration 731 / 7640) loss: 4.877130\n",
      "(Iteration 741 / 7640) loss: 4.820714\n",
      "(Iteration 751 / 7640) loss: 4.780771\n",
      "(Iteration 761 / 7640) loss: 4.830256\n",
      "(Epoch 2 / 20) train acc: 0.491000; val_acc: 0.491000\n",
      "(Iteration 771 / 7640) loss: 4.810094\n",
      "(Iteration 781 / 7640) loss: 4.796727\n",
      "(Iteration 791 / 7640) loss: 4.812441\n",
      "(Iteration 801 / 7640) loss: 4.859538\n",
      "(Iteration 811 / 7640) loss: 4.842357\n",
      "(Iteration 821 / 7640) loss: 4.582732\n",
      "(Iteration 831 / 7640) loss: 4.749280\n",
      "(Iteration 841 / 7640) loss: 4.564141\n",
      "(Iteration 851 / 7640) loss: 4.899764\n",
      "(Iteration 861 / 7640) loss: 4.579378\n",
      "(Iteration 871 / 7640) loss: 4.768389\n",
      "(Iteration 881 / 7640) loss: 4.593468\n",
      "(Iteration 891 / 7640) loss: 4.600148\n",
      "(Iteration 901 / 7640) loss: 4.676869\n",
      "(Iteration 911 / 7640) loss: 4.589057\n",
      "(Iteration 921 / 7640) loss: 4.632345\n",
      "(Iteration 931 / 7640) loss: 4.571545\n",
      "(Iteration 941 / 7640) loss: 4.587134\n",
      "(Iteration 951 / 7640) loss: 4.660804\n",
      "(Iteration 961 / 7640) loss: 4.591409\n",
      "(Iteration 971 / 7640) loss: 4.590127\n",
      "(Iteration 981 / 7640) loss: 4.506620\n",
      "(Iteration 991 / 7640) loss: 4.636595\n",
      "(Iteration 1001 / 7640) loss: 4.577988\n",
      "(Iteration 1011 / 7640) loss: 4.581418\n",
      "(Iteration 1021 / 7640) loss: 4.592453\n",
      "(Iteration 1031 / 7640) loss: 4.462702\n",
      "(Iteration 1041 / 7640) loss: 4.488571\n",
      "(Iteration 1051 / 7640) loss: 4.457810\n",
      "(Iteration 1061 / 7640) loss: 4.627145\n",
      "(Iteration 1071 / 7640) loss: 4.589871\n",
      "(Iteration 1081 / 7640) loss: 4.607518\n",
      "(Iteration 1091 / 7640) loss: 4.476571\n",
      "(Iteration 1101 / 7640) loss: 4.601108\n",
      "(Iteration 1111 / 7640) loss: 4.334005\n",
      "(Iteration 1121 / 7640) loss: 4.370610\n",
      "(Iteration 1131 / 7640) loss: 4.287693\n",
      "(Iteration 1141 / 7640) loss: 4.491630\n",
      "(Epoch 3 / 20) train acc: 0.516000; val_acc: 0.500000\n",
      "(Iteration 1151 / 7640) loss: 4.338274\n",
      "(Iteration 1161 / 7640) loss: 4.396199\n",
      "(Iteration 1171 / 7640) loss: 4.468789\n",
      "(Iteration 1181 / 7640) loss: 4.529370\n",
      "(Iteration 1191 / 7640) loss: 4.407441\n",
      "(Iteration 1201 / 7640) loss: 4.300776\n",
      "(Iteration 1211 / 7640) loss: 4.336670\n",
      "(Iteration 1221 / 7640) loss: 4.359386\n",
      "(Iteration 1231 / 7640) loss: 4.215185\n",
      "(Iteration 1241 / 7640) loss: 4.453448\n",
      "(Iteration 1251 / 7640) loss: 4.307832\n",
      "(Iteration 1261 / 7640) loss: 4.149410\n",
      "(Iteration 1271 / 7640) loss: 4.331494\n",
      "(Iteration 1281 / 7640) loss: 4.195279\n",
      "(Iteration 1291 / 7640) loss: 4.230439\n",
      "(Iteration 1301 / 7640) loss: 4.342981\n",
      "(Iteration 1311 / 7640) loss: 4.372662\n",
      "(Iteration 1321 / 7640) loss: 4.271026\n",
      "(Iteration 1331 / 7640) loss: 4.318568\n",
      "(Iteration 1341 / 7640) loss: 4.194338\n",
      "(Iteration 1351 / 7640) loss: 4.277145\n",
      "(Iteration 1361 / 7640) loss: 4.106591\n",
      "(Iteration 1371 / 7640) loss: 4.284948\n",
      "(Iteration 1381 / 7640) loss: 4.267335\n",
      "(Iteration 1391 / 7640) loss: 4.045280\n",
      "(Iteration 1401 / 7640) loss: 4.198640\n",
      "(Iteration 1411 / 7640) loss: 4.320470\n",
      "(Iteration 1421 / 7640) loss: 4.116745\n",
      "(Iteration 1431 / 7640) loss: 4.176674\n",
      "(Iteration 1441 / 7640) loss: 4.250472\n",
      "(Iteration 1451 / 7640) loss: 4.270749\n",
      "(Iteration 1461 / 7640) loss: 4.249587\n",
      "(Iteration 1471 / 7640) loss: 4.220357\n",
      "(Iteration 1481 / 7640) loss: 4.072016\n",
      "(Iteration 1491 / 7640) loss: 4.143668\n",
      "(Iteration 1501 / 7640) loss: 4.293745\n",
      "(Iteration 1511 / 7640) loss: 4.147494\n",
      "(Iteration 1521 / 7640) loss: 4.032445\n",
      "(Epoch 4 / 20) train acc: 0.560000; val_acc: 0.521000\n",
      "(Iteration 1531 / 7640) loss: 4.038152\n",
      "(Iteration 1541 / 7640) loss: 4.079301\n",
      "(Iteration 1551 / 7640) loss: 4.057368\n",
      "(Iteration 1561 / 7640) loss: 4.196077\n",
      "(Iteration 1571 / 7640) loss: 4.136014\n",
      "(Iteration 1581 / 7640) loss: 4.009987\n",
      "(Iteration 1591 / 7640) loss: 4.184858\n",
      "(Iteration 1601 / 7640) loss: 4.009209\n",
      "(Iteration 1611 / 7640) loss: 3.974594\n",
      "(Iteration 1621 / 7640) loss: 4.097127\n",
      "(Iteration 1631 / 7640) loss: 3.965184\n",
      "(Iteration 1641 / 7640) loss: 4.180258\n",
      "(Iteration 1651 / 7640) loss: 4.081288\n",
      "(Iteration 1661 / 7640) loss: 4.009111\n",
      "(Iteration 1671 / 7640) loss: 3.960668\n",
      "(Iteration 1681 / 7640) loss: 4.041349\n",
      "(Iteration 1691 / 7640) loss: 3.890198\n",
      "(Iteration 1701 / 7640) loss: 3.928686\n",
      "(Iteration 1711 / 7640) loss: 3.948757\n",
      "(Iteration 1721 / 7640) loss: 3.919490\n",
      "(Iteration 1731 / 7640) loss: 3.954845\n",
      "(Iteration 1741 / 7640) loss: 4.072398\n",
      "(Iteration 1751 / 7640) loss: 3.881314\n",
      "(Iteration 1761 / 7640) loss: 3.996445\n",
      "(Iteration 1771 / 7640) loss: 3.911540\n",
      "(Iteration 1781 / 7640) loss: 4.144968\n",
      "(Iteration 1791 / 7640) loss: 4.033603\n",
      "(Iteration 1801 / 7640) loss: 4.053123\n",
      "(Iteration 1811 / 7640) loss: 3.801396\n",
      "(Iteration 1821 / 7640) loss: 3.996657\n",
      "(Iteration 1831 / 7640) loss: 3.938376\n",
      "(Iteration 1841 / 7640) loss: 3.787531\n",
      "(Iteration 1851 / 7640) loss: 3.936952\n",
      "(Iteration 1861 / 7640) loss: 3.977666\n",
      "(Iteration 1871 / 7640) loss: 3.789892\n",
      "(Iteration 1881 / 7640) loss: 3.894302\n",
      "(Iteration 1891 / 7640) loss: 3.991962\n",
      "(Iteration 1901 / 7640) loss: 3.859561\n",
      "(Epoch 5 / 20) train acc: 0.620000; val_acc: 0.526000\n",
      "(Iteration 1911 / 7640) loss: 3.630612\n",
      "(Iteration 1921 / 7640) loss: 3.831077\n",
      "(Iteration 1931 / 7640) loss: 3.892267\n",
      "(Iteration 1941 / 7640) loss: 3.932693\n",
      "(Iteration 1951 / 7640) loss: 3.935596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1961 / 7640) loss: 4.025418\n",
      "(Iteration 1971 / 7640) loss: 3.683846\n",
      "(Iteration 1981 / 7640) loss: 3.838339\n",
      "(Iteration 1991 / 7640) loss: 3.863472\n",
      "(Iteration 2001 / 7640) loss: 3.790219\n",
      "(Iteration 2011 / 7640) loss: 3.707313\n",
      "(Iteration 2021 / 7640) loss: 3.806842\n",
      "(Iteration 2031 / 7640) loss: 3.711784\n",
      "(Iteration 2041 / 7640) loss: 3.776915\n",
      "(Iteration 2051 / 7640) loss: 3.901771\n",
      "(Iteration 2061 / 7640) loss: 3.777952\n",
      "(Iteration 2071 / 7640) loss: 3.809478\n",
      "(Iteration 2081 / 7640) loss: 3.911563\n",
      "(Iteration 2091 / 7640) loss: 3.579225\n",
      "(Iteration 2101 / 7640) loss: 3.698382\n",
      "(Iteration 2111 / 7640) loss: 3.685011\n",
      "(Iteration 2121 / 7640) loss: 3.757339\n",
      "(Iteration 2131 / 7640) loss: 3.943699\n",
      "(Iteration 2141 / 7640) loss: 3.729659\n",
      "(Iteration 2151 / 7640) loss: 3.682282\n",
      "(Iteration 2161 / 7640) loss: 3.625593\n",
      "(Iteration 2171 / 7640) loss: 3.766558\n",
      "(Iteration 2181 / 7640) loss: 3.735484\n",
      "(Iteration 2191 / 7640) loss: 3.737047\n",
      "(Iteration 2201 / 7640) loss: 3.778350\n",
      "(Iteration 2211 / 7640) loss: 3.816318\n",
      "(Iteration 2221 / 7640) loss: 3.719181\n",
      "(Iteration 2231 / 7640) loss: 3.608123\n",
      "(Iteration 2241 / 7640) loss: 3.774867\n",
      "(Iteration 2251 / 7640) loss: 3.607598\n",
      "(Iteration 2261 / 7640) loss: 3.545497\n",
      "(Iteration 2271 / 7640) loss: 3.656443\n",
      "(Iteration 2281 / 7640) loss: 3.569037\n",
      "(Iteration 2291 / 7640) loss: 3.527945\n",
      "(Epoch 6 / 20) train acc: 0.628000; val_acc: 0.534000\n",
      "(Iteration 2301 / 7640) loss: 3.686969\n",
      "(Iteration 2311 / 7640) loss: 3.759150\n",
      "(Iteration 2321 / 7640) loss: 3.559147\n",
      "(Iteration 2331 / 7640) loss: 3.557757\n",
      "(Iteration 2341 / 7640) loss: 3.711611\n",
      "(Iteration 2351 / 7640) loss: 3.415184\n",
      "(Iteration 2361 / 7640) loss: 3.738630\n",
      "(Iteration 2371 / 7640) loss: 3.500049\n",
      "(Iteration 2381 / 7640) loss: 3.408439\n",
      "(Iteration 2391 / 7640) loss: 3.508006\n",
      "(Iteration 2401 / 7640) loss: 3.686147\n",
      "(Iteration 2411 / 7640) loss: 3.561893\n",
      "(Iteration 2421 / 7640) loss: 3.574380\n",
      "(Iteration 2431 / 7640) loss: 3.562469\n",
      "(Iteration 2441 / 7640) loss: 3.585081\n",
      "(Iteration 2451 / 7640) loss: 3.538013\n",
      "(Iteration 2461 / 7640) loss: 3.592572\n",
      "(Iteration 2471 / 7640) loss: 3.432987\n",
      "(Iteration 2481 / 7640) loss: 3.245124\n",
      "(Iteration 2491 / 7640) loss: 3.469089\n",
      "(Iteration 2501 / 7640) loss: 3.559540\n",
      "(Iteration 2511 / 7640) loss: 3.513775\n",
      "(Iteration 2521 / 7640) loss: 3.343167\n",
      "(Iteration 2531 / 7640) loss: 3.481132\n",
      "(Iteration 2541 / 7640) loss: 3.528869\n",
      "(Iteration 2551 / 7640) loss: 3.542154\n",
      "(Iteration 2561 / 7640) loss: 3.332392\n",
      "(Iteration 2571 / 7640) loss: 3.447057\n",
      "(Iteration 2581 / 7640) loss: 3.349362\n",
      "(Iteration 2591 / 7640) loss: 3.578046\n",
      "(Iteration 2601 / 7640) loss: 3.333811\n",
      "(Iteration 2611 / 7640) loss: 3.418899\n",
      "(Iteration 2621 / 7640) loss: 3.517506\n",
      "(Iteration 2631 / 7640) loss: 3.409241\n",
      "(Iteration 2641 / 7640) loss: 3.294508\n",
      "(Iteration 2651 / 7640) loss: 3.381154\n",
      "(Iteration 2661 / 7640) loss: 3.480575\n",
      "(Iteration 2671 / 7640) loss: 3.395603\n",
      "(Epoch 7 / 20) train acc: 0.638000; val_acc: 0.524000\n",
      "(Iteration 2681 / 7640) loss: 3.496338\n",
      "(Iteration 2691 / 7640) loss: 3.288009\n",
      "(Iteration 2701 / 7640) loss: 3.455499\n",
      "(Iteration 2711 / 7640) loss: 3.255094\n",
      "(Iteration 2721 / 7640) loss: 3.608354\n",
      "(Iteration 2731 / 7640) loss: 3.258895\n",
      "(Iteration 2741 / 7640) loss: 3.319914\n",
      "(Iteration 2751 / 7640) loss: 3.406266\n",
      "(Iteration 2761 / 7640) loss: 3.351410\n",
      "(Iteration 2771 / 7640) loss: 3.329188\n",
      "(Iteration 2781 / 7640) loss: 3.302846\n",
      "(Iteration 2791 / 7640) loss: 3.262989\n",
      "(Iteration 2801 / 7640) loss: 3.385826\n",
      "(Iteration 2811 / 7640) loss: 3.408856\n",
      "(Iteration 2821 / 7640) loss: 3.327200\n",
      "(Iteration 2831 / 7640) loss: 3.440663\n",
      "(Iteration 2841 / 7640) loss: 3.419329\n",
      "(Iteration 2851 / 7640) loss: 3.299309\n",
      "(Iteration 2861 / 7640) loss: 3.284779\n",
      "(Iteration 2871 / 7640) loss: 3.414852\n",
      "(Iteration 2881 / 7640) loss: 3.268707\n",
      "(Iteration 2891 / 7640) loss: 3.185401\n",
      "(Iteration 2901 / 7640) loss: 3.373545\n",
      "(Iteration 2911 / 7640) loss: 3.389988\n",
      "(Iteration 2921 / 7640) loss: 3.215529\n",
      "(Iteration 2931 / 7640) loss: 3.350382\n",
      "(Iteration 2941 / 7640) loss: 3.158695\n",
      "(Iteration 2951 / 7640) loss: 3.232647\n",
      "(Iteration 2961 / 7640) loss: 3.224294\n",
      "(Iteration 2971 / 7640) loss: 3.245743\n",
      "(Iteration 2981 / 7640) loss: 3.326114\n",
      "(Iteration 2991 / 7640) loss: 3.207989\n",
      "(Iteration 3001 / 7640) loss: 3.150290\n",
      "(Iteration 3011 / 7640) loss: 3.470284\n",
      "(Iteration 3021 / 7640) loss: 3.357828\n",
      "(Iteration 3031 / 7640) loss: 3.154260\n",
      "(Iteration 3041 / 7640) loss: 3.153473\n",
      "(Iteration 3051 / 7640) loss: 3.123771\n",
      "(Epoch 8 / 20) train acc: 0.663000; val_acc: 0.541000\n",
      "(Iteration 3061 / 7640) loss: 3.125793\n",
      "(Iteration 3071 / 7640) loss: 3.297230\n",
      "(Iteration 3081 / 7640) loss: 3.107060\n",
      "(Iteration 3091 / 7640) loss: 3.202739\n",
      "(Iteration 3101 / 7640) loss: 3.280719\n",
      "(Iteration 3111 / 7640) loss: 3.177622\n",
      "(Iteration 3121 / 7640) loss: 3.155131\n",
      "(Iteration 3131 / 7640) loss: 3.138073\n",
      "(Iteration 3141 / 7640) loss: 3.225984\n",
      "(Iteration 3151 / 7640) loss: 3.152544\n",
      "(Iteration 3161 / 7640) loss: 3.122464\n",
      "(Iteration 3171 / 7640) loss: 2.991693\n",
      "(Iteration 3181 / 7640) loss: 3.051580\n",
      "(Iteration 3191 / 7640) loss: 3.063244\n",
      "(Iteration 3201 / 7640) loss: 3.092777\n",
      "(Iteration 3211 / 7640) loss: 3.212439\n",
      "(Iteration 3221 / 7640) loss: 3.013758\n",
      "(Iteration 3231 / 7640) loss: 3.142387\n",
      "(Iteration 3241 / 7640) loss: 3.125127\n",
      "(Iteration 3251 / 7640) loss: 3.093070\n",
      "(Iteration 3261 / 7640) loss: 3.113224\n",
      "(Iteration 3271 / 7640) loss: 3.023554\n",
      "(Iteration 3281 / 7640) loss: 2.988084\n",
      "(Iteration 3291 / 7640) loss: 3.069687\n",
      "(Iteration 3301 / 7640) loss: 3.031676\n",
      "(Iteration 3311 / 7640) loss: 3.088733\n",
      "(Iteration 3321 / 7640) loss: 3.085095\n",
      "(Iteration 3331 / 7640) loss: 3.050530\n",
      "(Iteration 3341 / 7640) loss: 3.043539\n",
      "(Iteration 3351 / 7640) loss: 3.027737\n",
      "(Iteration 3361 / 7640) loss: 3.073803\n",
      "(Iteration 3371 / 7640) loss: 3.004042\n",
      "(Iteration 3381 / 7640) loss: 3.082904\n",
      "(Iteration 3391 / 7640) loss: 2.925596\n",
      "(Iteration 3401 / 7640) loss: 2.905415\n",
      "(Iteration 3411 / 7640) loss: 2.967060\n",
      "(Iteration 3421 / 7640) loss: 3.163935\n",
      "(Iteration 3431 / 7640) loss: 3.202300\n",
      "(Epoch 9 / 20) train acc: 0.667000; val_acc: 0.542000\n",
      "(Iteration 3441 / 7640) loss: 2.885374\n",
      "(Iteration 3451 / 7640) loss: 3.006713\n",
      "(Iteration 3461 / 7640) loss: 3.171211\n",
      "(Iteration 3471 / 7640) loss: 2.843018\n",
      "(Iteration 3481 / 7640) loss: 2.936127\n",
      "(Iteration 3491 / 7640) loss: 2.888151\n",
      "(Iteration 3501 / 7640) loss: 3.073499\n",
      "(Iteration 3511 / 7640) loss: 3.163837\n",
      "(Iteration 3521 / 7640) loss: 3.026841\n",
      "(Iteration 3531 / 7640) loss: 2.845328\n",
      "(Iteration 3541 / 7640) loss: 2.922249\n",
      "(Iteration 3551 / 7640) loss: 2.991004\n",
      "(Iteration 3561 / 7640) loss: 2.915832\n",
      "(Iteration 3571 / 7640) loss: 2.997327\n",
      "(Iteration 3581 / 7640) loss: 3.126598\n",
      "(Iteration 3591 / 7640) loss: 3.147051\n",
      "(Iteration 3601 / 7640) loss: 2.864619\n",
      "(Iteration 3611 / 7640) loss: 2.882522\n",
      "(Iteration 3621 / 7640) loss: 2.894082\n",
      "(Iteration 3631 / 7640) loss: 2.864094\n",
      "(Iteration 3641 / 7640) loss: 2.877397\n",
      "(Iteration 3651 / 7640) loss: 2.962535\n",
      "(Iteration 3661 / 7640) loss: 2.988509\n",
      "(Iteration 3671 / 7640) loss: 2.810216\n",
      "(Iteration 3681 / 7640) loss: 2.951951\n",
      "(Iteration 3691 / 7640) loss: 2.970597\n",
      "(Iteration 3701 / 7640) loss: 3.063963\n",
      "(Iteration 3711 / 7640) loss: 2.886391\n",
      "(Iteration 3721 / 7640) loss: 3.035403\n",
      "(Iteration 3731 / 7640) loss: 3.120492\n",
      "(Iteration 3741 / 7640) loss: 2.835939\n",
      "(Iteration 3751 / 7640) loss: 2.739742\n",
      "(Iteration 3761 / 7640) loss: 3.051442\n",
      "(Iteration 3771 / 7640) loss: 2.813108\n",
      "(Iteration 3781 / 7640) loss: 2.954244\n",
      "(Iteration 3791 / 7640) loss: 2.990533\n",
      "(Iteration 3801 / 7640) loss: 2.785109\n",
      "(Iteration 3811 / 7640) loss: 2.935455\n",
      "(Epoch 10 / 20) train acc: 0.710000; val_acc: 0.536000\n",
      "(Iteration 3821 / 7640) loss: 2.794812\n",
      "(Iteration 3831 / 7640) loss: 2.800602\n",
      "(Iteration 3841 / 7640) loss: 2.734568\n",
      "(Iteration 3851 / 7640) loss: 2.911686\n",
      "(Iteration 3861 / 7640) loss: 2.809195\n",
      "(Iteration 3871 / 7640) loss: 3.022595\n",
      "(Iteration 3881 / 7640) loss: 2.681627\n",
      "(Iteration 3891 / 7640) loss: 2.910067\n",
      "(Iteration 3901 / 7640) loss: 2.866387\n",
      "(Iteration 3911 / 7640) loss: 2.744724\n",
      "(Iteration 3921 / 7640) loss: 2.696325\n",
      "(Iteration 3931 / 7640) loss: 2.785890\n",
      "(Iteration 3941 / 7640) loss: 2.787544\n",
      "(Iteration 3951 / 7640) loss: 2.612658\n",
      "(Iteration 3961 / 7640) loss: 2.974397\n",
      "(Iteration 3971 / 7640) loss: 2.635372\n",
      "(Iteration 3981 / 7640) loss: 2.783624\n",
      "(Iteration 3991 / 7640) loss: 2.797799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4001 / 7640) loss: 2.681794\n",
      "(Iteration 4011 / 7640) loss: 2.548416\n",
      "(Iteration 4021 / 7640) loss: 2.765003\n",
      "(Iteration 4031 / 7640) loss: 2.861593\n",
      "(Iteration 4041 / 7640) loss: 2.864353\n",
      "(Iteration 4051 / 7640) loss: 2.611807\n",
      "(Iteration 4061 / 7640) loss: 2.787370\n",
      "(Iteration 4071 / 7640) loss: 2.691134\n",
      "(Iteration 4081 / 7640) loss: 2.746314\n",
      "(Iteration 4091 / 7640) loss: 2.804693\n",
      "(Iteration 4101 / 7640) loss: 2.575145\n",
      "(Iteration 4111 / 7640) loss: 2.688811\n",
      "(Iteration 4121 / 7640) loss: 2.744768\n",
      "(Iteration 4131 / 7640) loss: 2.814595\n",
      "(Iteration 4141 / 7640) loss: 2.643386\n",
      "(Iteration 4151 / 7640) loss: 2.727929\n",
      "(Iteration 4161 / 7640) loss: 2.724870\n",
      "(Iteration 4171 / 7640) loss: 2.805906\n",
      "(Iteration 4181 / 7640) loss: 2.910267\n",
      "(Iteration 4191 / 7640) loss: 2.756112\n",
      "(Iteration 4201 / 7640) loss: 2.603711\n",
      "(Epoch 11 / 20) train acc: 0.706000; val_acc: 0.538000\n",
      "(Iteration 4211 / 7640) loss: 2.752630\n",
      "(Iteration 4221 / 7640) loss: 2.645184\n",
      "(Iteration 4231 / 7640) loss: 2.700193\n",
      "(Iteration 4241 / 7640) loss: 2.646393\n",
      "(Iteration 4251 / 7640) loss: 2.668626\n",
      "(Iteration 4261 / 7640) loss: 2.645419\n",
      "(Iteration 4271 / 7640) loss: 2.730907\n",
      "(Iteration 4281 / 7640) loss: 2.597417\n",
      "(Iteration 4291 / 7640) loss: 2.638873\n",
      "(Iteration 4301 / 7640) loss: 2.746536\n",
      "(Iteration 4311 / 7640) loss: 2.785272\n",
      "(Iteration 4321 / 7640) loss: 2.546586\n",
      "(Iteration 4331 / 7640) loss: 2.731277\n",
      "(Iteration 4341 / 7640) loss: 2.481616\n",
      "(Iteration 4351 / 7640) loss: 2.599303\n",
      "(Iteration 4361 / 7640) loss: 2.479532\n",
      "(Iteration 4371 / 7640) loss: 2.544702\n",
      "(Iteration 4381 / 7640) loss: 2.732530\n",
      "(Iteration 4391 / 7640) loss: 2.627930\n",
      "(Iteration 4401 / 7640) loss: 2.687860\n",
      "(Iteration 4411 / 7640) loss: 2.659961\n",
      "(Iteration 4421 / 7640) loss: 2.624347\n",
      "(Iteration 4431 / 7640) loss: 2.524460\n",
      "(Iteration 4441 / 7640) loss: 2.596055\n",
      "(Iteration 4451 / 7640) loss: 2.539956\n",
      "(Iteration 4461 / 7640) loss: 2.688908\n",
      "(Iteration 4471 / 7640) loss: 2.690288\n",
      "(Iteration 4481 / 7640) loss: 2.586783\n",
      "(Iteration 4491 / 7640) loss: 2.546240\n",
      "(Iteration 4501 / 7640) loss: 2.614062\n",
      "(Iteration 4511 / 7640) loss: 2.454727\n",
      "(Iteration 4521 / 7640) loss: 2.410333\n",
      "(Iteration 4531 / 7640) loss: 2.624346\n",
      "(Iteration 4541 / 7640) loss: 2.555391\n",
      "(Iteration 4551 / 7640) loss: 2.667604\n",
      "(Iteration 4561 / 7640) loss: 2.616779\n",
      "(Iteration 4571 / 7640) loss: 2.634926\n",
      "(Iteration 4581 / 7640) loss: 2.494373\n",
      "(Epoch 12 / 20) train acc: 0.750000; val_acc: 0.561000\n",
      "(Iteration 4591 / 7640) loss: 2.425794\n",
      "(Iteration 4601 / 7640) loss: 2.494882\n",
      "(Iteration 4611 / 7640) loss: 2.509601\n",
      "(Iteration 4621 / 7640) loss: 2.543397\n",
      "(Iteration 4631 / 7640) loss: 2.418894\n",
      "(Iteration 4641 / 7640) loss: 2.468960\n",
      "(Iteration 4651 / 7640) loss: 2.526511\n",
      "(Iteration 4661 / 7640) loss: 2.506136\n",
      "(Iteration 4671 / 7640) loss: 2.489156\n",
      "(Iteration 4681 / 7640) loss: 2.470560\n",
      "(Iteration 4691 / 7640) loss: 2.541135\n",
      "(Iteration 4701 / 7640) loss: 2.440769\n",
      "(Iteration 4711 / 7640) loss: 2.411855\n",
      "(Iteration 4721 / 7640) loss: 2.355208\n",
      "(Iteration 4731 / 7640) loss: 2.538232\n",
      "(Iteration 4741 / 7640) loss: 2.369159\n",
      "(Iteration 4751 / 7640) loss: 2.547811\n",
      "(Iteration 4761 / 7640) loss: 2.516604\n",
      "(Iteration 4771 / 7640) loss: 2.581861\n",
      "(Iteration 4781 / 7640) loss: 2.380786\n",
      "(Iteration 4791 / 7640) loss: 2.444165\n",
      "(Iteration 4801 / 7640) loss: 2.552568\n",
      "(Iteration 4811 / 7640) loss: 2.511229\n",
      "(Iteration 4821 / 7640) loss: 2.542317\n",
      "(Iteration 4831 / 7640) loss: 2.360814\n",
      "(Iteration 4841 / 7640) loss: 2.453725\n",
      "(Iteration 4851 / 7640) loss: 2.446927\n",
      "(Iteration 4861 / 7640) loss: 2.360987\n",
      "(Iteration 4871 / 7640) loss: 2.498115\n",
      "(Iteration 4881 / 7640) loss: 2.369619\n",
      "(Iteration 4891 / 7640) loss: 2.550591\n",
      "(Iteration 4901 / 7640) loss: 2.371950\n",
      "(Iteration 4911 / 7640) loss: 2.445331\n",
      "(Iteration 4921 / 7640) loss: 2.339597\n",
      "(Iteration 4931 / 7640) loss: 2.459696\n",
      "(Iteration 4941 / 7640) loss: 2.271667\n",
      "(Iteration 4951 / 7640) loss: 2.351773\n",
      "(Iteration 4961 / 7640) loss: 2.365863\n",
      "(Epoch 13 / 20) train acc: 0.746000; val_acc: 0.557000\n",
      "(Iteration 4971 / 7640) loss: 2.373780\n",
      "(Iteration 4981 / 7640) loss: 2.318428\n",
      "(Iteration 4991 / 7640) loss: 2.536884\n",
      "(Iteration 5001 / 7640) loss: 2.275335\n",
      "(Iteration 5011 / 7640) loss: 2.420664\n",
      "(Iteration 5021 / 7640) loss: 2.267025\n",
      "(Iteration 5031 / 7640) loss: 2.518762\n",
      "(Iteration 5041 / 7640) loss: 2.340141\n",
      "(Iteration 5051 / 7640) loss: 2.524549\n",
      "(Iteration 5061 / 7640) loss: 2.500851\n",
      "(Iteration 5071 / 7640) loss: 2.363280\n",
      "(Iteration 5081 / 7640) loss: 2.391793\n",
      "(Iteration 5091 / 7640) loss: 2.304554\n",
      "(Iteration 5101 / 7640) loss: 2.170730\n",
      "(Iteration 5111 / 7640) loss: 2.512665\n",
      "(Iteration 5121 / 7640) loss: 2.286720\n",
      "(Iteration 5131 / 7640) loss: 2.539141\n",
      "(Iteration 5141 / 7640) loss: 2.297873\n",
      "(Iteration 5151 / 7640) loss: 2.229264\n",
      "(Iteration 5161 / 7640) loss: 2.297080\n",
      "(Iteration 5171 / 7640) loss: 2.232286\n",
      "(Iteration 5181 / 7640) loss: 2.418581\n",
      "(Iteration 5191 / 7640) loss: 2.434249\n",
      "(Iteration 5201 / 7640) loss: 2.518081\n",
      "(Iteration 5211 / 7640) loss: 2.365330\n",
      "(Iteration 5221 / 7640) loss: 2.453380\n",
      "(Iteration 5231 / 7640) loss: 2.208800\n",
      "(Iteration 5241 / 7640) loss: 2.248257\n",
      "(Iteration 5251 / 7640) loss: 2.306449\n",
      "(Iteration 5261 / 7640) loss: 2.461876\n",
      "(Iteration 5271 / 7640) loss: 2.227235\n",
      "(Iteration 5281 / 7640) loss: 2.149466\n",
      "(Iteration 5291 / 7640) loss: 2.375697\n",
      "(Iteration 5301 / 7640) loss: 2.135948\n",
      "(Iteration 5311 / 7640) loss: 2.286968\n",
      "(Iteration 5321 / 7640) loss: 2.250888\n",
      "(Iteration 5331 / 7640) loss: 2.246898\n",
      "(Iteration 5341 / 7640) loss: 2.308305\n",
      "(Epoch 14 / 20) train acc: 0.749000; val_acc: 0.555000\n",
      "(Iteration 5351 / 7640) loss: 2.428903\n",
      "(Iteration 5361 / 7640) loss: 2.296171\n",
      "(Iteration 5371 / 7640) loss: 2.266247\n",
      "(Iteration 5381 / 7640) loss: 2.425892\n",
      "(Iteration 5391 / 7640) loss: 2.264284\n",
      "(Iteration 5401 / 7640) loss: 2.261465\n",
      "(Iteration 5411 / 7640) loss: 2.208595\n",
      "(Iteration 5421 / 7640) loss: 2.250076\n",
      "(Iteration 5431 / 7640) loss: 2.304012\n",
      "(Iteration 5441 / 7640) loss: 2.201501\n",
      "(Iteration 5451 / 7640) loss: 2.193680\n",
      "(Iteration 5461 / 7640) loss: 2.265523\n",
      "(Iteration 5471 / 7640) loss: 2.298537\n",
      "(Iteration 5481 / 7640) loss: 2.255445\n",
      "(Iteration 5491 / 7640) loss: 2.383131\n",
      "(Iteration 5501 / 7640) loss: 2.163695\n",
      "(Iteration 5511 / 7640) loss: 2.284231\n",
      "(Iteration 5521 / 7640) loss: 2.328824\n",
      "(Iteration 5531 / 7640) loss: 2.437611\n",
      "(Iteration 5541 / 7640) loss: 2.283345\n",
      "(Iteration 5551 / 7640) loss: 2.311735\n",
      "(Iteration 5561 / 7640) loss: 2.166369\n",
      "(Iteration 5571 / 7640) loss: 2.225090\n",
      "(Iteration 5581 / 7640) loss: 2.065528\n",
      "(Iteration 5591 / 7640) loss: 2.116350\n",
      "(Iteration 5601 / 7640) loss: 2.373364\n",
      "(Iteration 5611 / 7640) loss: 2.175641\n",
      "(Iteration 5621 / 7640) loss: 2.052740\n",
      "(Iteration 5631 / 7640) loss: 2.083040\n",
      "(Iteration 5641 / 7640) loss: 2.085900\n",
      "(Iteration 5651 / 7640) loss: 2.207668\n",
      "(Iteration 5661 / 7640) loss: 2.124164\n",
      "(Iteration 5671 / 7640) loss: 2.049461\n",
      "(Iteration 5681 / 7640) loss: 2.285520\n",
      "(Iteration 5691 / 7640) loss: 2.173225\n",
      "(Iteration 5701 / 7640) loss: 2.242694\n",
      "(Iteration 5711 / 7640) loss: 2.067845\n",
      "(Iteration 5721 / 7640) loss: 2.084611\n",
      "(Epoch 15 / 20) train acc: 0.770000; val_acc: 0.525000\n",
      "(Iteration 5731 / 7640) loss: 2.055356\n",
      "(Iteration 5741 / 7640) loss: 2.119315\n",
      "(Iteration 5751 / 7640) loss: 2.185752\n",
      "(Iteration 5761 / 7640) loss: 2.154995\n",
      "(Iteration 5771 / 7640) loss: 2.148712\n",
      "(Iteration 5781 / 7640) loss: 2.179666\n",
      "(Iteration 5791 / 7640) loss: 2.044225\n",
      "(Iteration 5801 / 7640) loss: 2.110859\n",
      "(Iteration 5811 / 7640) loss: 2.016898\n",
      "(Iteration 5821 / 7640) loss: 2.127849\n",
      "(Iteration 5831 / 7640) loss: 2.281594\n",
      "(Iteration 5841 / 7640) loss: 2.081212\n",
      "(Iteration 5851 / 7640) loss: 2.044209\n",
      "(Iteration 5861 / 7640) loss: 2.135243\n",
      "(Iteration 5871 / 7640) loss: 2.131101\n",
      "(Iteration 5881 / 7640) loss: 2.105041\n",
      "(Iteration 5891 / 7640) loss: 2.039719\n",
      "(Iteration 5901 / 7640) loss: 1.998139\n",
      "(Iteration 5911 / 7640) loss: 2.125958\n",
      "(Iteration 5921 / 7640) loss: 2.202936\n",
      "(Iteration 5931 / 7640) loss: 2.168027\n",
      "(Iteration 5941 / 7640) loss: 2.058611\n",
      "(Iteration 5951 / 7640) loss: 2.090570\n",
      "(Iteration 5961 / 7640) loss: 2.042454\n",
      "(Iteration 5971 / 7640) loss: 2.091052\n",
      "(Iteration 5981 / 7640) loss: 2.057800\n",
      "(Iteration 5991 / 7640) loss: 2.210037\n",
      "(Iteration 6001 / 7640) loss: 1.992963\n",
      "(Iteration 6011 / 7640) loss: 2.009238\n",
      "(Iteration 6021 / 7640) loss: 2.031105\n",
      "(Iteration 6031 / 7640) loss: 1.891938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 6041 / 7640) loss: 1.996802\n",
      "(Iteration 6051 / 7640) loss: 2.031526\n",
      "(Iteration 6061 / 7640) loss: 2.130973\n",
      "(Iteration 6071 / 7640) loss: 2.053285\n",
      "(Iteration 6081 / 7640) loss: 2.085416\n",
      "(Iteration 6091 / 7640) loss: 2.147548\n",
      "(Iteration 6101 / 7640) loss: 2.109170\n",
      "(Iteration 6111 / 7640) loss: 1.855892\n",
      "(Epoch 16 / 20) train acc: 0.769000; val_acc: 0.558000\n",
      "(Iteration 6121 / 7640) loss: 2.072727\n",
      "(Iteration 6131 / 7640) loss: 2.031423\n",
      "(Iteration 6141 / 7640) loss: 2.064300\n",
      "(Iteration 6151 / 7640) loss: 2.060322\n",
      "(Iteration 6161 / 7640) loss: 2.276056\n",
      "(Iteration 6171 / 7640) loss: 2.052664\n",
      "(Iteration 6181 / 7640) loss: 2.009145\n",
      "(Iteration 6191 / 7640) loss: 2.036620\n",
      "(Iteration 6201 / 7640) loss: 2.015668\n",
      "(Iteration 6211 / 7640) loss: 1.932735\n",
      "(Iteration 6221 / 7640) loss: 1.877654\n",
      "(Iteration 6231 / 7640) loss: 2.133383\n",
      "(Iteration 6241 / 7640) loss: 1.958815\n",
      "(Iteration 6251 / 7640) loss: 2.030548\n",
      "(Iteration 6261 / 7640) loss: 1.987198\n",
      "(Iteration 6271 / 7640) loss: 1.991794\n",
      "(Iteration 6281 / 7640) loss: 2.081397\n",
      "(Iteration 6291 / 7640) loss: 1.972273\n",
      "(Iteration 6301 / 7640) loss: 1.964801\n",
      "(Iteration 6311 / 7640) loss: 1.796360\n",
      "(Iteration 6321 / 7640) loss: 2.017639\n",
      "(Iteration 6331 / 7640) loss: 2.085494\n",
      "(Iteration 6341 / 7640) loss: 2.107771\n",
      "(Iteration 6351 / 7640) loss: 1.962777\n",
      "(Iteration 6361 / 7640) loss: 1.993459\n",
      "(Iteration 6371 / 7640) loss: 1.811153\n",
      "(Iteration 6381 / 7640) loss: 1.886820\n",
      "(Iteration 6391 / 7640) loss: 1.864846\n",
      "(Iteration 6401 / 7640) loss: 1.856638\n",
      "(Iteration 6411 / 7640) loss: 1.968793\n",
      "(Iteration 6421 / 7640) loss: 1.959884\n",
      "(Iteration 6431 / 7640) loss: 2.043850\n",
      "(Iteration 6441 / 7640) loss: 1.960276\n",
      "(Iteration 6451 / 7640) loss: 1.967432\n",
      "(Iteration 6461 / 7640) loss: 1.909786\n",
      "(Iteration 6471 / 7640) loss: 1.940515\n",
      "(Iteration 6481 / 7640) loss: 2.068304\n",
      "(Iteration 6491 / 7640) loss: 2.006348\n",
      "(Epoch 17 / 20) train acc: 0.775000; val_acc: 0.561000\n",
      "(Iteration 6501 / 7640) loss: 1.982471\n",
      "(Iteration 6511 / 7640) loss: 1.947913\n",
      "(Iteration 6521 / 7640) loss: 2.011030\n",
      "(Iteration 6531 / 7640) loss: 1.994638\n",
      "(Iteration 6541 / 7640) loss: 1.901944\n",
      "(Iteration 6551 / 7640) loss: 1.959848\n",
      "(Iteration 6561 / 7640) loss: 1.807213\n",
      "(Iteration 6571 / 7640) loss: 1.960101\n",
      "(Iteration 6581 / 7640) loss: 2.001721\n",
      "(Iteration 6591 / 7640) loss: 1.909760\n",
      "(Iteration 6601 / 7640) loss: 1.897524\n",
      "(Iteration 6611 / 7640) loss: 2.025451\n",
      "(Iteration 6621 / 7640) loss: 1.993268\n",
      "(Iteration 6631 / 7640) loss: 1.875552\n",
      "(Iteration 6641 / 7640) loss: 1.868528\n",
      "(Iteration 6651 / 7640) loss: 1.818171\n",
      "(Iteration 6661 / 7640) loss: 1.877534\n",
      "(Iteration 6671 / 7640) loss: 1.760709\n",
      "(Iteration 6681 / 7640) loss: 1.943432\n",
      "(Iteration 6691 / 7640) loss: 1.825378\n",
      "(Iteration 6701 / 7640) loss: 1.917453\n",
      "(Iteration 6711 / 7640) loss: 1.826109\n",
      "(Iteration 6721 / 7640) loss: 1.828921\n",
      "(Iteration 6731 / 7640) loss: 1.792166\n",
      "(Iteration 6741 / 7640) loss: 1.878076\n",
      "(Iteration 6751 / 7640) loss: 1.942076\n",
      "(Iteration 6761 / 7640) loss: 1.898197\n",
      "(Iteration 6771 / 7640) loss: 1.952146\n",
      "(Iteration 6781 / 7640) loss: 1.962124\n",
      "(Iteration 6791 / 7640) loss: 1.793502\n",
      "(Iteration 6801 / 7640) loss: 1.827791\n",
      "(Iteration 6811 / 7640) loss: 1.844723\n",
      "(Iteration 6821 / 7640) loss: 1.995730\n",
      "(Iteration 6831 / 7640) loss: 1.906882\n",
      "(Iteration 6841 / 7640) loss: 1.782188\n",
      "(Iteration 6851 / 7640) loss: 1.801170\n",
      "(Iteration 6861 / 7640) loss: 1.766982\n",
      "(Iteration 6871 / 7640) loss: 1.891390\n",
      "(Epoch 18 / 20) train acc: 0.814000; val_acc: 0.555000\n",
      "(Iteration 6881 / 7640) loss: 1.713115\n",
      "(Iteration 6891 / 7640) loss: 1.957378\n",
      "(Iteration 6901 / 7640) loss: 1.842318\n",
      "(Iteration 6911 / 7640) loss: 1.932443\n",
      "(Iteration 6921 / 7640) loss: 1.779792\n",
      "(Iteration 6931 / 7640) loss: 1.681898\n",
      "(Iteration 6941 / 7640) loss: 1.697016\n",
      "(Iteration 6951 / 7640) loss: 1.811883\n",
      "(Iteration 6961 / 7640) loss: 1.733914\n",
      "(Iteration 6971 / 7640) loss: 1.785781\n",
      "(Iteration 6981 / 7640) loss: 1.717598\n",
      "(Iteration 6991 / 7640) loss: 1.789885\n",
      "(Iteration 7001 / 7640) loss: 1.742158\n",
      "(Iteration 7011 / 7640) loss: 1.655078\n",
      "(Iteration 7021 / 7640) loss: 1.806204\n",
      "(Iteration 7031 / 7640) loss: 1.895678\n",
      "(Iteration 7041 / 7640) loss: 1.724199\n",
      "(Iteration 7051 / 7640) loss: 1.803393\n",
      "(Iteration 7061 / 7640) loss: 1.899916\n",
      "(Iteration 7071 / 7640) loss: 1.671911\n",
      "(Iteration 7081 / 7640) loss: 1.710609\n",
      "(Iteration 7091 / 7640) loss: 1.781472\n",
      "(Iteration 7101 / 7640) loss: 1.716288\n",
      "(Iteration 7111 / 7640) loss: 1.827647\n",
      "(Iteration 7121 / 7640) loss: 1.874873\n",
      "(Iteration 7131 / 7640) loss: 1.834272\n",
      "(Iteration 7141 / 7640) loss: 1.743007\n",
      "(Iteration 7151 / 7640) loss: 1.767189\n",
      "(Iteration 7161 / 7640) loss: 1.735744\n",
      "(Iteration 7171 / 7640) loss: 1.838458\n",
      "(Iteration 7181 / 7640) loss: 1.715462\n",
      "(Iteration 7191 / 7640) loss: 1.836434\n",
      "(Iteration 7201 / 7640) loss: 1.683845\n",
      "(Iteration 7211 / 7640) loss: 1.703141\n",
      "(Iteration 7221 / 7640) loss: 1.749330\n",
      "(Iteration 7231 / 7640) loss: 1.883726\n",
      "(Iteration 7241 / 7640) loss: 1.651398\n",
      "(Iteration 7251 / 7640) loss: 1.742892\n",
      "(Epoch 19 / 20) train acc: 0.805000; val_acc: 0.553000\n",
      "(Iteration 7261 / 7640) loss: 1.816898\n",
      "(Iteration 7271 / 7640) loss: 1.919691\n",
      "(Iteration 7281 / 7640) loss: 1.772073\n",
      "(Iteration 7291 / 7640) loss: 1.663962\n",
      "(Iteration 7301 / 7640) loss: 1.909101\n",
      "(Iteration 7311 / 7640) loss: 1.678862\n",
      "(Iteration 7321 / 7640) loss: 1.953093\n",
      "(Iteration 7331 / 7640) loss: 1.906822\n",
      "(Iteration 7341 / 7640) loss: 1.729382\n",
      "(Iteration 7351 / 7640) loss: 1.986090\n",
      "(Iteration 7361 / 7640) loss: 1.661672\n",
      "(Iteration 7371 / 7640) loss: 1.750282\n",
      "(Iteration 7381 / 7640) loss: 1.599909\n",
      "(Iteration 7391 / 7640) loss: 1.670982\n",
      "(Iteration 7401 / 7640) loss: 1.691568\n",
      "(Iteration 7411 / 7640) loss: 1.702735\n",
      "(Iteration 7421 / 7640) loss: 1.786206\n",
      "(Iteration 7431 / 7640) loss: 1.705092\n",
      "(Iteration 7441 / 7640) loss: 1.674421\n",
      "(Iteration 7451 / 7640) loss: 1.898447\n",
      "(Iteration 7461 / 7640) loss: 1.625277\n",
      "(Iteration 7471 / 7640) loss: 1.856220\n",
      "(Iteration 7481 / 7640) loss: 1.754973\n",
      "(Iteration 7491 / 7640) loss: 1.728417\n",
      "(Iteration 7501 / 7640) loss: 1.701904\n",
      "(Iteration 7511 / 7640) loss: 1.630551\n",
      "(Iteration 7521 / 7640) loss: 1.653156\n",
      "(Iteration 7531 / 7640) loss: 1.643213\n",
      "(Iteration 7541 / 7640) loss: 1.628562\n",
      "(Iteration 7551 / 7640) loss: 1.615034\n",
      "(Iteration 7561 / 7640) loss: 1.811864\n",
      "(Iteration 7571 / 7640) loss: 1.688282\n",
      "(Iteration 7581 / 7640) loss: 1.759062\n",
      "(Iteration 7591 / 7640) loss: 1.556602\n",
      "(Iteration 7601 / 7640) loss: 1.674997\n",
      "(Iteration 7611 / 7640) loss: 1.600650\n",
      "(Iteration 7621 / 7640) loss: 1.574510\n",
      "(Iteration 7631 / 7640) loss: 1.663319\n",
      "(Epoch 20 / 20) train acc: 0.828000; val_acc: 0.540000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.561000\n",
      "running with  rmsprop\n",
      "(Iteration 1 / 7640) loss: 6.689853\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.107000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OctaveProject\\cs231n\\assignment2\\cs231n\\layers.py:681: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 11 / 7640) loss: 6.589780\n",
      "(Iteration 21 / 7640) loss: 6.348729\n",
      "(Iteration 31 / 7640) loss: 6.317140\n",
      "(Iteration 41 / 7640) loss: 6.131933\n",
      "(Iteration 51 / 7640) loss: 6.045244\n",
      "(Iteration 61 / 7640) loss: 5.782558\n",
      "(Iteration 71 / 7640) loss: 5.910501\n",
      "(Iteration 81 / 7640) loss: 5.783575\n",
      "(Iteration 91 / 7640) loss: 5.698039\n",
      "(Iteration 101 / 7640) loss: 5.565475\n",
      "(Iteration 111 / 7640) loss: 5.640993\n",
      "(Iteration 121 / 7640) loss: 5.818261\n",
      "(Iteration 131 / 7640) loss: 5.409601\n",
      "(Iteration 141 / 7640) loss: 5.502021\n",
      "(Iteration 151 / 7640) loss: 5.419896\n",
      "(Iteration 161 / 7640) loss: 5.286446\n",
      "(Iteration 171 / 7640) loss: 5.455498\n",
      "(Iteration 181 / 7640) loss: 5.309610\n",
      "(Iteration 191 / 7640) loss: 5.136599\n",
      "(Iteration 201 / 7640) loss: 5.233553\n",
      "(Iteration 211 / 7640) loss: 4.939007\n",
      "(Iteration 221 / 7640) loss: 5.090692\n",
      "(Iteration 231 / 7640) loss: 5.118610\n",
      "(Iteration 241 / 7640) loss: 5.032029\n",
      "(Iteration 251 / 7640) loss: 4.916156\n",
      "(Iteration 261 / 7640) loss: 5.048253\n",
      "(Iteration 271 / 7640) loss: 4.917751\n",
      "(Iteration 281 / 7640) loss: 4.895820\n",
      "(Iteration 291 / 7640) loss: 4.810431\n",
      "(Iteration 301 / 7640) loss: 4.868816\n",
      "(Iteration 311 / 7640) loss: 4.587239\n",
      "(Iteration 321 / 7640) loss: 4.773356\n",
      "(Iteration 331 / 7640) loss: 4.704500\n",
      "(Iteration 341 / 7640) loss: 4.802077\n",
      "(Iteration 351 / 7640) loss: 4.657876\n",
      "(Iteration 361 / 7640) loss: 4.572798\n",
      "(Iteration 371 / 7640) loss: 4.606055\n",
      "(Iteration 381 / 7640) loss: 4.445689\n",
      "(Epoch 1 / 20) train acc: 0.341000; val_acc: 0.328000\n",
      "(Iteration 391 / 7640) loss: 4.522287\n",
      "(Iteration 401 / 7640) loss: 4.552371\n",
      "(Iteration 411 / 7640) loss: 4.535107\n",
      "(Iteration 421 / 7640) loss: 4.414056\n",
      "(Iteration 431 / 7640) loss: 4.443382\n",
      "(Iteration 441 / 7640) loss: 4.511490\n",
      "(Iteration 451 / 7640) loss: 4.379358\n",
      "(Iteration 461 / 7640) loss: 4.248050\n",
      "(Iteration 471 / 7640) loss: 4.451430\n",
      "(Iteration 481 / 7640) loss: 4.431210\n",
      "(Iteration 491 / 7640) loss: 4.384045\n",
      "(Iteration 501 / 7640) loss: 4.243857\n",
      "(Iteration 511 / 7640) loss: 4.381190\n",
      "(Iteration 521 / 7640) loss: 4.156838\n",
      "(Iteration 531 / 7640) loss: 4.179507\n",
      "(Iteration 541 / 7640) loss: 4.198829\n",
      "(Iteration 551 / 7640) loss: 4.134026\n",
      "(Iteration 561 / 7640) loss: 4.023224\n",
      "(Iteration 571 / 7640) loss: 3.882276\n",
      "(Iteration 581 / 7640) loss: 4.054811\n",
      "(Iteration 591 / 7640) loss: 3.969471\n",
      "(Iteration 601 / 7640) loss: 3.980559\n",
      "(Iteration 611 / 7640) loss: 3.916608\n",
      "(Iteration 621 / 7640) loss: 4.006144\n",
      "(Iteration 631 / 7640) loss: 3.961948\n",
      "(Iteration 641 / 7640) loss: 3.928119\n",
      "(Iteration 651 / 7640) loss: 3.886057\n",
      "(Iteration 661 / 7640) loss: 3.762771\n",
      "(Iteration 671 / 7640) loss: 3.891406\n",
      "(Iteration 681 / 7640) loss: 3.804039\n",
      "(Iteration 691 / 7640) loss: 3.807984\n",
      "(Iteration 701 / 7640) loss: 3.787068\n",
      "(Iteration 711 / 7640) loss: 3.661449\n",
      "(Iteration 721 / 7640) loss: 3.731956\n",
      "(Iteration 731 / 7640) loss: 3.742560\n",
      "(Iteration 741 / 7640) loss: 3.853909\n",
      "(Iteration 751 / 7640) loss: 3.460840\n",
      "(Iteration 761 / 7640) loss: 3.636232\n",
      "(Epoch 2 / 20) train acc: 0.372000; val_acc: 0.390000\n",
      "(Iteration 771 / 7640) loss: 3.635941\n",
      "(Iteration 781 / 7640) loss: 3.650906\n",
      "(Iteration 791 / 7640) loss: 3.655872\n",
      "(Iteration 801 / 7640) loss: 3.449549\n",
      "(Iteration 811 / 7640) loss: 3.527655\n",
      "(Iteration 821 / 7640) loss: 3.480426\n",
      "(Iteration 831 / 7640) loss: 3.582250\n",
      "(Iteration 841 / 7640) loss: 3.521588\n",
      "(Iteration 851 / 7640) loss: 3.346672\n",
      "(Iteration 861 / 7640) loss: 3.418465\n",
      "(Iteration 871 / 7640) loss: 3.631758\n",
      "(Iteration 881 / 7640) loss: 3.550431\n",
      "(Iteration 891 / 7640) loss: 3.554395\n",
      "(Iteration 901 / 7640) loss: 3.452535\n",
      "(Iteration 911 / 7640) loss: 3.372875\n",
      "(Iteration 921 / 7640) loss: 3.251745\n",
      "(Iteration 931 / 7640) loss: 3.255129\n",
      "(Iteration 941 / 7640) loss: 3.408476\n",
      "(Iteration 951 / 7640) loss: 3.197833\n",
      "(Iteration 961 / 7640) loss: 3.316483\n",
      "(Iteration 971 / 7640) loss: 3.326574\n",
      "(Iteration 981 / 7640) loss: 3.146427\n",
      "(Iteration 991 / 7640) loss: 3.124129\n",
      "(Iteration 1001 / 7640) loss: 3.136245\n",
      "(Iteration 1011 / 7640) loss: 3.250004\n",
      "(Iteration 1021 / 7640) loss: 3.210952\n",
      "(Iteration 1031 / 7640) loss: 3.109840\n",
      "(Iteration 1041 / 7640) loss: 3.144132\n",
      "(Iteration 1051 / 7640) loss: 3.141189\n",
      "(Iteration 1061 / 7640) loss: 2.969476\n",
      "(Iteration 1071 / 7640) loss: 3.226182\n",
      "(Iteration 1081 / 7640) loss: 3.115343\n",
      "(Iteration 1091 / 7640) loss: 3.039796\n",
      "(Iteration 1101 / 7640) loss: 3.039375\n",
      "(Iteration 1111 / 7640) loss: 3.009489\n",
      "(Iteration 1121 / 7640) loss: 2.906457\n",
      "(Iteration 1131 / 7640) loss: 2.887698\n",
      "(Iteration 1141 / 7640) loss: 2.980769\n",
      "(Epoch 3 / 20) train acc: 0.405000; val_acc: 0.412000\n",
      "(Iteration 1151 / 7640) loss: 2.980502\n",
      "(Iteration 1161 / 7640) loss: 2.887407\n",
      "(Iteration 1171 / 7640) loss: 2.878852\n",
      "(Iteration 1181 / 7640) loss: 2.869767\n",
      "(Iteration 1191 / 7640) loss: 2.932739\n",
      "(Iteration 1201 / 7640) loss: 2.711984\n",
      "(Iteration 1211 / 7640) loss: 2.801462\n",
      "(Iteration 1221 / 7640) loss: 2.807568\n",
      "(Iteration 1231 / 7640) loss: 2.712598\n",
      "(Iteration 1241 / 7640) loss: 2.805091\n",
      "(Iteration 1251 / 7640) loss: 2.639015\n",
      "(Iteration 1261 / 7640) loss: 2.532519\n",
      "(Iteration 1271 / 7640) loss: 2.722292\n",
      "(Iteration 1281 / 7640) loss: 2.688596\n",
      "(Iteration 1291 / 7640) loss: 2.787750\n",
      "(Iteration 1301 / 7640) loss: 2.775623\n",
      "(Iteration 1311 / 7640) loss: 2.705997\n",
      "(Iteration 1321 / 7640) loss: 2.631796\n",
      "(Iteration 1331 / 7640) loss: 2.634530\n",
      "(Iteration 1341 / 7640) loss: 2.627845\n",
      "(Iteration 1351 / 7640) loss: 2.543359\n",
      "(Iteration 1361 / 7640) loss: 2.641442\n",
      "(Iteration 1371 / 7640) loss: 2.575315\n",
      "(Iteration 1381 / 7640) loss: 2.527921\n",
      "(Iteration 1391 / 7640) loss: 2.552860\n",
      "(Iteration 1401 / 7640) loss: 2.687844\n",
      "(Iteration 1411 / 7640) loss: 2.705253\n",
      "(Iteration 1421 / 7640) loss: 2.675860\n",
      "(Iteration 1431 / 7640) loss: 2.676626\n",
      "(Iteration 1441 / 7640) loss: 2.450185\n",
      "(Iteration 1451 / 7640) loss: 2.606401\n",
      "(Iteration 1461 / 7640) loss: 2.499044\n",
      "(Iteration 1471 / 7640) loss: 2.505890\n",
      "(Iteration 1481 / 7640) loss: 2.467216\n",
      "(Iteration 1491 / 7640) loss: 2.491434\n",
      "(Iteration 1501 / 7640) loss: 2.472380\n",
      "(Iteration 1511 / 7640) loss: 2.403151\n",
      "(Iteration 1521 / 7640) loss: 2.453219\n",
      "(Epoch 4 / 20) train acc: 0.408000; val_acc: 0.425000\n",
      "(Iteration 1531 / 7640) loss: 2.615121\n",
      "(Iteration 1541 / 7640) loss: 2.399849\n",
      "(Iteration 1551 / 7640) loss: 2.305088\n",
      "(Iteration 1561 / 7640) loss: 2.524576\n",
      "(Iteration 1571 / 7640) loss: 2.189140\n",
      "(Iteration 1581 / 7640) loss: 2.401307\n",
      "(Iteration 1591 / 7640) loss: 2.262631\n",
      "(Iteration 1601 / 7640) loss: 2.328951\n",
      "(Iteration 1611 / 7640) loss: 2.404394\n",
      "(Iteration 1621 / 7640) loss: 2.059784\n",
      "(Iteration 1631 / 7640) loss: 2.414543\n",
      "(Iteration 1641 / 7640) loss: 2.315999\n",
      "(Iteration 1651 / 7640) loss: 2.224993\n",
      "(Iteration 1661 / 7640) loss: 2.424743\n",
      "(Iteration 1671 / 7640) loss: 2.453038\n",
      "(Iteration 1681 / 7640) loss: 2.298129\n",
      "(Iteration 1691 / 7640) loss: 2.297489\n",
      "(Iteration 1701 / 7640) loss: 2.222789\n",
      "(Iteration 1711 / 7640) loss: 2.201789\n",
      "(Iteration 1721 / 7640) loss: 2.275190\n",
      "(Iteration 1731 / 7640) loss: 2.067265\n",
      "(Iteration 1741 / 7640) loss: 2.141837\n",
      "(Iteration 1751 / 7640) loss: 2.280845\n",
      "(Iteration 1761 / 7640) loss: 2.146549\n",
      "(Iteration 1771 / 7640) loss: 2.229552\n",
      "(Iteration 1781 / 7640) loss: 2.386822\n",
      "(Iteration 1791 / 7640) loss: 2.241974\n",
      "(Iteration 1801 / 7640) loss: 2.057309\n",
      "(Iteration 1811 / 7640) loss: 2.193826\n",
      "(Iteration 1821 / 7640) loss: 2.133746\n",
      "(Iteration 1831 / 7640) loss: 2.161759\n",
      "(Iteration 1841 / 7640) loss: 2.139068\n",
      "(Iteration 1851 / 7640) loss: 2.291562\n",
      "(Iteration 1861 / 7640) loss: 2.280091\n",
      "(Iteration 1871 / 7640) loss: 2.238118\n",
      "(Iteration 1881 / 7640) loss: 2.139535\n",
      "(Iteration 1891 / 7640) loss: 2.143810\n",
      "(Iteration 1901 / 7640) loss: 2.015557\n",
      "(Epoch 5 / 20) train acc: 0.455000; val_acc: 0.438000\n",
      "(Iteration 1911 / 7640) loss: 2.074425\n",
      "(Iteration 1921 / 7640) loss: 2.079456\n",
      "(Iteration 1931 / 7640) loss: 2.197417\n",
      "(Iteration 1941 / 7640) loss: 2.041761\n",
      "(Iteration 1951 / 7640) loss: 1.960699\n",
      "(Iteration 1961 / 7640) loss: 2.068515\n",
      "(Iteration 1971 / 7640) loss: 2.368267\n",
      "(Iteration 1981 / 7640) loss: 2.115978\n",
      "(Iteration 1991 / 7640) loss: 1.976429\n",
      "(Iteration 2001 / 7640) loss: 1.918612\n",
      "(Iteration 2011 / 7640) loss: 1.953328\n",
      "(Iteration 2021 / 7640) loss: 2.098199\n",
      "(Iteration 2031 / 7640) loss: 1.953378\n",
      "(Iteration 2041 / 7640) loss: 2.122519\n",
      "(Iteration 2051 / 7640) loss: 2.036005\n",
      "(Iteration 2061 / 7640) loss: 2.032939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2071 / 7640) loss: 2.363314\n",
      "(Iteration 2081 / 7640) loss: 2.079312\n",
      "(Iteration 2091 / 7640) loss: 2.203128\n",
      "(Iteration 2101 / 7640) loss: 1.907980\n",
      "(Iteration 2111 / 7640) loss: 1.914117\n",
      "(Iteration 2121 / 7640) loss: 2.141557\n",
      "(Iteration 2131 / 7640) loss: 1.940053\n",
      "(Iteration 2141 / 7640) loss: 2.132814\n",
      "(Iteration 2151 / 7640) loss: 2.104405\n",
      "(Iteration 2161 / 7640) loss: 2.011649\n",
      "(Iteration 2171 / 7640) loss: 2.105145\n",
      "(Iteration 2181 / 7640) loss: 2.057800\n",
      "(Iteration 2191 / 7640) loss: 2.183988\n",
      "(Iteration 2201 / 7640) loss: 1.980381\n",
      "(Iteration 2211 / 7640) loss: 2.123580\n",
      "(Iteration 2221 / 7640) loss: 1.823480\n",
      "(Iteration 2231 / 7640) loss: 2.128096\n",
      "(Iteration 2241 / 7640) loss: 1.983406\n",
      "(Iteration 2251 / 7640) loss: 2.091477\n",
      "(Iteration 2261 / 7640) loss: 1.845294\n",
      "(Iteration 2271 / 7640) loss: 1.740864\n",
      "(Iteration 2281 / 7640) loss: 1.943736\n",
      "(Iteration 2291 / 7640) loss: 2.008438\n",
      "(Epoch 6 / 20) train acc: 0.442000; val_acc: 0.447000\n",
      "(Iteration 2301 / 7640) loss: 1.867016\n",
      "(Iteration 2311 / 7640) loss: 1.826714\n",
      "(Iteration 2321 / 7640) loss: 1.791526\n",
      "(Iteration 2331 / 7640) loss: 1.959536\n",
      "(Iteration 2341 / 7640) loss: 1.889365\n",
      "(Iteration 2351 / 7640) loss: 1.948490\n",
      "(Iteration 2361 / 7640) loss: 1.892277\n",
      "(Iteration 2371 / 7640) loss: 2.037370\n",
      "(Iteration 2381 / 7640) loss: 1.995611\n",
      "(Iteration 2391 / 7640) loss: 2.050747\n",
      "(Iteration 2401 / 7640) loss: 2.036286\n",
      "(Iteration 2411 / 7640) loss: 1.914016\n",
      "(Iteration 2421 / 7640) loss: 1.963775\n",
      "(Iteration 2431 / 7640) loss: 1.811560\n",
      "(Iteration 2441 / 7640) loss: 2.198792\n",
      "(Iteration 2451 / 7640) loss: 2.222470\n",
      "(Iteration 2461 / 7640) loss: 1.846551\n",
      "(Iteration 2471 / 7640) loss: 1.919743\n",
      "(Iteration 2481 / 7640) loss: 1.830163\n",
      "(Iteration 2491 / 7640) loss: 1.972320\n",
      "(Iteration 2501 / 7640) loss: 1.846248\n",
      "(Iteration 2511 / 7640) loss: 1.982995\n",
      "(Iteration 2521 / 7640) loss: 1.876847\n",
      "(Iteration 2531 / 7640) loss: 1.825121\n",
      "(Iteration 2541 / 7640) loss: 1.952045\n",
      "(Iteration 2551 / 7640) loss: 1.924035\n",
      "(Iteration 2561 / 7640) loss: 1.898476\n",
      "(Iteration 2571 / 7640) loss: 1.928000\n",
      "(Iteration 2581 / 7640) loss: 1.887202\n",
      "(Iteration 2591 / 7640) loss: 1.943083\n",
      "(Iteration 2601 / 7640) loss: 1.740513\n",
      "(Iteration 2611 / 7640) loss: 2.011201\n",
      "(Iteration 2621 / 7640) loss: 1.921976\n",
      "(Iteration 2631 / 7640) loss: 1.705838\n",
      "(Iteration 2641 / 7640) loss: 1.751748\n",
      "(Iteration 2651 / 7640) loss: 2.046399\n",
      "(Iteration 2661 / 7640) loss: 1.817814\n",
      "(Iteration 2671 / 7640) loss: 2.070477\n",
      "(Epoch 7 / 20) train acc: 0.446000; val_acc: 0.466000\n",
      "(Iteration 2681 / 7640) loss: 1.772634\n",
      "(Iteration 2691 / 7640) loss: 1.831754\n",
      "(Iteration 2701 / 7640) loss: 1.852130\n",
      "(Iteration 2711 / 7640) loss: 2.040675\n",
      "(Iteration 2721 / 7640) loss: 1.869857\n",
      "(Iteration 2731 / 7640) loss: 1.837093\n",
      "(Iteration 2741 / 7640) loss: 1.756363\n",
      "(Iteration 2751 / 7640) loss: 1.841422\n",
      "(Iteration 2761 / 7640) loss: 1.942439\n",
      "(Iteration 2771 / 7640) loss: 1.853581\n",
      "(Iteration 2781 / 7640) loss: 1.767646\n",
      "(Iteration 2791 / 7640) loss: 1.745911\n",
      "(Iteration 2801 / 7640) loss: 1.942697\n",
      "(Iteration 2811 / 7640) loss: 2.083854\n",
      "(Iteration 2821 / 7640) loss: 1.589062\n",
      "(Iteration 2831 / 7640) loss: 1.955252\n",
      "(Iteration 2841 / 7640) loss: 1.809542\n",
      "(Iteration 2851 / 7640) loss: 1.804904\n",
      "(Iteration 2861 / 7640) loss: 1.768648\n",
      "(Iteration 2871 / 7640) loss: 1.774717\n",
      "(Iteration 2881 / 7640) loss: 1.764896\n",
      "(Iteration 2891 / 7640) loss: 1.961979\n",
      "(Iteration 2901 / 7640) loss: 1.828950\n",
      "(Iteration 2911 / 7640) loss: 1.766449\n",
      "(Iteration 2921 / 7640) loss: 1.772662\n",
      "(Iteration 2931 / 7640) loss: 1.942460\n",
      "(Iteration 2941 / 7640) loss: 1.825273\n",
      "(Iteration 2951 / 7640) loss: 1.742048\n",
      "(Iteration 2961 / 7640) loss: 2.048433\n",
      "(Iteration 2971 / 7640) loss: 2.012993\n",
      "(Iteration 2981 / 7640) loss: 1.762901\n",
      "(Iteration 2991 / 7640) loss: 1.932357\n",
      "(Iteration 3001 / 7640) loss: 1.765462\n",
      "(Iteration 3011 / 7640) loss: 1.826231\n",
      "(Iteration 3021 / 7640) loss: 1.802018\n",
      "(Iteration 3031 / 7640) loss: 1.847875\n",
      "(Iteration 3041 / 7640) loss: 1.867744\n",
      "(Iteration 3051 / 7640) loss: 1.732954\n",
      "(Epoch 8 / 20) train acc: 0.502000; val_acc: 0.456000\n",
      "(Iteration 3061 / 7640) loss: 1.724267\n",
      "(Iteration 3071 / 7640) loss: 1.786926\n",
      "(Iteration 3081 / 7640) loss: 1.864856\n",
      "(Iteration 3091 / 7640) loss: 1.893573\n",
      "(Iteration 3101 / 7640) loss: 1.788826\n",
      "(Iteration 3111 / 7640) loss: 1.731618\n",
      "(Iteration 3121 / 7640) loss: 1.860743\n",
      "(Iteration 3131 / 7640) loss: 1.929820\n",
      "(Iteration 3141 / 7640) loss: 2.143997\n",
      "(Iteration 3151 / 7640) loss: 1.840324\n",
      "(Iteration 3161 / 7640) loss: 1.728704\n",
      "(Iteration 3171 / 7640) loss: 1.797608\n",
      "(Iteration 3181 / 7640) loss: 1.806919\n",
      "(Iteration 3191 / 7640) loss: 1.821267\n",
      "(Iteration 3201 / 7640) loss: 1.725918\n",
      "(Iteration 3211 / 7640) loss: 1.946052\n",
      "(Iteration 3221 / 7640) loss: 1.818546\n",
      "(Iteration 3231 / 7640) loss: 1.900779\n",
      "(Iteration 3241 / 7640) loss: 1.741023\n",
      "(Iteration 3251 / 7640) loss: 1.896219\n",
      "(Iteration 3261 / 7640) loss: 1.757826\n",
      "(Iteration 3271 / 7640) loss: 1.768818\n",
      "(Iteration 3281 / 7640) loss: 2.004014\n",
      "(Iteration 3291 / 7640) loss: 1.695081\n",
      "(Iteration 3301 / 7640) loss: 1.782955\n",
      "(Iteration 3311 / 7640) loss: 1.826553\n",
      "(Iteration 3321 / 7640) loss: 1.615742\n",
      "(Iteration 3331 / 7640) loss: 1.612588\n",
      "(Iteration 3341 / 7640) loss: 1.832427\n",
      "(Iteration 3351 / 7640) loss: 1.920790\n",
      "(Iteration 3361 / 7640) loss: 1.836731\n",
      "(Iteration 3371 / 7640) loss: 1.970421\n",
      "(Iteration 3381 / 7640) loss: 1.585577\n",
      "(Iteration 3391 / 7640) loss: 1.692489\n",
      "(Iteration 3401 / 7640) loss: 1.705415\n",
      "(Iteration 3411 / 7640) loss: 1.510288\n",
      "(Iteration 3421 / 7640) loss: 1.696088\n",
      "(Iteration 3431 / 7640) loss: 1.948141\n",
      "(Epoch 9 / 20) train acc: 0.419000; val_acc: 0.411000\n",
      "(Iteration 3441 / 7640) loss: 1.864967\n",
      "(Iteration 3451 / 7640) loss: 1.692455\n",
      "(Iteration 3461 / 7640) loss: 1.754907\n",
      "(Iteration 3471 / 7640) loss: 1.768849\n",
      "(Iteration 3481 / 7640) loss: 1.667496\n",
      "(Iteration 3491 / 7640) loss: 1.995363\n",
      "(Iteration 3501 / 7640) loss: 1.703767\n",
      "(Iteration 3511 / 7640) loss: 1.753560\n",
      "(Iteration 3521 / 7640) loss: 1.769079\n",
      "(Iteration 3531 / 7640) loss: 1.861424\n",
      "(Iteration 3541 / 7640) loss: 1.836408\n",
      "(Iteration 3551 / 7640) loss: 1.737017\n",
      "(Iteration 3561 / 7640) loss: 1.702322\n",
      "(Iteration 3571 / 7640) loss: 1.871101\n",
      "(Iteration 3581 / 7640) loss: 1.693892\n",
      "(Iteration 3591 / 7640) loss: 1.704642\n",
      "(Iteration 3601 / 7640) loss: 1.773413\n",
      "(Iteration 3611 / 7640) loss: 1.646562\n",
      "(Iteration 3621 / 7640) loss: 1.797319\n",
      "(Iteration 3631 / 7640) loss: 1.760533\n",
      "(Iteration 3641 / 7640) loss: 1.802767\n",
      "(Iteration 3651 / 7640) loss: 1.697288\n",
      "(Iteration 3661 / 7640) loss: 1.725081\n",
      "(Iteration 3671 / 7640) loss: 1.714916\n",
      "(Iteration 3681 / 7640) loss: 1.813517\n",
      "(Iteration 3691 / 7640) loss: 1.749580\n",
      "(Iteration 3701 / 7640) loss: 2.070363\n",
      "(Iteration 3711 / 7640) loss: 2.082794\n",
      "(Iteration 3721 / 7640) loss: 1.785098\n",
      "(Iteration 3731 / 7640) loss: 1.772576\n",
      "(Iteration 3741 / 7640) loss: 1.762363\n",
      "(Iteration 3751 / 7640) loss: 1.793818\n",
      "(Iteration 3761 / 7640) loss: 1.891641\n",
      "(Iteration 3771 / 7640) loss: 1.941919\n",
      "(Iteration 3781 / 7640) loss: 1.779821\n",
      "(Iteration 3791 / 7640) loss: 1.781082\n",
      "(Iteration 3801 / 7640) loss: 1.765739\n",
      "(Iteration 3811 / 7640) loss: 1.705725\n",
      "(Epoch 10 / 20) train acc: 0.455000; val_acc: 0.461000\n",
      "(Iteration 3821 / 7640) loss: 1.847489\n",
      "(Iteration 3831 / 7640) loss: 1.891185\n",
      "(Iteration 3841 / 7640) loss: 1.758587\n",
      "(Iteration 3851 / 7640) loss: 1.833325\n",
      "(Iteration 3861 / 7640) loss: 1.504062\n",
      "(Iteration 3871 / 7640) loss: 1.717297\n",
      "(Iteration 3881 / 7640) loss: 1.713321\n",
      "(Iteration 3891 / 7640) loss: 2.064487\n",
      "(Iteration 3901 / 7640) loss: 1.760540\n",
      "(Iteration 3911 / 7640) loss: 1.530130\n",
      "(Iteration 3921 / 7640) loss: 1.712738\n",
      "(Iteration 3931 / 7640) loss: 1.819722\n",
      "(Iteration 3941 / 7640) loss: 1.678874\n",
      "(Iteration 3951 / 7640) loss: 1.676747\n",
      "(Iteration 3961 / 7640) loss: 1.825866\n",
      "(Iteration 3971 / 7640) loss: 1.813075\n",
      "(Iteration 3981 / 7640) loss: 1.791115\n",
      "(Iteration 3991 / 7640) loss: 1.834819\n",
      "(Iteration 4001 / 7640) loss: 1.642891\n",
      "(Iteration 4011 / 7640) loss: 1.758062\n",
      "(Iteration 4021 / 7640) loss: 1.828206\n",
      "(Iteration 4031 / 7640) loss: 1.623309\n",
      "(Iteration 4041 / 7640) loss: 1.716907\n",
      "(Iteration 4051 / 7640) loss: 1.719915\n",
      "(Iteration 4061 / 7640) loss: 1.689653\n",
      "(Iteration 4071 / 7640) loss: 1.748209\n",
      "(Iteration 4081 / 7640) loss: 1.794642\n",
      "(Iteration 4091 / 7640) loss: 1.936763\n",
      "(Iteration 4101 / 7640) loss: 1.674836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4111 / 7640) loss: 1.643381\n",
      "(Iteration 4121 / 7640) loss: 1.754099\n",
      "(Iteration 4131 / 7640) loss: 1.732665\n",
      "(Iteration 4141 / 7640) loss: 1.686460\n",
      "(Iteration 4151 / 7640) loss: 1.773168\n",
      "(Iteration 4161 / 7640) loss: 1.602650\n",
      "(Iteration 4171 / 7640) loss: 1.759446\n",
      "(Iteration 4181 / 7640) loss: 1.767968\n",
      "(Iteration 4191 / 7640) loss: 1.816274\n",
      "(Iteration 4201 / 7640) loss: 1.625628\n",
      "(Epoch 11 / 20) train acc: 0.483000; val_acc: 0.468000\n",
      "(Iteration 4211 / 7640) loss: 1.554434\n",
      "(Iteration 4221 / 7640) loss: 1.724933\n",
      "(Iteration 4231 / 7640) loss: 1.725829\n",
      "(Iteration 4241 / 7640) loss: 1.889785\n",
      "(Iteration 4251 / 7640) loss: 1.773675\n",
      "(Iteration 4261 / 7640) loss: 1.680468\n",
      "(Iteration 4271 / 7640) loss: 1.843254\n",
      "(Iteration 4281 / 7640) loss: 1.651450\n",
      "(Iteration 4291 / 7640) loss: 1.464198\n",
      "(Iteration 4301 / 7640) loss: 1.800910\n",
      "(Iteration 4311 / 7640) loss: 1.772575\n",
      "(Iteration 4321 / 7640) loss: 1.775519\n",
      "(Iteration 4331 / 7640) loss: 1.690981\n",
      "(Iteration 4341 / 7640) loss: 1.666807\n",
      "(Iteration 4351 / 7640) loss: 1.758555\n",
      "(Iteration 4361 / 7640) loss: 1.631781\n",
      "(Iteration 4371 / 7640) loss: 1.696339\n",
      "(Iteration 4381 / 7640) loss: 1.845515\n",
      "(Iteration 4391 / 7640) loss: 1.802437\n",
      "(Iteration 4401 / 7640) loss: 1.570638\n",
      "(Iteration 4411 / 7640) loss: 1.568046\n",
      "(Iteration 4421 / 7640) loss: 1.781381\n",
      "(Iteration 4431 / 7640) loss: 1.608647\n",
      "(Iteration 4441 / 7640) loss: 1.823436\n",
      "(Iteration 4451 / 7640) loss: 1.641766\n",
      "(Iteration 4461 / 7640) loss: 1.849317\n",
      "(Iteration 4471 / 7640) loss: 1.886699\n",
      "(Iteration 4481 / 7640) loss: 1.789622\n",
      "(Iteration 4491 / 7640) loss: 1.652335\n",
      "(Iteration 4501 / 7640) loss: 1.671874\n",
      "(Iteration 4511 / 7640) loss: 1.785418\n",
      "(Iteration 4521 / 7640) loss: 1.803525\n",
      "(Iteration 4531 / 7640) loss: 1.753905\n",
      "(Iteration 4541 / 7640) loss: 1.825845\n",
      "(Iteration 4551 / 7640) loss: 1.603086\n",
      "(Iteration 4561 / 7640) loss: 1.869352\n",
      "(Iteration 4571 / 7640) loss: 1.703877\n",
      "(Iteration 4581 / 7640) loss: 1.745908\n",
      "(Epoch 12 / 20) train acc: 0.436000; val_acc: 0.426000\n",
      "(Iteration 4591 / 7640) loss: 1.742052\n",
      "(Iteration 4601 / 7640) loss: 1.809669\n",
      "(Iteration 4611 / 7640) loss: 1.693573\n",
      "(Iteration 4621 / 7640) loss: 1.745248\n",
      "(Iteration 4631 / 7640) loss: 1.884033\n",
      "(Iteration 4641 / 7640) loss: 1.681081\n",
      "(Iteration 4651 / 7640) loss: 1.770023\n",
      "(Iteration 4661 / 7640) loss: 1.774662\n",
      "(Iteration 4671 / 7640) loss: 1.685697\n",
      "(Iteration 4681 / 7640) loss: 1.751083\n",
      "(Iteration 4691 / 7640) loss: 1.640994\n",
      "(Iteration 4701 / 7640) loss: 1.635827\n",
      "(Iteration 4711 / 7640) loss: 1.921032\n",
      "(Iteration 4721 / 7640) loss: 1.620880\n",
      "(Iteration 4731 / 7640) loss: 1.795600\n",
      "(Iteration 4741 / 7640) loss: 1.702401\n",
      "(Iteration 4751 / 7640) loss: 1.831829\n",
      "(Iteration 4761 / 7640) loss: 1.814742\n",
      "(Iteration 4771 / 7640) loss: 1.750736\n",
      "(Iteration 4781 / 7640) loss: 1.630731\n",
      "(Iteration 4791 / 7640) loss: 1.660864\n",
      "(Iteration 4801 / 7640) loss: 1.752037\n",
      "(Iteration 4811 / 7640) loss: 1.676409\n",
      "(Iteration 4821 / 7640) loss: 1.681111\n",
      "(Iteration 4831 / 7640) loss: 1.723616\n",
      "(Iteration 4841 / 7640) loss: 1.645828\n",
      "(Iteration 4851 / 7640) loss: 1.868448\n",
      "(Iteration 4861 / 7640) loss: 1.559035\n",
      "(Iteration 4871 / 7640) loss: 1.859185\n",
      "(Iteration 4881 / 7640) loss: 1.714912\n",
      "(Iteration 4891 / 7640) loss: 1.796441\n",
      "(Iteration 4901 / 7640) loss: 2.001293\n",
      "(Iteration 4911 / 7640) loss: 1.711690\n",
      "(Iteration 4921 / 7640) loss: 1.718756\n",
      "(Iteration 4931 / 7640) loss: 1.666065\n",
      "(Iteration 4941 / 7640) loss: 1.626221\n",
      "(Iteration 4951 / 7640) loss: 1.567978\n",
      "(Iteration 4961 / 7640) loss: 1.585110\n",
      "(Epoch 13 / 20) train acc: 0.461000; val_acc: 0.460000\n",
      "(Iteration 4971 / 7640) loss: 1.574151\n",
      "(Iteration 4981 / 7640) loss: 1.808233\n",
      "(Iteration 4991 / 7640) loss: 1.542649\n",
      "(Iteration 5001 / 7640) loss: 1.730356\n",
      "(Iteration 5011 / 7640) loss: 1.642436\n",
      "(Iteration 5021 / 7640) loss: 1.854259\n",
      "(Iteration 5031 / 7640) loss: 1.634642\n",
      "(Iteration 5041 / 7640) loss: 1.708303\n",
      "(Iteration 5051 / 7640) loss: 1.794180\n",
      "(Iteration 5061 / 7640) loss: 1.743102\n",
      "(Iteration 5071 / 7640) loss: 1.620547\n",
      "(Iteration 5081 / 7640) loss: 1.832688\n",
      "(Iteration 5091 / 7640) loss: 1.746367\n",
      "(Iteration 5101 / 7640) loss: 1.746373\n",
      "(Iteration 5111 / 7640) loss: 1.529260\n",
      "(Iteration 5121 / 7640) loss: 1.598053\n",
      "(Iteration 5131 / 7640) loss: 1.717860\n",
      "(Iteration 5141 / 7640) loss: 1.884414\n",
      "(Iteration 5151 / 7640) loss: 1.542551\n",
      "(Iteration 5161 / 7640) loss: 1.606162\n",
      "(Iteration 5171 / 7640) loss: 1.879122\n",
      "(Iteration 5181 / 7640) loss: 1.807789\n",
      "(Iteration 5191 / 7640) loss: 1.676875\n",
      "(Iteration 5201 / 7640) loss: 1.721893\n",
      "(Iteration 5211 / 7640) loss: 1.696046\n",
      "(Iteration 5221 / 7640) loss: 1.673708\n",
      "(Iteration 5231 / 7640) loss: 1.832740\n",
      "(Iteration 5241 / 7640) loss: 1.753260\n",
      "(Iteration 5251 / 7640) loss: 1.901269\n",
      "(Iteration 5261 / 7640) loss: 1.619533\n",
      "(Iteration 5271 / 7640) loss: 1.564792\n",
      "(Iteration 5281 / 7640) loss: 1.748379\n",
      "(Iteration 5291 / 7640) loss: 1.854976\n",
      "(Iteration 5301 / 7640) loss: 1.727966\n",
      "(Iteration 5311 / 7640) loss: 2.034134\n",
      "(Iteration 5321 / 7640) loss: 1.825260\n",
      "(Iteration 5331 / 7640) loss: 1.477796\n",
      "(Iteration 5341 / 7640) loss: 1.854936\n",
      "(Epoch 14 / 20) train acc: 0.483000; val_acc: 0.437000\n",
      "(Iteration 5351 / 7640) loss: 1.711781\n",
      "(Iteration 5361 / 7640) loss: 1.650003\n",
      "(Iteration 5371 / 7640) loss: 1.706574\n",
      "(Iteration 5381 / 7640) loss: 1.691870\n",
      "(Iteration 5391 / 7640) loss: 1.738149\n",
      "(Iteration 5401 / 7640) loss: 1.632337\n",
      "(Iteration 5411 / 7640) loss: 1.705429\n",
      "(Iteration 5421 / 7640) loss: 1.767555\n",
      "(Iteration 5431 / 7640) loss: 1.670693\n",
      "(Iteration 5441 / 7640) loss: 1.539185\n",
      "(Iteration 5451 / 7640) loss: 1.820247\n",
      "(Iteration 5461 / 7640) loss: 1.736566\n",
      "(Iteration 5471 / 7640) loss: 1.851817\n",
      "(Iteration 5481 / 7640) loss: 1.634756\n",
      "(Iteration 5491 / 7640) loss: 1.864777\n",
      "(Iteration 5501 / 7640) loss: 1.882892\n",
      "(Iteration 5511 / 7640) loss: 1.714177\n",
      "(Iteration 5521 / 7640) loss: 1.809285\n",
      "(Iteration 5531 / 7640) loss: 1.632844\n",
      "(Iteration 5541 / 7640) loss: 1.757346\n",
      "(Iteration 5551 / 7640) loss: 1.494978\n",
      "(Iteration 5561 / 7640) loss: 1.852301\n",
      "(Iteration 5571 / 7640) loss: 1.685888\n",
      "(Iteration 5581 / 7640) loss: 1.727436\n",
      "(Iteration 5591 / 7640) loss: 1.685252\n",
      "(Iteration 5601 / 7640) loss: 1.813795\n",
      "(Iteration 5611 / 7640) loss: 1.725598\n",
      "(Iteration 5621 / 7640) loss: 1.770378\n",
      "(Iteration 5631 / 7640) loss: 1.615838\n",
      "(Iteration 5641 / 7640) loss: 1.801182\n",
      "(Iteration 5651 / 7640) loss: 1.662266\n",
      "(Iteration 5661 / 7640) loss: 1.663762\n",
      "(Iteration 5671 / 7640) loss: 1.783363\n",
      "(Iteration 5681 / 7640) loss: 1.738761\n",
      "(Iteration 5691 / 7640) loss: 1.708947\n",
      "(Iteration 5701 / 7640) loss: 1.703754\n",
      "(Iteration 5711 / 7640) loss: 1.686112\n",
      "(Iteration 5721 / 7640) loss: 1.726473\n",
      "(Epoch 15 / 20) train acc: 0.488000; val_acc: 0.480000\n",
      "(Iteration 5731 / 7640) loss: 1.752203\n",
      "(Iteration 5741 / 7640) loss: 1.871239\n",
      "(Iteration 5751 / 7640) loss: 1.711793\n",
      "(Iteration 5761 / 7640) loss: 1.634357\n",
      "(Iteration 5771 / 7640) loss: 1.741567\n",
      "(Iteration 5781 / 7640) loss: 1.894273\n",
      "(Iteration 5791 / 7640) loss: 1.814310\n",
      "(Iteration 5801 / 7640) loss: 1.618046\n",
      "(Iteration 5811 / 7640) loss: 1.568383\n",
      "(Iteration 5821 / 7640) loss: 1.674561\n",
      "(Iteration 5831 / 7640) loss: 1.793056\n",
      "(Iteration 5841 / 7640) loss: 1.838976\n",
      "(Iteration 5851 / 7640) loss: 1.607055\n",
      "(Iteration 5861 / 7640) loss: 1.876655\n",
      "(Iteration 5871 / 7640) loss: 1.781785\n",
      "(Iteration 5881 / 7640) loss: 1.722076\n",
      "(Iteration 5891 / 7640) loss: 1.525718\n",
      "(Iteration 5901 / 7640) loss: 1.812961\n",
      "(Iteration 5911 / 7640) loss: 1.836518\n",
      "(Iteration 5921 / 7640) loss: 1.774516\n",
      "(Iteration 5931 / 7640) loss: 1.761074\n",
      "(Iteration 5941 / 7640) loss: 1.750947\n",
      "(Iteration 5951 / 7640) loss: 1.586404\n",
      "(Iteration 5961 / 7640) loss: 1.655711\n",
      "(Iteration 5971 / 7640) loss: 1.744301\n",
      "(Iteration 5981 / 7640) loss: 1.709786\n",
      "(Iteration 5991 / 7640) loss: 1.877785\n",
      "(Iteration 6001 / 7640) loss: 1.644897\n",
      "(Iteration 6011 / 7640) loss: 1.824060\n",
      "(Iteration 6021 / 7640) loss: 1.629295\n",
      "(Iteration 6031 / 7640) loss: 1.615745\n",
      "(Iteration 6041 / 7640) loss: 1.610315\n",
      "(Iteration 6051 / 7640) loss: 1.831730\n",
      "(Iteration 6061 / 7640) loss: 1.585619\n",
      "(Iteration 6071 / 7640) loss: 1.745239\n",
      "(Iteration 6081 / 7640) loss: 1.593647\n",
      "(Iteration 6091 / 7640) loss: 1.768699\n",
      "(Iteration 6101 / 7640) loss: 1.713766\n",
      "(Iteration 6111 / 7640) loss: 1.749153\n",
      "(Epoch 16 / 20) train acc: 0.468000; val_acc: 0.445000\n",
      "(Iteration 6121 / 7640) loss: 1.604696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 6131 / 7640) loss: 1.669561\n",
      "(Iteration 6141 / 7640) loss: 1.678803\n",
      "(Iteration 6151 / 7640) loss: 1.629445\n",
      "(Iteration 6161 / 7640) loss: 1.804056\n",
      "(Iteration 6171 / 7640) loss: 1.645415\n",
      "(Iteration 6181 / 7640) loss: 1.757159\n",
      "(Iteration 6191 / 7640) loss: 1.728906\n",
      "(Iteration 6201 / 7640) loss: 1.811730\n",
      "(Iteration 6211 / 7640) loss: 1.721362\n",
      "(Iteration 6221 / 7640) loss: 1.699392\n",
      "(Iteration 6231 / 7640) loss: 1.612832\n",
      "(Iteration 6241 / 7640) loss: 1.629890\n",
      "(Iteration 6251 / 7640) loss: 1.806730\n",
      "(Iteration 6261 / 7640) loss: 1.701630\n",
      "(Iteration 6271 / 7640) loss: 1.895120\n",
      "(Iteration 6281 / 7640) loss: 1.793025\n",
      "(Iteration 6291 / 7640) loss: 1.579584\n",
      "(Iteration 6301 / 7640) loss: 2.008959\n",
      "(Iteration 6311 / 7640) loss: 1.659072\n",
      "(Iteration 6321 / 7640) loss: 1.642341\n",
      "(Iteration 6331 / 7640) loss: 1.697196\n",
      "(Iteration 6341 / 7640) loss: 1.751852\n",
      "(Iteration 6351 / 7640) loss: 1.633802\n",
      "(Iteration 6361 / 7640) loss: 1.714964\n",
      "(Iteration 6371 / 7640) loss: 1.727817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\OctaveProject\\cs231n\\assignment2\\cs231n\\optim.py:166: RuntimeWarning: invalid value encountered in sqrt\n",
      "  next_x = x - config['learning_rate'] * dx / (np.sqrt(config['cache']) + config['epsilon'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 6381 / 7640) loss: 1.649755\n",
      "(Iteration 6391 / 7640) loss: 1.754902\n",
      "(Iteration 6401 / 7640) loss: 1.680806\n",
      "(Iteration 6411 / 7640) loss: 1.712246\n",
      "(Iteration 6421 / 7640) loss: 1.499696\n",
      "(Iteration 6431 / 7640) loss: 1.851920\n",
      "(Iteration 6441 / 7640) loss: 1.504808\n",
      "(Iteration 6451 / 7640) loss: 1.962530\n",
      "(Iteration 6461 / 7640) loss: 1.462552\n",
      "(Iteration 6471 / 7640) loss: 1.784198\n",
      "(Iteration 6481 / 7640) loss: 1.715375\n",
      "(Iteration 6491 / 7640) loss: 1.521135\n",
      "(Epoch 17 / 20) train acc: 0.440000; val_acc: 0.464000\n",
      "(Iteration 6501 / 7640) loss: 1.861005\n",
      "(Iteration 6511 / 7640) loss: 1.792497\n",
      "(Iteration 6521 / 7640) loss: 1.652082\n",
      "(Iteration 6531 / 7640) loss: 1.641020\n",
      "(Iteration 6541 / 7640) loss: 1.730546\n",
      "(Iteration 6551 / 7640) loss: 1.683148\n",
      "(Iteration 6561 / 7640) loss: 1.653138\n",
      "(Iteration 6571 / 7640) loss: 1.539016\n",
      "(Iteration 6581 / 7640) loss: 1.870898\n",
      "(Iteration 6591 / 7640) loss: 1.621693\n",
      "(Iteration 6601 / 7640) loss: 1.432869\n",
      "(Iteration 6611 / 7640) loss: 1.889541\n",
      "(Iteration 6621 / 7640) loss: 1.592967\n",
      "(Iteration 6631 / 7640) loss: 1.762675\n",
      "(Iteration 6641 / 7640) loss: 1.628471\n",
      "(Iteration 6651 / 7640) loss: 1.969759\n",
      "(Iteration 6661 / 7640) loss: 1.690907\n",
      "(Iteration 6671 / 7640) loss: 1.762521\n",
      "(Iteration 6681 / 7640) loss: 1.670766\n",
      "(Iteration 6691 / 7640) loss: 1.556078\n",
      "(Iteration 6701 / 7640) loss: 1.690273\n",
      "(Iteration 6711 / 7640) loss: 1.847746\n",
      "(Iteration 6721 / 7640) loss: 1.718733\n",
      "(Iteration 6731 / 7640) loss: 1.719025\n",
      "(Iteration 6741 / 7640) loss: 1.833942\n",
      "(Iteration 6751 / 7640) loss: 1.664730\n",
      "(Iteration 6761 / 7640) loss: 1.730786\n",
      "(Iteration 6771 / 7640) loss: 1.668255\n",
      "(Iteration 6781 / 7640) loss: 1.820536\n",
      "(Iteration 6791 / 7640) loss: 1.598999\n",
      "(Iteration 6801 / 7640) loss: 1.777134\n",
      "(Iteration 6811 / 7640) loss: 1.646992\n",
      "(Iteration 6821 / 7640) loss: 1.661371\n",
      "(Iteration 6831 / 7640) loss: 1.729836\n",
      "(Iteration 6841 / 7640) loss: 1.645392\n",
      "(Iteration 6851 / 7640) loss: 1.708201\n",
      "(Iteration 6861 / 7640) loss: 1.743571\n",
      "(Iteration 6871 / 7640) loss: 1.677422\n",
      "(Epoch 18 / 20) train acc: 0.458000; val_acc: 0.448000\n",
      "(Iteration 6881 / 7640) loss: 1.616126\n",
      "(Iteration 6891 / 7640) loss: 1.756415\n",
      "(Iteration 6901 / 7640) loss: 1.615507\n",
      "(Iteration 6911 / 7640) loss: 1.807704\n",
      "(Iteration 6921 / 7640) loss: 1.471605\n",
      "(Iteration 6931 / 7640) loss: 1.785661\n",
      "(Iteration 6941 / 7640) loss: 1.797592\n",
      "(Iteration 6951 / 7640) loss: 1.722851\n",
      "(Iteration 6961 / 7640) loss: 1.849256\n",
      "(Iteration 6971 / 7640) loss: 1.661960\n",
      "(Iteration 6981 / 7640) loss: 1.697858\n",
      "(Iteration 6991 / 7640) loss: 1.660106\n",
      "(Iteration 7001 / 7640) loss: 1.696500\n",
      "(Iteration 7011 / 7640) loss: 1.540271\n",
      "(Iteration 7021 / 7640) loss: 1.699316\n",
      "(Iteration 7031 / 7640) loss: 1.733927\n",
      "(Iteration 7041 / 7640) loss: 1.786829\n",
      "(Iteration 7051 / 7640) loss: 1.866823\n",
      "(Iteration 7061 / 7640) loss: 1.795452\n",
      "(Iteration 7071 / 7640) loss: 1.612155\n",
      "(Iteration 7081 / 7640) loss: 1.610018\n",
      "(Iteration 7091 / 7640) loss: 1.786401\n",
      "(Iteration 7101 / 7640) loss: 1.575015\n",
      "(Iteration 7111 / 7640) loss: 1.642501\n",
      "(Iteration 7121 / 7640) loss: 1.698397\n",
      "(Iteration 7131 / 7640) loss: 1.794909\n",
      "(Iteration 7141 / 7640) loss: 1.663199\n",
      "(Iteration 7151 / 7640) loss: 1.834793\n",
      "(Iteration 7161 / 7640) loss: 1.768709\n",
      "(Iteration 7171 / 7640) loss: 1.726050\n",
      "(Iteration 7181 / 7640) loss: 1.591879\n",
      "(Iteration 7191 / 7640) loss: 1.768770\n",
      "(Iteration 7201 / 7640) loss: 1.712392\n",
      "(Iteration 7211 / 7640) loss: 1.654047\n",
      "(Iteration 7221 / 7640) loss: 1.715237\n",
      "(Iteration 7231 / 7640) loss: 1.652290\n",
      "(Iteration 7241 / 7640) loss: 1.648122\n",
      "(Iteration 7251 / 7640) loss: 1.646502\n",
      "(Epoch 19 / 20) train acc: 0.473000; val_acc: 0.473000\n",
      "(Iteration 7261 / 7640) loss: 1.857102\n",
      "(Iteration 7271 / 7640) loss: 1.643385\n",
      "(Iteration 7281 / 7640) loss: 1.800558\n",
      "(Iteration 7291 / 7640) loss: 1.665598\n",
      "(Iteration 7301 / 7640) loss: 1.764518\n",
      "(Iteration 7311 / 7640) loss: 1.841576\n",
      "(Iteration 7321 / 7640) loss: 1.755748\n",
      "(Iteration 7331 / 7640) loss: 1.717566\n",
      "(Iteration 7341 / 7640) loss: 1.595412\n",
      "(Iteration 7351 / 7640) loss: 1.529442\n",
      "(Iteration 7361 / 7640) loss: 1.623961\n",
      "(Iteration 7371 / 7640) loss: 1.536761\n",
      "(Iteration 7381 / 7640) loss: 1.832743\n",
      "(Iteration 7391 / 7640) loss: 1.845752\n",
      "(Iteration 7401 / 7640) loss: 1.732447\n",
      "(Iteration 7411 / 7640) loss: 1.785842\n",
      "(Iteration 7421 / 7640) loss: 1.782783\n",
      "(Iteration 7431 / 7640) loss: 1.633805\n",
      "(Iteration 7441 / 7640) loss: 1.618171\n",
      "(Iteration 7451 / 7640) loss: 1.776487\n",
      "(Iteration 7461 / 7640) loss: 1.568439\n",
      "(Iteration 7471 / 7640) loss: 1.550613\n",
      "(Iteration 7481 / 7640) loss: 1.808827\n",
      "(Iteration 7491 / 7640) loss: 1.789754\n",
      "(Iteration 7501 / 7640) loss: 1.590598\n",
      "(Iteration 7511 / 7640) loss: 1.693443\n",
      "(Iteration 7521 / 7640) loss: 1.643614\n",
      "(Iteration 7531 / 7640) loss: 1.676111\n",
      "(Iteration 7541 / 7640) loss: 1.566731\n",
      "(Iteration 7551 / 7640) loss: 1.718874\n",
      "(Iteration 7561 / 7640) loss: 1.760638\n",
      "(Iteration 7571 / 7640) loss: 1.728919\n",
      "(Iteration 7581 / 7640) loss: 1.770729\n",
      "(Iteration 7591 / 7640) loss: 1.532400\n",
      "(Iteration 7601 / 7640) loss: 1.757775\n",
      "(Iteration 7611 / 7640) loss: 1.775536\n",
      "(Iteration 7621 / 7640) loss: 1.652149\n",
      "(Iteration 7631 / 7640) loss: 1.666519\n",
      "(Epoch 20 / 20) train acc: 0.489000; val_acc: 0.482000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.482000\n",
      "running with  adam\n",
      "(Iteration 1 / 7640) loss: 6.529411\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.089000\n",
      "(Iteration 11 / 7640) loss: 7.170773\n",
      "(Iteration 21 / 7640) loss: 7.275251\n",
      "(Iteration 31 / 7640) loss: 6.688486\n",
      "(Iteration 41 / 7640) loss: 6.044192\n",
      "(Iteration 51 / 7640) loss: 5.578927\n",
      "(Iteration 61 / 7640) loss: 5.391167\n",
      "(Iteration 71 / 7640) loss: 5.025832\n",
      "(Iteration 81 / 7640) loss: 4.873370\n",
      "(Iteration 91 / 7640) loss: 4.666413\n",
      "(Iteration 101 / 7640) loss: 4.552017\n",
      "(Iteration 111 / 7640) loss: 4.489485\n",
      "(Iteration 121 / 7640) loss: 4.363295\n",
      "(Iteration 131 / 7640) loss: 4.251597\n",
      "(Iteration 141 / 7640) loss: 4.315580\n",
      "(Iteration 151 / 7640) loss: 4.059594\n",
      "(Iteration 161 / 7640) loss: 4.081400\n",
      "(Iteration 171 / 7640) loss: 3.900958\n",
      "(Iteration 181 / 7640) loss: 3.820308\n",
      "(Iteration 191 / 7640) loss: 3.759765\n",
      "(Iteration 201 / 7640) loss: 3.701459\n",
      "(Iteration 211 / 7640) loss: 3.742537\n",
      "(Iteration 221 / 7640) loss: 3.462965\n",
      "(Iteration 231 / 7640) loss: 3.459469\n",
      "(Iteration 241 / 7640) loss: 3.658459\n",
      "(Iteration 251 / 7640) loss: 3.472339\n",
      "(Iteration 261 / 7640) loss: 3.256203\n",
      "(Iteration 271 / 7640) loss: 3.361457\n",
      "(Iteration 281 / 7640) loss: 3.410889\n",
      "(Iteration 291 / 7640) loss: 3.264557\n",
      "(Iteration 301 / 7640) loss: 3.299103\n",
      "(Iteration 311 / 7640) loss: 3.172597\n",
      "(Iteration 321 / 7640) loss: 3.122636\n",
      "(Iteration 331 / 7640) loss: 3.268734\n",
      "(Iteration 341 / 7640) loss: 3.200875\n",
      "(Iteration 351 / 7640) loss: 3.119362\n",
      "(Iteration 361 / 7640) loss: 3.139775\n",
      "(Iteration 371 / 7640) loss: 3.012641\n",
      "(Iteration 381 / 7640) loss: 2.932914\n",
      "(Epoch 1 / 20) train acc: 0.327000; val_acc: 0.336000\n",
      "(Iteration 391 / 7640) loss: 2.996988\n",
      "(Iteration 401 / 7640) loss: 2.927085\n",
      "(Iteration 411 / 7640) loss: 3.084987\n",
      "(Iteration 421 / 7640) loss: 3.060408\n",
      "(Iteration 431 / 7640) loss: 2.737505\n",
      "(Iteration 441 / 7640) loss: 2.759937\n",
      "(Iteration 451 / 7640) loss: 2.756504\n",
      "(Iteration 461 / 7640) loss: 2.909501\n",
      "(Iteration 471 / 7640) loss: 2.845795\n",
      "(Iteration 481 / 7640) loss: 2.837022\n",
      "(Iteration 491 / 7640) loss: 2.775321\n",
      "(Iteration 501 / 7640) loss: 2.684113\n",
      "(Iteration 511 / 7640) loss: 2.648459\n",
      "(Iteration 521 / 7640) loss: 2.603073\n",
      "(Iteration 531 / 7640) loss: 2.607637\n",
      "(Iteration 541 / 7640) loss: 2.659566\n",
      "(Iteration 551 / 7640) loss: 2.677517\n",
      "(Iteration 561 / 7640) loss: 2.670584\n",
      "(Iteration 571 / 7640) loss: 2.795900\n",
      "(Iteration 581 / 7640) loss: 2.548231\n",
      "(Iteration 591 / 7640) loss: 2.620225\n",
      "(Iteration 601 / 7640) loss: 2.583075\n",
      "(Iteration 611 / 7640) loss: 2.596029\n",
      "(Iteration 621 / 7640) loss: 2.583986\n",
      "(Iteration 631 / 7640) loss: 2.608768\n",
      "(Iteration 641 / 7640) loss: 2.554000\n",
      "(Iteration 651 / 7640) loss: 2.501502\n",
      "(Iteration 661 / 7640) loss: 2.598275\n",
      "(Iteration 671 / 7640) loss: 2.365554\n",
      "(Iteration 681 / 7640) loss: 2.556336\n",
      "(Iteration 691 / 7640) loss: 2.543793\n",
      "(Iteration 701 / 7640) loss: 2.344878\n",
      "(Iteration 711 / 7640) loss: 2.324581\n",
      "(Iteration 721 / 7640) loss: 2.389234\n",
      "(Iteration 731 / 7640) loss: 2.213909\n",
      "(Iteration 741 / 7640) loss: 2.371520\n",
      "(Iteration 751 / 7640) loss: 2.374585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 761 / 7640) loss: 2.273844\n",
      "(Epoch 2 / 20) train acc: 0.383000; val_acc: 0.373000\n",
      "(Iteration 771 / 7640) loss: 2.333025\n",
      "(Iteration 781 / 7640) loss: 2.378256\n",
      "(Iteration 791 / 7640) loss: 2.453417\n",
      "(Iteration 801 / 7640) loss: 2.298884\n",
      "(Iteration 811 / 7640) loss: 2.402336\n",
      "(Iteration 821 / 7640) loss: 2.348206\n",
      "(Iteration 831 / 7640) loss: 2.229239\n",
      "(Iteration 841 / 7640) loss: 2.346681\n",
      "(Iteration 851 / 7640) loss: 2.251670\n",
      "(Iteration 861 / 7640) loss: 2.193424\n",
      "(Iteration 871 / 7640) loss: 2.211038\n",
      "(Iteration 881 / 7640) loss: 2.412475\n",
      "(Iteration 891 / 7640) loss: 2.257050\n",
      "(Iteration 901 / 7640) loss: 2.394095\n",
      "(Iteration 911 / 7640) loss: 2.215102\n",
      "(Iteration 921 / 7640) loss: 2.226062\n",
      "(Iteration 931 / 7640) loss: 2.313781\n",
      "(Iteration 941 / 7640) loss: 2.260916\n",
      "(Iteration 951 / 7640) loss: 2.206473\n",
      "(Iteration 961 / 7640) loss: 2.099625\n",
      "(Iteration 971 / 7640) loss: 2.156533\n",
      "(Iteration 981 / 7640) loss: 2.181695\n",
      "(Iteration 991 / 7640) loss: 2.130339\n",
      "(Iteration 1001 / 7640) loss: 2.201664\n",
      "(Iteration 1011 / 7640) loss: 2.224137\n",
      "(Iteration 1021 / 7640) loss: 2.064793\n",
      "(Iteration 1031 / 7640) loss: 2.240640\n",
      "(Iteration 1041 / 7640) loss: 2.198437\n",
      "(Iteration 1051 / 7640) loss: 2.066364\n",
      "(Iteration 1061 / 7640) loss: 2.167533\n",
      "(Iteration 1071 / 7640) loss: 2.158527\n",
      "(Iteration 1081 / 7640) loss: 2.200285\n",
      "(Iteration 1091 / 7640) loss: 2.151075\n",
      "(Iteration 1101 / 7640) loss: 2.081072\n",
      "(Iteration 1111 / 7640) loss: 2.017167\n",
      "(Iteration 1121 / 7640) loss: 2.225547\n",
      "(Iteration 1131 / 7640) loss: 2.118322\n",
      "(Iteration 1141 / 7640) loss: 2.134769\n",
      "(Epoch 3 / 20) train acc: 0.432000; val_acc: 0.428000\n",
      "(Iteration 1151 / 7640) loss: 2.053794\n",
      "(Iteration 1161 / 7640) loss: 2.081795\n",
      "(Iteration 1171 / 7640) loss: 2.029020\n",
      "(Iteration 1181 / 7640) loss: 2.021527\n",
      "(Iteration 1191 / 7640) loss: 2.058877\n",
      "(Iteration 1201 / 7640) loss: 2.006553\n",
      "(Iteration 1211 / 7640) loss: 2.077110\n",
      "(Iteration 1221 / 7640) loss: 2.109588\n",
      "(Iteration 1231 / 7640) loss: 2.064407\n",
      "(Iteration 1241 / 7640) loss: 1.890798\n",
      "(Iteration 1251 / 7640) loss: 2.073383\n",
      "(Iteration 1261 / 7640) loss: 2.012488\n",
      "(Iteration 1271 / 7640) loss: 1.822918\n",
      "(Iteration 1281 / 7640) loss: 2.162769\n",
      "(Iteration 1291 / 7640) loss: 1.967393\n",
      "(Iteration 1301 / 7640) loss: 1.970552\n",
      "(Iteration 1311 / 7640) loss: 2.023231\n",
      "(Iteration 1321 / 7640) loss: 2.085225\n",
      "(Iteration 1331 / 7640) loss: 1.999317\n",
      "(Iteration 1341 / 7640) loss: 1.913926\n",
      "(Iteration 1351 / 7640) loss: 1.824928\n",
      "(Iteration 1361 / 7640) loss: 2.011923\n",
      "(Iteration 1371 / 7640) loss: 2.162917\n",
      "(Iteration 1381 / 7640) loss: 1.981515\n",
      "(Iteration 1391 / 7640) loss: 1.922416\n",
      "(Iteration 1401 / 7640) loss: 1.995335\n",
      "(Iteration 1411 / 7640) loss: 2.021877\n",
      "(Iteration 1421 / 7640) loss: 1.983090\n",
      "(Iteration 1431 / 7640) loss: 1.960976\n",
      "(Iteration 1441 / 7640) loss: 1.899001\n",
      "(Iteration 1451 / 7640) loss: 1.927984\n",
      "(Iteration 1461 / 7640) loss: 1.809224\n",
      "(Iteration 1471 / 7640) loss: 1.936107\n",
      "(Iteration 1481 / 7640) loss: 2.009590\n",
      "(Iteration 1491 / 7640) loss: 2.016276\n",
      "(Iteration 1501 / 7640) loss: 1.961621\n",
      "(Iteration 1511 / 7640) loss: 1.888811\n",
      "(Iteration 1521 / 7640) loss: 1.959862\n",
      "(Epoch 4 / 20) train acc: 0.394000; val_acc: 0.433000\n",
      "(Iteration 1531 / 7640) loss: 1.902278\n",
      "(Iteration 1541 / 7640) loss: 1.853804\n",
      "(Iteration 1551 / 7640) loss: 2.114635\n",
      "(Iteration 1561 / 7640) loss: 1.996971\n",
      "(Iteration 1571 / 7640) loss: 2.062787\n",
      "(Iteration 1581 / 7640) loss: 1.925452\n",
      "(Iteration 1591 / 7640) loss: 1.917120\n",
      "(Iteration 1601 / 7640) loss: 1.897685\n",
      "(Iteration 1611 / 7640) loss: 1.903129\n",
      "(Iteration 1621 / 7640) loss: 1.942061\n",
      "(Iteration 1631 / 7640) loss: 1.851534\n",
      "(Iteration 1641 / 7640) loss: 1.981264\n",
      "(Iteration 1651 / 7640) loss: 1.958589\n",
      "(Iteration 1661 / 7640) loss: 1.899838\n",
      "(Iteration 1671 / 7640) loss: 1.803891\n",
      "(Iteration 1681 / 7640) loss: 2.078074\n",
      "(Iteration 1691 / 7640) loss: 1.845315\n",
      "(Iteration 1701 / 7640) loss: 1.750470\n",
      "(Iteration 1711 / 7640) loss: 1.879709\n",
      "(Iteration 1721 / 7640) loss: 1.875603\n",
      "(Iteration 1731 / 7640) loss: 1.904275\n",
      "(Iteration 1741 / 7640) loss: 1.883468\n",
      "(Iteration 1751 / 7640) loss: 1.923285\n",
      "(Iteration 1761 / 7640) loss: 1.884050\n",
      "(Iteration 1771 / 7640) loss: 1.927259\n",
      "(Iteration 1781 / 7640) loss: 1.963030\n",
      "(Iteration 1791 / 7640) loss: 1.790395\n",
      "(Iteration 1801 / 7640) loss: 1.877267\n",
      "(Iteration 1811 / 7640) loss: 1.975727\n",
      "(Iteration 1821 / 7640) loss: 2.034911\n",
      "(Iteration 1831 / 7640) loss: 1.923376\n",
      "(Iteration 1841 / 7640) loss: 1.884360\n",
      "(Iteration 1851 / 7640) loss: 1.674472\n",
      "(Iteration 1861 / 7640) loss: 1.941094\n",
      "(Iteration 1871 / 7640) loss: 1.789869\n",
      "(Iteration 1881 / 7640) loss: 1.765368\n",
      "(Iteration 1891 / 7640) loss: 1.906723\n",
      "(Iteration 1901 / 7640) loss: 2.002926\n",
      "(Epoch 5 / 20) train acc: 0.436000; val_acc: 0.449000\n",
      "(Iteration 1911 / 7640) loss: 1.921573\n",
      "(Iteration 1921 / 7640) loss: 1.872700\n",
      "(Iteration 1931 / 7640) loss: 1.907390\n",
      "(Iteration 1941 / 7640) loss: 1.930539\n",
      "(Iteration 1951 / 7640) loss: 1.709292\n",
      "(Iteration 1961 / 7640) loss: 1.804391\n",
      "(Iteration 1971 / 7640) loss: 1.860820\n",
      "(Iteration 1981 / 7640) loss: 1.805830\n",
      "(Iteration 1991 / 7640) loss: 1.738510\n",
      "(Iteration 2001 / 7640) loss: 1.811433\n",
      "(Iteration 2011 / 7640) loss: 1.782077\n",
      "(Iteration 2021 / 7640) loss: 1.843318\n",
      "(Iteration 2031 / 7640) loss: 1.846125\n",
      "(Iteration 2041 / 7640) loss: 1.846069\n",
      "(Iteration 2051 / 7640) loss: 1.920186\n",
      "(Iteration 2061 / 7640) loss: 2.006341\n",
      "(Iteration 2071 / 7640) loss: 2.018688\n",
      "(Iteration 2081 / 7640) loss: 1.799254\n",
      "(Iteration 2091 / 7640) loss: 1.774777\n",
      "(Iteration 2101 / 7640) loss: 1.894846\n",
      "(Iteration 2111 / 7640) loss: 1.923274\n",
      "(Iteration 2121 / 7640) loss: 1.785254\n",
      "(Iteration 2131 / 7640) loss: 1.870564\n",
      "(Iteration 2141 / 7640) loss: 1.817369\n",
      "(Iteration 2151 / 7640) loss: 1.787354\n",
      "(Iteration 2161 / 7640) loss: 1.837416\n",
      "(Iteration 2171 / 7640) loss: 1.731122\n",
      "(Iteration 2181 / 7640) loss: 1.601937\n",
      "(Iteration 2191 / 7640) loss: 1.843250\n",
      "(Iteration 2201 / 7640) loss: 1.915251\n",
      "(Iteration 2211 / 7640) loss: 1.809333\n",
      "(Iteration 2221 / 7640) loss: 1.881532\n",
      "(Iteration 2231 / 7640) loss: 1.934841\n",
      "(Iteration 2241 / 7640) loss: 1.767544\n",
      "(Iteration 2251 / 7640) loss: 1.830272\n",
      "(Iteration 2261 / 7640) loss: 1.979946\n",
      "(Iteration 2271 / 7640) loss: 1.743731\n",
      "(Iteration 2281 / 7640) loss: 1.971975\n",
      "(Iteration 2291 / 7640) loss: 1.761660\n",
      "(Epoch 6 / 20) train acc: 0.450000; val_acc: 0.461000\n",
      "(Iteration 2301 / 7640) loss: 1.868430\n",
      "(Iteration 2311 / 7640) loss: 1.892307\n",
      "(Iteration 2321 / 7640) loss: 1.873306\n",
      "(Iteration 2331 / 7640) loss: 1.827183\n",
      "(Iteration 2341 / 7640) loss: 1.664573\n",
      "(Iteration 2351 / 7640) loss: 1.614914\n",
      "(Iteration 2361 / 7640) loss: 1.773541\n",
      "(Iteration 2371 / 7640) loss: 1.765659\n",
      "(Iteration 2381 / 7640) loss: 1.804657\n",
      "(Iteration 2391 / 7640) loss: 1.875232\n",
      "(Iteration 2401 / 7640) loss: 1.721633\n",
      "(Iteration 2411 / 7640) loss: 1.696439\n",
      "(Iteration 2421 / 7640) loss: 1.735903\n",
      "(Iteration 2431 / 7640) loss: 1.836811\n",
      "(Iteration 2441 / 7640) loss: 1.833294\n",
      "(Iteration 2451 / 7640) loss: 1.743718\n",
      "(Iteration 2461 / 7640) loss: 1.824744\n",
      "(Iteration 2471 / 7640) loss: 1.918503\n",
      "(Iteration 2481 / 7640) loss: 1.799149\n",
      "(Iteration 2491 / 7640) loss: 1.726271\n",
      "(Iteration 2501 / 7640) loss: 1.768147\n",
      "(Iteration 2511 / 7640) loss: 1.676349\n",
      "(Iteration 2521 / 7640) loss: 1.905443\n",
      "(Iteration 2531 / 7640) loss: 1.700533\n",
      "(Iteration 2541 / 7640) loss: 1.740583\n",
      "(Iteration 2551 / 7640) loss: 1.762126\n",
      "(Iteration 2561 / 7640) loss: 1.840060\n",
      "(Iteration 2571 / 7640) loss: 1.755799\n",
      "(Iteration 2581 / 7640) loss: 1.781452\n",
      "(Iteration 2591 / 7640) loss: 1.862471\n",
      "(Iteration 2601 / 7640) loss: 1.708570\n",
      "(Iteration 2611 / 7640) loss: 1.806839\n",
      "(Iteration 2621 / 7640) loss: 1.824961\n",
      "(Iteration 2631 / 7640) loss: 1.837193\n",
      "(Iteration 2641 / 7640) loss: 1.545458\n",
      "(Iteration 2651 / 7640) loss: 1.685078\n",
      "(Iteration 2661 / 7640) loss: 1.757447\n",
      "(Iteration 2671 / 7640) loss: 1.528629\n",
      "(Epoch 7 / 20) train acc: 0.451000; val_acc: 0.451000\n",
      "(Iteration 2681 / 7640) loss: 1.645452\n",
      "(Iteration 2691 / 7640) loss: 1.722973\n",
      "(Iteration 2701 / 7640) loss: 1.937763\n",
      "(Iteration 2711 / 7640) loss: 1.939572\n",
      "(Iteration 2721 / 7640) loss: 1.830184\n",
      "(Iteration 2731 / 7640) loss: 1.817840\n",
      "(Iteration 2741 / 7640) loss: 1.699877\n",
      "(Iteration 2751 / 7640) loss: 1.584370\n",
      "(Iteration 2761 / 7640) loss: 1.825848\n",
      "(Iteration 2771 / 7640) loss: 1.817054\n",
      "(Iteration 2781 / 7640) loss: 1.657005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2791 / 7640) loss: 1.840645\n",
      "(Iteration 2801 / 7640) loss: 1.801487\n",
      "(Iteration 2811 / 7640) loss: 1.905064\n",
      "(Iteration 2821 / 7640) loss: 1.726643\n",
      "(Iteration 2831 / 7640) loss: 1.742603\n",
      "(Iteration 2841 / 7640) loss: 1.732458\n",
      "(Iteration 2851 / 7640) loss: 1.816495\n",
      "(Iteration 2861 / 7640) loss: 1.771600\n",
      "(Iteration 2871 / 7640) loss: 1.834814\n",
      "(Iteration 2881 / 7640) loss: 1.865036\n",
      "(Iteration 2891 / 7640) loss: 1.684684\n",
      "(Iteration 2901 / 7640) loss: 1.803095\n",
      "(Iteration 2911 / 7640) loss: 1.850158\n",
      "(Iteration 2921 / 7640) loss: 1.779201\n",
      "(Iteration 2931 / 7640) loss: 1.812621\n",
      "(Iteration 2941 / 7640) loss: 1.782184\n",
      "(Iteration 2951 / 7640) loss: 1.741954\n",
      "(Iteration 2961 / 7640) loss: 1.792936\n",
      "(Iteration 2971 / 7640) loss: 1.725357\n",
      "(Iteration 2981 / 7640) loss: 1.695146\n",
      "(Iteration 2991 / 7640) loss: 1.620975\n",
      "(Iteration 3001 / 7640) loss: 1.859030\n",
      "(Iteration 3011 / 7640) loss: 1.791655\n",
      "(Iteration 3021 / 7640) loss: 1.803989\n",
      "(Iteration 3031 / 7640) loss: 1.877203\n",
      "(Iteration 3041 / 7640) loss: 1.814606\n",
      "(Iteration 3051 / 7640) loss: 1.574453\n",
      "(Epoch 8 / 20) train acc: 0.423000; val_acc: 0.439000\n",
      "(Iteration 3061 / 7640) loss: 1.734772\n",
      "(Iteration 3071 / 7640) loss: 1.712186\n",
      "(Iteration 3081 / 7640) loss: 1.614470\n",
      "(Iteration 3091 / 7640) loss: 1.692701\n",
      "(Iteration 3101 / 7640) loss: 1.680465\n",
      "(Iteration 3111 / 7640) loss: 1.789240\n",
      "(Iteration 3121 / 7640) loss: 1.725380\n",
      "(Iteration 3131 / 7640) loss: 1.662124\n",
      "(Iteration 3141 / 7640) loss: 1.672520\n",
      "(Iteration 3151 / 7640) loss: 1.789000\n",
      "(Iteration 3161 / 7640) loss: 1.767290\n",
      "(Iteration 3171 / 7640) loss: 1.770669\n",
      "(Iteration 3181 / 7640) loss: 1.826575\n",
      "(Iteration 3191 / 7640) loss: 1.789113\n",
      "(Iteration 3201 / 7640) loss: 1.680642\n",
      "(Iteration 3211 / 7640) loss: 1.683309\n",
      "(Iteration 3221 / 7640) loss: 1.747611\n",
      "(Iteration 3231 / 7640) loss: 1.646443\n",
      "(Iteration 3241 / 7640) loss: 1.826456\n",
      "(Iteration 3251 / 7640) loss: 1.682598\n",
      "(Iteration 3261 / 7640) loss: 1.767177\n",
      "(Iteration 3271 / 7640) loss: 1.689513\n",
      "(Iteration 3281 / 7640) loss: 1.820020\n",
      "(Iteration 3291 / 7640) loss: 1.733974\n",
      "(Iteration 3301 / 7640) loss: 1.683638\n",
      "(Iteration 3311 / 7640) loss: 1.526671\n",
      "(Iteration 3321 / 7640) loss: 1.870001\n",
      "(Iteration 3331 / 7640) loss: 1.699470\n",
      "(Iteration 3341 / 7640) loss: 1.602021\n",
      "(Iteration 3351 / 7640) loss: 1.815977\n",
      "(Iteration 3361 / 7640) loss: 1.614421\n",
      "(Iteration 3371 / 7640) loss: 1.744213\n",
      "(Iteration 3381 / 7640) loss: 1.811875\n",
      "(Iteration 3391 / 7640) loss: 1.736147\n",
      "(Iteration 3401 / 7640) loss: 1.592406\n",
      "(Iteration 3411 / 7640) loss: 1.772067\n",
      "(Iteration 3421 / 7640) loss: 1.644872\n",
      "(Iteration 3431 / 7640) loss: 1.867014\n",
      "(Epoch 9 / 20) train acc: 0.469000; val_acc: 0.444000\n",
      "(Iteration 3441 / 7640) loss: 1.766288\n",
      "(Iteration 3451 / 7640) loss: 1.582218\n",
      "(Iteration 3461 / 7640) loss: 1.831679\n",
      "(Iteration 3471 / 7640) loss: 1.722955\n",
      "(Iteration 3481 / 7640) loss: 1.701216\n",
      "(Iteration 3491 / 7640) loss: 1.690801\n",
      "(Iteration 3501 / 7640) loss: 1.878190\n",
      "(Iteration 3511 / 7640) loss: 1.697621\n",
      "(Iteration 3521 / 7640) loss: 1.711804\n",
      "(Iteration 3531 / 7640) loss: 1.684012\n",
      "(Iteration 3541 / 7640) loss: 1.742474\n",
      "(Iteration 3551 / 7640) loss: 1.700421\n",
      "(Iteration 3561 / 7640) loss: 1.536462\n",
      "(Iteration 3571 / 7640) loss: 1.689285\n",
      "(Iteration 3581 / 7640) loss: 1.795288\n",
      "(Iteration 3591 / 7640) loss: 1.753831\n",
      "(Iteration 3601 / 7640) loss: 1.819248\n",
      "(Iteration 3611 / 7640) loss: 1.724324\n",
      "(Iteration 3621 / 7640) loss: 1.658567\n",
      "(Iteration 3631 / 7640) loss: 1.646076\n",
      "(Iteration 3641 / 7640) loss: 1.627571\n",
      "(Iteration 3651 / 7640) loss: 1.649062\n",
      "(Iteration 3661 / 7640) loss: 1.712009\n",
      "(Iteration 3671 / 7640) loss: 1.650460\n",
      "(Iteration 3681 / 7640) loss: 1.671873\n",
      "(Iteration 3691 / 7640) loss: 1.749380\n",
      "(Iteration 3701 / 7640) loss: 1.649939\n",
      "(Iteration 3711 / 7640) loss: 1.738193\n",
      "(Iteration 3721 / 7640) loss: 1.751337\n",
      "(Iteration 3731 / 7640) loss: 1.711307\n",
      "(Iteration 3741 / 7640) loss: 1.906242\n",
      "(Iteration 3751 / 7640) loss: 1.764406\n",
      "(Iteration 3761 / 7640) loss: 1.842796\n",
      "(Iteration 3771 / 7640) loss: 1.653934\n",
      "(Iteration 3781 / 7640) loss: 1.584053\n",
      "(Iteration 3791 / 7640) loss: 1.603190\n",
      "(Iteration 3801 / 7640) loss: 1.778676\n",
      "(Iteration 3811 / 7640) loss: 1.623995\n",
      "(Epoch 10 / 20) train acc: 0.441000; val_acc: 0.461000\n",
      "(Iteration 3821 / 7640) loss: 1.948771\n",
      "(Iteration 3831 / 7640) loss: 1.834233\n",
      "(Iteration 3841 / 7640) loss: 1.772999\n",
      "(Iteration 3851 / 7640) loss: 1.827420\n",
      "(Iteration 3861 / 7640) loss: 1.703072\n",
      "(Iteration 3871 / 7640) loss: 1.590702\n",
      "(Iteration 3881 / 7640) loss: 1.768945\n",
      "(Iteration 3891 / 7640) loss: 1.670623\n",
      "(Iteration 3901 / 7640) loss: 1.699139\n",
      "(Iteration 3911 / 7640) loss: 1.801970\n",
      "(Iteration 3921 / 7640) loss: 1.609183\n",
      "(Iteration 3931 / 7640) loss: 1.598814\n",
      "(Iteration 3941 / 7640) loss: 1.700267\n",
      "(Iteration 3951 / 7640) loss: 1.664061\n",
      "(Iteration 3961 / 7640) loss: 1.774593\n",
      "(Iteration 3971 / 7640) loss: 1.804204\n",
      "(Iteration 3981 / 7640) loss: 1.613207\n",
      "(Iteration 3991 / 7640) loss: 1.632362\n",
      "(Iteration 4001 / 7640) loss: 1.709890\n",
      "(Iteration 4011 / 7640) loss: 1.728571\n",
      "(Iteration 4021 / 7640) loss: 1.636095\n",
      "(Iteration 4031 / 7640) loss: 1.595901\n",
      "(Iteration 4041 / 7640) loss: 1.744640\n",
      "(Iteration 4051 / 7640) loss: 1.590020\n",
      "(Iteration 4061 / 7640) loss: 1.665375\n",
      "(Iteration 4071 / 7640) loss: 1.732790\n",
      "(Iteration 4081 / 7640) loss: 1.697612\n",
      "(Iteration 4091 / 7640) loss: 1.645458\n",
      "(Iteration 4101 / 7640) loss: 1.683182\n",
      "(Iteration 4111 / 7640) loss: 1.524019\n",
      "(Iteration 4121 / 7640) loss: 1.769310\n",
      "(Iteration 4131 / 7640) loss: 1.677806\n",
      "(Iteration 4141 / 7640) loss: 1.773799\n",
      "(Iteration 4151 / 7640) loss: 1.686167\n",
      "(Iteration 4161 / 7640) loss: 1.729571\n",
      "(Iteration 4171 / 7640) loss: 1.758901\n",
      "(Iteration 4181 / 7640) loss: 1.598450\n",
      "(Iteration 4191 / 7640) loss: 1.571552\n",
      "(Iteration 4201 / 7640) loss: 1.560540\n",
      "(Epoch 11 / 20) train acc: 0.444000; val_acc: 0.465000\n",
      "(Iteration 4211 / 7640) loss: 1.690857\n",
      "(Iteration 4221 / 7640) loss: 1.763014\n",
      "(Iteration 4231 / 7640) loss: 1.791843\n",
      "(Iteration 4241 / 7640) loss: 1.682682\n",
      "(Iteration 4251 / 7640) loss: 1.883332\n",
      "(Iteration 4261 / 7640) loss: 1.671323\n",
      "(Iteration 4271 / 7640) loss: 1.859194\n",
      "(Iteration 4281 / 7640) loss: 1.826411\n",
      "(Iteration 4291 / 7640) loss: 1.780963\n",
      "(Iteration 4301 / 7640) loss: 1.951374\n",
      "(Iteration 4311 / 7640) loss: 1.637271\n",
      "(Iteration 4321 / 7640) loss: 1.580079\n",
      "(Iteration 4331 / 7640) loss: 1.883040\n",
      "(Iteration 4341 / 7640) loss: 1.600044\n",
      "(Iteration 4351 / 7640) loss: 1.770809\n",
      "(Iteration 4361 / 7640) loss: 1.775999\n",
      "(Iteration 4371 / 7640) loss: 1.592130\n",
      "(Iteration 4381 / 7640) loss: 1.704220\n",
      "(Iteration 4391 / 7640) loss: 1.446836\n",
      "(Iteration 4401 / 7640) loss: 1.660218\n",
      "(Iteration 4411 / 7640) loss: 1.615452\n",
      "(Iteration 4421 / 7640) loss: 1.640436\n",
      "(Iteration 4431 / 7640) loss: 1.714814\n",
      "(Iteration 4441 / 7640) loss: 1.569962\n",
      "(Iteration 4451 / 7640) loss: 1.730296\n",
      "(Iteration 4461 / 7640) loss: 1.695706\n",
      "(Iteration 4471 / 7640) loss: 1.826824\n",
      "(Iteration 4481 / 7640) loss: 1.636404\n",
      "(Iteration 4491 / 7640) loss: 1.638632\n",
      "(Iteration 4501 / 7640) loss: 1.646174\n",
      "(Iteration 4511 / 7640) loss: 1.774249\n",
      "(Iteration 4521 / 7640) loss: 1.796440\n",
      "(Iteration 4531 / 7640) loss: 1.594926\n",
      "(Iteration 4541 / 7640) loss: 1.707560\n",
      "(Iteration 4551 / 7640) loss: 1.717001\n",
      "(Iteration 4561 / 7640) loss: 1.694471\n",
      "(Iteration 4571 / 7640) loss: 1.921785\n",
      "(Iteration 4581 / 7640) loss: 1.737567\n",
      "(Epoch 12 / 20) train acc: 0.455000; val_acc: 0.453000\n",
      "(Iteration 4591 / 7640) loss: 1.660351\n",
      "(Iteration 4601 / 7640) loss: 1.600119\n",
      "(Iteration 4611 / 7640) loss: 1.730556\n",
      "(Iteration 4621 / 7640) loss: 1.668893\n",
      "(Iteration 4631 / 7640) loss: 1.673280\n",
      "(Iteration 4641 / 7640) loss: 1.969518\n",
      "(Iteration 4651 / 7640) loss: 1.641139\n",
      "(Iteration 4661 / 7640) loss: 1.689785\n",
      "(Iteration 4671 / 7640) loss: 1.630093\n",
      "(Iteration 4681 / 7640) loss: 1.563337\n",
      "(Iteration 4691 / 7640) loss: 1.608564\n",
      "(Iteration 4701 / 7640) loss: 1.512009\n",
      "(Iteration 4711 / 7640) loss: 1.698629\n",
      "(Iteration 4721 / 7640) loss: 1.746991\n",
      "(Iteration 4731 / 7640) loss: 1.698590\n",
      "(Iteration 4741 / 7640) loss: 1.867910\n",
      "(Iteration 4751 / 7640) loss: 1.611284\n",
      "(Iteration 4761 / 7640) loss: 1.817973\n",
      "(Iteration 4771 / 7640) loss: 1.817708\n",
      "(Iteration 4781 / 7640) loss: 1.568305\n",
      "(Iteration 4791 / 7640) loss: 1.802584\n",
      "(Iteration 4801 / 7640) loss: 1.741141\n",
      "(Iteration 4811 / 7640) loss: 1.591493\n",
      "(Iteration 4821 / 7640) loss: 1.694016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4831 / 7640) loss: 1.664682\n",
      "(Iteration 4841 / 7640) loss: 1.653885\n",
      "(Iteration 4851 / 7640) loss: 1.751058\n",
      "(Iteration 4861 / 7640) loss: 1.702282\n",
      "(Iteration 4871 / 7640) loss: 1.858015\n",
      "(Iteration 4881 / 7640) loss: 1.662232\n",
      "(Iteration 4891 / 7640) loss: 1.703306\n",
      "(Iteration 4901 / 7640) loss: 1.688673\n",
      "(Iteration 4911 / 7640) loss: 1.654923\n",
      "(Iteration 4921 / 7640) loss: 1.975778\n",
      "(Iteration 4931 / 7640) loss: 1.573969\n",
      "(Iteration 4941 / 7640) loss: 1.613515\n",
      "(Iteration 4951 / 7640) loss: 1.604356\n",
      "(Iteration 4961 / 7640) loss: 1.766052\n",
      "(Epoch 13 / 20) train acc: 0.457000; val_acc: 0.461000\n",
      "(Iteration 4971 / 7640) loss: 1.671188\n",
      "(Iteration 4981 / 7640) loss: 1.518478\n",
      "(Iteration 4991 / 7640) loss: 1.726706\n",
      "(Iteration 5001 / 7640) loss: 1.819873\n",
      "(Iteration 5011 / 7640) loss: 1.677119\n",
      "(Iteration 5021 / 7640) loss: 1.758632\n",
      "(Iteration 5031 / 7640) loss: 1.717400\n",
      "(Iteration 5041 / 7640) loss: 1.653049\n",
      "(Iteration 5051 / 7640) loss: 1.451503\n",
      "(Iteration 5061 / 7640) loss: 1.852620\n",
      "(Iteration 5071 / 7640) loss: 1.539618\n",
      "(Iteration 5081 / 7640) loss: 1.621978\n",
      "(Iteration 5091 / 7640) loss: 1.635964\n",
      "(Iteration 5101 / 7640) loss: 1.830450\n",
      "(Iteration 5111 / 7640) loss: 1.598217\n",
      "(Iteration 5121 / 7640) loss: 1.743039\n",
      "(Iteration 5131 / 7640) loss: 1.715831\n",
      "(Iteration 5141 / 7640) loss: 1.501819\n",
      "(Iteration 5151 / 7640) loss: 1.660680\n",
      "(Iteration 5161 / 7640) loss: 1.896450\n",
      "(Iteration 5171 / 7640) loss: 1.692534\n",
      "(Iteration 5181 / 7640) loss: 1.853261\n",
      "(Iteration 5191 / 7640) loss: 1.605911\n",
      "(Iteration 5201 / 7640) loss: 1.768173\n",
      "(Iteration 5211 / 7640) loss: 1.681206\n",
      "(Iteration 5221 / 7640) loss: 1.646558\n",
      "(Iteration 5231 / 7640) loss: 1.702413\n",
      "(Iteration 5241 / 7640) loss: 1.657887\n",
      "(Iteration 5251 / 7640) loss: 1.492384\n",
      "(Iteration 5261 / 7640) loss: 1.699776\n",
      "(Iteration 5271 / 7640) loss: 1.557269\n",
      "(Iteration 5281 / 7640) loss: 1.719278\n",
      "(Iteration 5291 / 7640) loss: 1.812912\n",
      "(Iteration 5301 / 7640) loss: 1.573607\n",
      "(Iteration 5311 / 7640) loss: 1.731712\n",
      "(Iteration 5321 / 7640) loss: 1.623023\n",
      "(Iteration 5331 / 7640) loss: 1.662533\n",
      "(Iteration 5341 / 7640) loss: 1.611137\n",
      "(Epoch 14 / 20) train acc: 0.473000; val_acc: 0.480000\n",
      "(Iteration 5351 / 7640) loss: 1.590754\n",
      "(Iteration 5361 / 7640) loss: 1.704776\n",
      "(Iteration 5371 / 7640) loss: 1.603697\n",
      "(Iteration 5381 / 7640) loss: 1.667716\n",
      "(Iteration 5391 / 7640) loss: 1.682727\n",
      "(Iteration 5401 / 7640) loss: 1.761251\n",
      "(Iteration 5411 / 7640) loss: 1.712450\n",
      "(Iteration 5421 / 7640) loss: 1.635726\n",
      "(Iteration 5431 / 7640) loss: 1.845642\n",
      "(Iteration 5441 / 7640) loss: 1.703392\n",
      "(Iteration 5451 / 7640) loss: 1.773483\n",
      "(Iteration 5461 / 7640) loss: 1.680997\n",
      "(Iteration 5471 / 7640) loss: 1.692381\n",
      "(Iteration 5481 / 7640) loss: 1.571360\n",
      "(Iteration 5491 / 7640) loss: 1.564114\n",
      "(Iteration 5501 / 7640) loss: 1.830383\n",
      "(Iteration 5511 / 7640) loss: 1.661261\n",
      "(Iteration 5521 / 7640) loss: 1.695862\n",
      "(Iteration 5531 / 7640) loss: 1.588810\n",
      "(Iteration 5541 / 7640) loss: 1.611450\n",
      "(Iteration 5551 / 7640) loss: 1.726777\n",
      "(Iteration 5561 / 7640) loss: 1.630131\n",
      "(Iteration 5571 / 7640) loss: 1.631128\n",
      "(Iteration 5581 / 7640) loss: 1.559833\n",
      "(Iteration 5591 / 7640) loss: 1.604945\n",
      "(Iteration 5601 / 7640) loss: 1.495583\n",
      "(Iteration 5611 / 7640) loss: 1.707918\n",
      "(Iteration 5621 / 7640) loss: 1.677104\n",
      "(Iteration 5631 / 7640) loss: 1.700815\n",
      "(Iteration 5641 / 7640) loss: 1.583093\n",
      "(Iteration 5651 / 7640) loss: 1.653752\n",
      "(Iteration 5661 / 7640) loss: 1.610175\n",
      "(Iteration 5671 / 7640) loss: 1.791366\n",
      "(Iteration 5681 / 7640) loss: 1.759045\n",
      "(Iteration 5691 / 7640) loss: 1.863849\n",
      "(Iteration 5701 / 7640) loss: 1.707363\n",
      "(Iteration 5711 / 7640) loss: 1.903710\n",
      "(Iteration 5721 / 7640) loss: 1.630199\n",
      "(Epoch 15 / 20) train acc: 0.499000; val_acc: 0.437000\n",
      "(Iteration 5731 / 7640) loss: 1.660583\n",
      "(Iteration 5741 / 7640) loss: 1.697740\n",
      "(Iteration 5751 / 7640) loss: 1.714002\n",
      "(Iteration 5761 / 7640) loss: 1.657810\n",
      "(Iteration 5771 / 7640) loss: 1.658671\n",
      "(Iteration 5781 / 7640) loss: 1.769706\n",
      "(Iteration 5791 / 7640) loss: 1.701003\n",
      "(Iteration 5801 / 7640) loss: 1.634202\n",
      "(Iteration 5811 / 7640) loss: 1.829255\n",
      "(Iteration 5821 / 7640) loss: 1.667770\n",
      "(Iteration 5831 / 7640) loss: 1.878684\n",
      "(Iteration 5841 / 7640) loss: 1.632429\n",
      "(Iteration 5851 / 7640) loss: 1.704865\n",
      "(Iteration 5861 / 7640) loss: 1.650291\n",
      "(Iteration 5871 / 7640) loss: 1.652555\n",
      "(Iteration 5881 / 7640) loss: 1.772952\n",
      "(Iteration 5891 / 7640) loss: 1.595989\n",
      "(Iteration 5901 / 7640) loss: 1.670745\n",
      "(Iteration 5911 / 7640) loss: 1.753659\n",
      "(Iteration 5921 / 7640) loss: 1.752429\n",
      "(Iteration 5931 / 7640) loss: 1.477451\n",
      "(Iteration 5941 / 7640) loss: 1.623520\n",
      "(Iteration 5951 / 7640) loss: 1.538145\n",
      "(Iteration 5961 / 7640) loss: 1.713310\n",
      "(Iteration 5971 / 7640) loss: 1.654581\n",
      "(Iteration 5981 / 7640) loss: 1.662964\n",
      "(Iteration 5991 / 7640) loss: 1.538354\n",
      "(Iteration 6001 / 7640) loss: 1.565158\n",
      "(Iteration 6011 / 7640) loss: 1.686527\n",
      "(Iteration 6021 / 7640) loss: 1.727546\n",
      "(Iteration 6031 / 7640) loss: 1.580652\n",
      "(Iteration 6041 / 7640) loss: 1.482912\n",
      "(Iteration 6051 / 7640) loss: 1.734321\n",
      "(Iteration 6061 / 7640) loss: 1.737133\n",
      "(Iteration 6071 / 7640) loss: 1.789678\n",
      "(Iteration 6081 / 7640) loss: 1.785952\n",
      "(Iteration 6091 / 7640) loss: 1.587872\n",
      "(Iteration 6101 / 7640) loss: 1.702441\n",
      "(Iteration 6111 / 7640) loss: 1.628916\n",
      "(Epoch 16 / 20) train acc: 0.499000; val_acc: 0.445000\n",
      "(Iteration 6121 / 7640) loss: 1.481142\n",
      "(Iteration 6131 / 7640) loss: 1.755627\n",
      "(Iteration 6141 / 7640) loss: 1.782501\n",
      "(Iteration 6151 / 7640) loss: 1.697493\n",
      "(Iteration 6161 / 7640) loss: 1.684273\n",
      "(Iteration 6171 / 7640) loss: 1.659778\n",
      "(Iteration 6181 / 7640) loss: 1.701198\n",
      "(Iteration 6191 / 7640) loss: 1.715310\n",
      "(Iteration 6201 / 7640) loss: 1.590096\n",
      "(Iteration 6211 / 7640) loss: 1.652941\n",
      "(Iteration 6221 / 7640) loss: 1.641595\n",
      "(Iteration 6231 / 7640) loss: 1.693149\n",
      "(Iteration 6241 / 7640) loss: 1.515654\n",
      "(Iteration 6251 / 7640) loss: 1.773069\n",
      "(Iteration 6261 / 7640) loss: 1.730509\n",
      "(Iteration 6271 / 7640) loss: 1.746508\n",
      "(Iteration 6281 / 7640) loss: 1.721384\n",
      "(Iteration 6291 / 7640) loss: 1.646200\n",
      "(Iteration 6301 / 7640) loss: 1.781275\n",
      "(Iteration 6311 / 7640) loss: 1.810870\n",
      "(Iteration 6321 / 7640) loss: 1.910844\n",
      "(Iteration 6331 / 7640) loss: 1.873199\n",
      "(Iteration 6341 / 7640) loss: 1.611066\n",
      "(Iteration 6351 / 7640) loss: 1.584653\n",
      "(Iteration 6361 / 7640) loss: 1.738291\n",
      "(Iteration 6371 / 7640) loss: 1.612874\n",
      "(Iteration 6381 / 7640) loss: 1.728587\n",
      "(Iteration 6391 / 7640) loss: 1.840272\n",
      "(Iteration 6401 / 7640) loss: 1.541143\n",
      "(Iteration 6411 / 7640) loss: 1.575052\n",
      "(Iteration 6421 / 7640) loss: 1.868450\n",
      "(Iteration 6431 / 7640) loss: 1.673517\n",
      "(Iteration 6441 / 7640) loss: 1.627314\n",
      "(Iteration 6451 / 7640) loss: 1.603312\n",
      "(Iteration 6461 / 7640) loss: 1.674825\n",
      "(Iteration 6471 / 7640) loss: 1.528171\n",
      "(Iteration 6481 / 7640) loss: 1.560775\n",
      "(Iteration 6491 / 7640) loss: 1.771426\n",
      "(Epoch 17 / 20) train acc: 0.446000; val_acc: 0.446000\n",
      "(Iteration 6501 / 7640) loss: 1.637189\n",
      "(Iteration 6511 / 7640) loss: 1.617058\n",
      "(Iteration 6521 / 7640) loss: 1.606294\n",
      "(Iteration 6531 / 7640) loss: 1.597986\n",
      "(Iteration 6541 / 7640) loss: 1.721649\n",
      "(Iteration 6551 / 7640) loss: 1.691884\n",
      "(Iteration 6561 / 7640) loss: 1.549440\n",
      "(Iteration 6571 / 7640) loss: 1.621527\n",
      "(Iteration 6581 / 7640) loss: 1.489472\n",
      "(Iteration 6591 / 7640) loss: 1.737198\n",
      "(Iteration 6601 / 7640) loss: 1.586720\n",
      "(Iteration 6611 / 7640) loss: 1.813357\n",
      "(Iteration 6621 / 7640) loss: 1.568169\n",
      "(Iteration 6631 / 7640) loss: 1.732760\n",
      "(Iteration 6641 / 7640) loss: 1.583554\n",
      "(Iteration 6651 / 7640) loss: 1.575872\n",
      "(Iteration 6661 / 7640) loss: 1.580343\n",
      "(Iteration 6671 / 7640) loss: 1.691387\n",
      "(Iteration 6681 / 7640) loss: 1.869125\n",
      "(Iteration 6691 / 7640) loss: 1.741377\n",
      "(Iteration 6701 / 7640) loss: 1.647832\n",
      "(Iteration 6711 / 7640) loss: 1.645023\n",
      "(Iteration 6721 / 7640) loss: 1.687127\n",
      "(Iteration 6731 / 7640) loss: 1.651863\n",
      "(Iteration 6741 / 7640) loss: 1.642317\n",
      "(Iteration 6751 / 7640) loss: 1.691326\n",
      "(Iteration 6761 / 7640) loss: 1.725536\n",
      "(Iteration 6771 / 7640) loss: 1.619485\n",
      "(Iteration 6781 / 7640) loss: 1.834854\n",
      "(Iteration 6791 / 7640) loss: 1.615890\n",
      "(Iteration 6801 / 7640) loss: 1.629674\n",
      "(Iteration 6811 / 7640) loss: 1.659121\n",
      "(Iteration 6821 / 7640) loss: 1.830195\n",
      "(Iteration 6831 / 7640) loss: 1.640677\n",
      "(Iteration 6841 / 7640) loss: 1.910469\n",
      "(Iteration 6851 / 7640) loss: 1.663564\n",
      "(Iteration 6861 / 7640) loss: 1.570693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 6871 / 7640) loss: 1.706669\n",
      "(Epoch 18 / 20) train acc: 0.461000; val_acc: 0.446000\n",
      "(Iteration 6881 / 7640) loss: 1.549432\n",
      "(Iteration 6891 / 7640) loss: 1.718526\n",
      "(Iteration 6901 / 7640) loss: 1.683177\n",
      "(Iteration 6911 / 7640) loss: 1.695740\n",
      "(Iteration 6921 / 7640) loss: 1.599535\n",
      "(Iteration 6931 / 7640) loss: 1.624196\n",
      "(Iteration 6941 / 7640) loss: 1.599351\n",
      "(Iteration 6951 / 7640) loss: 1.621465\n",
      "(Iteration 6961 / 7640) loss: 1.574192\n",
      "(Iteration 6971 / 7640) loss: 1.584672\n",
      "(Iteration 6981 / 7640) loss: 1.660360\n",
      "(Iteration 6991 / 7640) loss: 1.590241\n",
      "(Iteration 7001 / 7640) loss: 1.666974\n",
      "(Iteration 7011 / 7640) loss: 1.527547\n",
      "(Iteration 7021 / 7640) loss: 1.658056\n",
      "(Iteration 7031 / 7640) loss: 1.541853\n",
      "(Iteration 7041 / 7640) loss: 1.754861\n",
      "(Iteration 7051 / 7640) loss: 1.653504\n",
      "(Iteration 7061 / 7640) loss: 1.765228\n",
      "(Iteration 7071 / 7640) loss: 1.544532\n",
      "(Iteration 7081 / 7640) loss: 1.493856\n",
      "(Iteration 7091 / 7640) loss: 1.734892\n",
      "(Iteration 7101 / 7640) loss: 1.756279\n",
      "(Iteration 7111 / 7640) loss: 1.724945\n",
      "(Iteration 7121 / 7640) loss: 1.618484\n",
      "(Iteration 7131 / 7640) loss: 1.637031\n",
      "(Iteration 7141 / 7640) loss: 1.620296\n",
      "(Iteration 7151 / 7640) loss: 1.661225\n",
      "(Iteration 7161 / 7640) loss: 1.735564\n",
      "(Iteration 7171 / 7640) loss: 1.618069\n",
      "(Iteration 7181 / 7640) loss: 1.646003\n",
      "(Iteration 7191 / 7640) loss: 1.667192\n",
      "(Iteration 7201 / 7640) loss: 1.606214\n",
      "(Iteration 7211 / 7640) loss: 1.867796\n",
      "(Iteration 7221 / 7640) loss: 1.695150\n",
      "(Iteration 7231 / 7640) loss: 1.809064\n",
      "(Iteration 7241 / 7640) loss: 1.520534\n",
      "(Iteration 7251 / 7640) loss: 1.686352\n",
      "(Epoch 19 / 20) train acc: 0.480000; val_acc: 0.473000\n",
      "(Iteration 7261 / 7640) loss: 1.759801\n",
      "(Iteration 7271 / 7640) loss: 1.810159\n",
      "(Iteration 7281 / 7640) loss: 1.816093\n",
      "(Iteration 7291 / 7640) loss: 1.600075\n",
      "(Iteration 7301 / 7640) loss: 1.711993\n",
      "(Iteration 7311 / 7640) loss: 1.640246\n",
      "(Iteration 7321 / 7640) loss: 1.677663\n",
      "(Iteration 7331 / 7640) loss: 1.681076\n",
      "(Iteration 7341 / 7640) loss: 1.538738\n",
      "(Iteration 7351 / 7640) loss: 1.695523\n",
      "(Iteration 7361 / 7640) loss: 1.708547\n",
      "(Iteration 7371 / 7640) loss: 1.566009\n",
      "(Iteration 7381 / 7640) loss: 1.707418\n",
      "(Iteration 7391 / 7640) loss: 1.591221\n",
      "(Iteration 7401 / 7640) loss: 1.619647\n",
      "(Iteration 7411 / 7640) loss: 1.542588\n",
      "(Iteration 7421 / 7640) loss: 1.680789\n",
      "(Iteration 7431 / 7640) loss: 1.605077\n",
      "(Iteration 7441 / 7640) loss: 1.604115\n",
      "(Iteration 7451 / 7640) loss: 1.732543\n",
      "(Iteration 7461 / 7640) loss: 1.610203\n",
      "(Iteration 7471 / 7640) loss: 1.621050\n",
      "(Iteration 7481 / 7640) loss: 1.638176\n",
      "(Iteration 7491 / 7640) loss: 1.641290\n",
      "(Iteration 7501 / 7640) loss: 1.607355\n",
      "(Iteration 7511 / 7640) loss: 1.434575\n",
      "(Iteration 7521 / 7640) loss: 1.660115\n",
      "(Iteration 7531 / 7640) loss: 1.771204\n",
      "(Iteration 7541 / 7640) loss: 1.742792\n",
      "(Iteration 7551 / 7640) loss: 1.522676\n",
      "(Iteration 7561 / 7640) loss: 1.720417\n",
      "(Iteration 7571 / 7640) loss: 1.633649\n",
      "(Iteration 7581 / 7640) loss: 1.583016\n",
      "(Iteration 7591 / 7640) loss: 1.707953\n",
      "(Iteration 7601 / 7640) loss: 1.660152\n",
      "(Iteration 7611 / 7640) loss: 1.616839\n",
      "(Iteration 7621 / 7640) loss: 1.607342\n",
      "(Iteration 7631 / 7640) loss: 1.723686\n",
      "(Epoch 20 / 20) train acc: 0.485000; val_acc: 0.466000\n",
      "learning_rate = 0.001000, reg = 0.010000, best val loss = 0.480000\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_acc = 0\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_scale = 1e-2\n",
    "reg = 1e-2\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['adagrad', 'nesterov_momentum', 'adamopt', 'sgd', 'sgd_momentum', 'rmsprop', 'adam']:\n",
    "    print('running with ', update_rule)\n",
    "    model = FullyConnectedNet([256, 512, 256, 128, 64], weight_scale=weight_scale, reg = reg)\n",
    "\n",
    "    solver = Solver(model, small_data,\n",
    "                  num_epochs=20, batch_size=128,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rate,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "    print('learning_rate = %f, reg = %f, best val loss = %f' %(learning_rate, reg, solver.best_val_acc))\n",
    "    if solver.best_val_acc > best_acc:\n",
    "        best_acc = solver.best_val_acc\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\tensorflow\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.55\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
